{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 627
    },
    "colab_type": "code",
    "id": "bFxuttO9Khp9",
    "outputId": "e94cd48b-761b-4b0f-da67-7d7193f424ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: scikit-learn in /usr/local/lib/python3.6/dist-packages (0.20.1)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.13.3 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (1.1.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.8.2 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (1.14.6)\n",
      "Collecting numpy\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ff/7f/9d804d2348471c67a7d8b5f84f9bc59fd1cefa148986f2b74552f8573555/numpy-1.15.4-cp36-cp36m-manylinux1_x86_64.whl (13.9MB)\n",
      "\u001b[K    100% |████████████████████████████████| 13.9MB 2.7MB/s \n",
      "\u001b[31mthinc 6.12.1 has requirement msgpack<0.6.0,>=0.5.6, but you'll have msgpack 0.6.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mfeaturetools 0.4.1 has requirement pandas>=0.23.0, but you'll have pandas 0.22.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mcufflinks 0.14.6 has requirement plotly>=3.0.0, but you'll have plotly 1.12.12 which is incompatible.\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: numpy\n",
      "  Found existing installation: numpy 1.14.6\n",
      "    Uninstalling numpy-1.14.6:\n",
      "      Successfully uninstalled numpy-1.14.6\n",
      "Successfully installed numpy-1.15.4\n",
      "Collecting pandas\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e1/d8/feeb346d41f181e83fba45224ab14a8d8af019b48af742e047f3845d8cff/pandas-0.23.4-cp36-cp36m-manylinux1_x86_64.whl (8.9MB)\n",
      "\u001b[K    100% |████████████████████████████████| 8.9MB 3.3MB/s \n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: python-dateutil>=2.5.0 in /usr/local/lib/python3.6/dist-packages (from pandas) (2.5.3)\n",
      "Requirement already satisfied, skipping upgrade: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas) (2018.7)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from pandas) (1.15.4)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.5.0->pandas) (1.11.0)\n",
      "\u001b[31mcufflinks 0.14.6 has requirement plotly>=3.0.0, but you'll have plotly 1.12.12 which is incompatible.\u001b[0m\n",
      "Installing collected packages: pandas\n",
      "  Found existing installation: pandas 0.22.0\n",
      "    Uninstalling pandas-0.22.0:\n",
      "      Successfully uninstalled pandas-0.22.0\n",
      "Successfully installed pandas-0.23.4\n",
      "Requirement already up-to-date: scipy in /usr/local/lib/python3.6/dist-packages (1.1.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.8.2 in /usr/local/lib/python3.6/dist-packages (from scipy) (1.15.4)\n",
      "Requirement already up-to-date: imbalanced-learn in /usr/local/lib/python3.6/dist-packages (0.4.3)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.13.3 in /usr/local/lib/python3.6/dist-packages (from imbalanced-learn) (1.1.0)\n",
      "Requirement already satisfied, skipping upgrade: scikit-learn>=0.20 in /usr/local/lib/python3.6/dist-packages (from imbalanced-learn) (0.20.1)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.8.2 in /usr/local/lib/python3.6/dist-packages (from imbalanced-learn) (1.15.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U scikit-learn\n",
    "!pip install -U numpy\n",
    "!pip install -U pandas\n",
    "!pip install -U scipy\n",
    "!pip install --upgrade imbalanced-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dkqSlyTOpxho"
   },
   "source": [
    "#Α. Στοιχεία Ομάδας\n",
    "\n",
    "### **Ομάδα Α46**\n",
    "\n",
    "Μαρκουλέσκου Έλενα-Μπιάνκα  (03115126)\n",
    "\n",
    "Παπασκαρλάτος Αλέξανδρος      (03111097)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N8kmdgUVp3Do"
   },
   "source": [
    "#Β. Εισαγωγή Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UrKLIU4YqONB"
   },
   "source": [
    "##Σύντομη περιγραφή\n",
    "\n",
    "Το παρόν dataset έχει τίτλο [Polish companies bankruptcy data Data Set](http://archive.ics.uci.edu/ml/datasets/Polish+companies+bankruptcy+data#) και αφορά την πρόβλεψη για χρεοκοπία πολωνικών εταιριών. Οι χρεοκοπημένες εταιρίες αναλύθηκαν για την περίοδο 2000-2012, ενώ αυτές που λειτουργούσαν ακόμα αξιολογήθηκαν για την περίοδο 2007-2013. \n",
    "\n",
    "Το dataset αποτελείται από 5 αρχεία. Το πρώτο αντιστοιχεί σε οικονομικά δεδομένα εταιρειών από την πρώτη χρονιά της περιόδου πρόβλεψης και και αντίστοιχο label που υποδεικνύει τη χρεοκοπία ή μη μετά από 5 χρόνια. Το δεύτερο περιέχει δεδομένα από τη δεύτερη χροονιά και υποδεικύει την κατάσταση χρεοκοπίας ή μη μετά από 4 χρόνια κ.ο.κ.\n",
    "\n",
    "##Δείγματα, χαρακτηριστικά και κλάσεις\n",
    "Για το καθένα από τα 5 αρχεία έχουμε:\n",
    "\n",
    "\n",
    "* 1st Year: 7027 δείγματα, 271 χρεοκοπημένες εταιρίες,  6756 που δε χρεοκόπησαν\n",
    "* 2nd Year: 10173 δείγματα, 400 χρεοκοπημένες εταιρίες,   9773 που δε χρεοκόπησαν\n",
    "* 3rd Year: 10503 δείγματα, 495 χρεοκοπημένες εταιρίες,   10008 που δε χρεοκόπησαν\n",
    "* 4th Year:  9792 δείγματα, 515 χρεοκοπημένες εταιρίες,   9277 που δε χρεοκόπησαν\n",
    "* 5th Year: 5910 δείγματα, 410 χρεοκοπημένες εταιρίες,    5500 που δε χρεοκόπησαν\n",
    "\n",
    "Αξίζει εδώ να παρατηρήσουμε πως ήδη μπορούμε να συμπεράνουμε ότι το dataset μας **δεν** είναι ισορροπημένο (θα το δούμε αυτό και στη συνέχεια).\n",
    "\n",
    "Συνολικά, λοιπόν, έχουμε **43450 δείγματα**.\n",
    "\n",
    "Η περιγραφή του dataset μας υποδεικύει πως έχουμε **64 χαρακτηριστικά**, τα είναι όλα αριθμητικά, συνεπώς **δεν έχουμε κατηγορικά χαρακτηριστικά**.\n",
    "\n",
    "Οι **ετικέτες** των κλάσεων είναι **2** και αντιστοιχούν σε χρεοκοπία και όχι χρεοκοπία.\n",
    "\n",
    "Παρακάτω εισάγουμε τα 5 αρχεία για να εξάγουμε περισσότερα συμπεράσματα."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 638,
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "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",
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": ""
      }
     }
    },
    "colab_type": "code",
    "id": "_XsNKaUa-NGW",
    "outputId": "70a49784-00d6-4b5d-937d-03dfa04b0aca"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-d46d1417-03c5-4d8d-8f5f-32798be61020\" name=\"files[]\" multiple disabled />\n",
       "     <output id=\"result-d46d1417-03c5-4d8d-8f5f-32798be61020\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving 1year.arff to 1year (1).arff\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-292f82be1b7a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0muploaded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0muploaded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36mupload\u001b[0;34m()\u001b[0m\n\u001b[1;32m     70\u001b[0m     result = _output.eval_js(\n\u001b[1;32m     71\u001b[0m         'google.colab._files._uploadFilesContinue(\"{output_id}\")'.format(\n\u001b[0;32m---> 72\u001b[0;31m             output_id=output_id))\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'action'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'append'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m       \u001b[0;31m# JS side uses a generator of promises to process all of the files- some\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/output/_js.py\u001b[0m in \u001b[0;36meval_js\u001b[0;34m(script, ignore_result)\u001b[0m\n\u001b[1;32m     37\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mignore_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     if (reply.get('type') == 'colab_reply' and\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from google.colab import files\n",
    "\n",
    "uploaded = files.upload()\n",
    "\n",
    "for fn in uploaded.keys():\n",
    "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
    "      name=fn, length=len(uploaded[fn])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "fkzquRv4Kfw9",
    "outputId": "d7e8be19-c08b-41c0-9149-fdd42f3c774a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'1year (1).arff'   2year.arff   4year.arff   sample_data\n",
      " 1year.arff\t   3year.arff   5year.arff   tmp\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y9bqG7c9MLFx"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.io import arff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1918
    },
    "colab_type": "code",
    "id": "lqADnx4XMPmw",
    "outputId": "c84031e8-6432-4c94-8a90-d571250fb09d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7027, 65)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Attr1</th>\n",
       "      <th>Attr2</th>\n",
       "      <th>Attr3</th>\n",
       "      <th>Attr4</th>\n",
       "      <th>Attr5</th>\n",
       "      <th>Attr6</th>\n",
       "      <th>Attr7</th>\n",
       "      <th>Attr8</th>\n",
       "      <th>Attr9</th>\n",
       "      <th>Attr10</th>\n",
       "      <th>...</th>\n",
       "      <th>Attr56</th>\n",
       "      <th>Attr57</th>\n",
       "      <th>Attr58</th>\n",
       "      <th>Attr59</th>\n",
       "      <th>Attr60</th>\n",
       "      <th>Attr61</th>\n",
       "      <th>Attr62</th>\n",
       "      <th>Attr63</th>\n",
       "      <th>Attr64</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.200550</td>\n",
       "      <td>0.379510</td>\n",
       "      <td>0.396410</td>\n",
       "      <td>2.04720</td>\n",
       "      <td>32.3510</td>\n",
       "      <td>0.388250</td>\n",
       "      <td>0.249760</td>\n",
       "      <td>1.330500</td>\n",
       "      <td>1.13890</td>\n",
       "      <td>0.504940</td>\n",
       "      <td>...</td>\n",
       "      <td>0.121960</td>\n",
       "      <td>0.397180</td>\n",
       "      <td>0.87804</td>\n",
       "      <td>0.001924</td>\n",
       "      <td>8.4160</td>\n",
       "      <td>5.1372</td>\n",
       "      <td>82.65800</td>\n",
       "      <td>4.41580</td>\n",
       "      <td>7.42770</td>\n",
       "      <td>b'0'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.209120</td>\n",
       "      <td>0.499880</td>\n",
       "      <td>0.472250</td>\n",
       "      <td>1.94470</td>\n",
       "      <td>14.7860</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.258340</td>\n",
       "      <td>0.996010</td>\n",
       "      <td>1.69960</td>\n",
       "      <td>0.497880</td>\n",
       "      <td>...</td>\n",
       "      <td>0.121300</td>\n",
       "      <td>0.420020</td>\n",
       "      <td>0.85300</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.1486</td>\n",
       "      <td>3.2732</td>\n",
       "      <td>107.35000</td>\n",
       "      <td>3.40000</td>\n",
       "      <td>60.98700</td>\n",
       "      <td>b'0'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.248660</td>\n",
       "      <td>0.695920</td>\n",
       "      <td>0.267130</td>\n",
       "      <td>1.55480</td>\n",
       "      <td>-1.1523</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.309060</td>\n",
       "      <td>0.436950</td>\n",
       "      <td>1.30900</td>\n",
       "      <td>0.304080</td>\n",
       "      <td>...</td>\n",
       "      <td>0.241140</td>\n",
       "      <td>0.817740</td>\n",
       "      <td>0.76599</td>\n",
       "      <td>0.694840</td>\n",
       "      <td>4.9909</td>\n",
       "      <td>3.9510</td>\n",
       "      <td>134.27000</td>\n",
       "      <td>2.71850</td>\n",
       "      <td>5.20780</td>\n",
       "      <td>b'0'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.081483</td>\n",
       "      <td>0.307340</td>\n",
       "      <td>0.458790</td>\n",
       "      <td>2.49280</td>\n",
       "      <td>51.9520</td>\n",
       "      <td>0.149880</td>\n",
       "      <td>0.092704</td>\n",
       "      <td>1.866100</td>\n",
       "      <td>1.05710</td>\n",
       "      <td>0.573530</td>\n",
       "      <td>...</td>\n",
       "      <td>0.054015</td>\n",
       "      <td>0.142070</td>\n",
       "      <td>0.94598</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.5746</td>\n",
       "      <td>3.6147</td>\n",
       "      <td>86.43500</td>\n",
       "      <td>4.22280</td>\n",
       "      <td>5.54970</td>\n",
       "      <td>b'0'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.187320</td>\n",
       "      <td>0.613230</td>\n",
       "      <td>0.229600</td>\n",
       "      <td>1.40630</td>\n",
       "      <td>-7.3128</td>\n",
       "      <td>0.187320</td>\n",
       "      <td>0.187320</td>\n",
       "      <td>0.630700</td>\n",
       "      <td>1.15590</td>\n",
       "      <td>0.386770</td>\n",
       "      <td>...</td>\n",
       "      <td>0.134850</td>\n",
       "      <td>0.484310</td>\n",
       "      <td>0.86515</td>\n",
       "      <td>0.124440</td>\n",
       "      <td>6.3985</td>\n",
       "      <td>4.3158</td>\n",
       "      <td>127.21000</td>\n",
       "      <td>2.86920</td>\n",
       "      <td>7.89800</td>\n",
       "      <td>b'0'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.228220</td>\n",
       "      <td>0.497940</td>\n",
       "      <td>0.359690</td>\n",
       "      <td>1.75020</td>\n",
       "      <td>-47.7170</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.281390</td>\n",
       "      <td>1.008300</td>\n",
       "      <td>1.97860</td>\n",
       "      <td>0.502060</td>\n",
       "      <td>...</td>\n",
       "      <td>0.139320</td>\n",
       "      <td>0.454570</td>\n",
       "      <td>0.85891</td>\n",
       "      <td>0.023002</td>\n",
       "      <td>3.4028</td>\n",
       "      <td>8.9949</td>\n",
       "      <td>88.44400</td>\n",
       "      <td>4.12690</td>\n",
       "      <td>12.29900</td>\n",
       "      <td>b'0'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.111090</td>\n",
       "      <td>0.647440</td>\n",
       "      <td>0.289710</td>\n",
       "      <td>1.47050</td>\n",
       "      <td>2.5349</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.111090</td>\n",
       "      <td>0.544540</td>\n",
       "      <td>1.73480</td>\n",
       "      <td>0.352560</td>\n",
       "      <td>...</td>\n",
       "      <td>0.605900</td>\n",
       "      <td>0.315100</td>\n",
       "      <td>0.40871</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.3222</td>\n",
       "      <td>2.9098</td>\n",
       "      <td>129.55000</td>\n",
       "      <td>2.81730</td>\n",
       "      <td>18.35200</td>\n",
       "      <td>b'0'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.532320</td>\n",
       "      <td>0.027059</td>\n",
       "      <td>0.705540</td>\n",
       "      <td>53.95400</td>\n",
       "      <td>299.5800</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.652400</td>\n",
       "      <td>35.957000</td>\n",
       "      <td>0.65273</td>\n",
       "      <td>0.972940</td>\n",
       "      <td>...</td>\n",
       "      <td>0.086730</td>\n",
       "      <td>0.547130</td>\n",
       "      <td>0.49521</td>\n",
       "      <td>0.013194</td>\n",
       "      <td>9.1300</td>\n",
       "      <td>82.0500</td>\n",
       "      <td>7.45030</td>\n",
       "      <td>48.99100</td>\n",
       "      <td>2.32170</td>\n",
       "      <td>b'0'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.009020</td>\n",
       "      <td>0.632020</td>\n",
       "      <td>0.053735</td>\n",
       "      <td>1.12630</td>\n",
       "      <td>-37.8420</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014434</td>\n",
       "      <td>0.582230</td>\n",
       "      <td>1.33320</td>\n",
       "      <td>0.367980</td>\n",
       "      <td>...</td>\n",
       "      <td>0.180110</td>\n",
       "      <td>0.024512</td>\n",
       "      <td>0.84165</td>\n",
       "      <td>0.340940</td>\n",
       "      <td>9.9665</td>\n",
       "      <td>4.2382</td>\n",
       "      <td>116.50000</td>\n",
       "      <td>3.13300</td>\n",
       "      <td>2.56030</td>\n",
       "      <td>b'0'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.124080</td>\n",
       "      <td>0.838370</td>\n",
       "      <td>0.142040</td>\n",
       "      <td>1.16940</td>\n",
       "      <td>-91.8830</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.153280</td>\n",
       "      <td>0.192790</td>\n",
       "      <td>2.11560</td>\n",
       "      <td>0.161630</td>\n",
       "      <td>...</td>\n",
       "      <td>0.079665</td>\n",
       "      <td>0.767680</td>\n",
       "      <td>0.92847</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.3192</td>\n",
       "      <td>6.4994</td>\n",
       "      <td>144.63000</td>\n",
       "      <td>2.52360</td>\n",
       "      <td>107.67000</td>\n",
       "      <td>b'0'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.240010</td>\n",
       "      <td>0.443550</td>\n",
       "      <td>0.188350</td>\n",
       "      <td>1.44000</td>\n",
       "      <td>-21.1650</td>\n",
       "      <td>-0.931900</td>\n",
       "      <td>0.240010</td>\n",
       "      <td>1.254500</td>\n",
       "      <td>4.74470</td>\n",
       "      <td>0.556450</td>\n",
       "      <td>...</td>\n",
       "      <td>0.353590</td>\n",
       "      <td>0.431320</td>\n",
       "      <td>0.64794</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>16.5710</td>\n",
       "      <td>17.0870</td>\n",
       "      <td>32.92800</td>\n",
       "      <td>11.08500</td>\n",
       "      <td>12.36900</td>\n",
       "      <td>b'0'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-0.027117</td>\n",
       "      <td>0.111480</td>\n",
       "      <td>0.119890</td>\n",
       "      <td>2.07540</td>\n",
       "      <td>-31.6430</td>\n",
       "      <td>-0.084883</td>\n",
       "      <td>-0.024300</td>\n",
       "      <td>7.674100</td>\n",
       "      <td>0.90732</td>\n",
       "      <td>0.855510</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.102140</td>\n",
       "      <td>-0.031697</td>\n",
       "      <td>1.10210</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.6683</td>\n",
       "      <td>13.4610</td>\n",
       "      <td>76.73200</td>\n",
       "      <td>4.75680</td>\n",
       "      <td>0.68991</td>\n",
       "      <td>b'0'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.266690</td>\n",
       "      <td>0.349940</td>\n",
       "      <td>0.611470</td>\n",
       "      <td>3.02430</td>\n",
       "      <td>43.0870</td>\n",
       "      <td>0.559830</td>\n",
       "      <td>0.332070</td>\n",
       "      <td>1.857700</td>\n",
       "      <td>1.12680</td>\n",
       "      <td>0.650060</td>\n",
       "      <td>...</td>\n",
       "      <td>0.112500</td>\n",
       "      <td>0.410250</td>\n",
       "      <td>0.88750</td>\n",
       "      <td>0.073630</td>\n",
       "      <td>9.5593</td>\n",
       "      <td>5.6298</td>\n",
       "      <td>38.16800</td>\n",
       "      <td>9.56290</td>\n",
       "      <td>33.41300</td>\n",
       "      <td>b'0'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.067731</td>\n",
       "      <td>0.198850</td>\n",
       "      <td>0.081562</td>\n",
       "      <td>2.95760</td>\n",
       "      <td>90.6060</td>\n",
       "      <td>0.212650</td>\n",
       "      <td>0.078063</td>\n",
       "      <td>4.029000</td>\n",
       "      <td>1.25700</td>\n",
       "      <td>0.801150</td>\n",
       "      <td>...</td>\n",
       "      <td>0.204440</td>\n",
       "      <td>0.084542</td>\n",
       "      <td>0.79556</td>\n",
       "      <td>0.196190</td>\n",
       "      <td>8.2122</td>\n",
       "      <td>2.7917</td>\n",
       "      <td>60.21800</td>\n",
       "      <td>6.06130</td>\n",
       "      <td>0.28803</td>\n",
       "      <td>b'0'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.029182</td>\n",
       "      <td>0.211310</td>\n",
       "      <td>0.452640</td>\n",
       "      <td>7.57460</td>\n",
       "      <td>57.8440</td>\n",
       "      <td>0.010387</td>\n",
       "      <td>-0.034653</td>\n",
       "      <td>3.732400</td>\n",
       "      <td>1.02410</td>\n",
       "      <td>0.788690</td>\n",
       "      <td>...</td>\n",
       "      <td>0.023565</td>\n",
       "      <td>-0.037001</td>\n",
       "      <td>0.97644</td>\n",
       "      <td>0.180630</td>\n",
       "      <td>3.4646</td>\n",
       "      <td>11.3380</td>\n",
       "      <td>31.80700</td>\n",
       "      <td>11.47500</td>\n",
       "      <td>1.65110</td>\n",
       "      <td>b'0'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-0.033801</td>\n",
       "      <td>1.154000</td>\n",
       "      <td>-0.205990</td>\n",
       "      <td>0.82150</td>\n",
       "      <td>-74.4510</td>\n",
       "      <td>-0.104130</td>\n",
       "      <td>-0.033801</td>\n",
       "      <td>-0.159000</td>\n",
       "      <td>0.97767</td>\n",
       "      <td>-0.183490</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.022837</td>\n",
       "      <td>0.184210</td>\n",
       "      <td>1.02280</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.2894</td>\n",
       "      <td>3.9519</td>\n",
       "      <td>182.81000</td>\n",
       "      <td>1.99660</td>\n",
       "      <td>44.29600</td>\n",
       "      <td>b'0'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.270530</td>\n",
       "      <td>0.299130</td>\n",
       "      <td>0.468700</td>\n",
       "      <td>2.56690</td>\n",
       "      <td>73.3950</td>\n",
       "      <td>0.727930</td>\n",
       "      <td>0.336190</td>\n",
       "      <td>2.231500</td>\n",
       "      <td>1.22140</td>\n",
       "      <td>0.667500</td>\n",
       "      <td>...</td>\n",
       "      <td>0.181270</td>\n",
       "      <td>0.405290</td>\n",
       "      <td>0.81873</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.0660</td>\n",
       "      <td>4.1474</td>\n",
       "      <td>60.29700</td>\n",
       "      <td>6.05340</td>\n",
       "      <td>7.79900</td>\n",
       "      <td>b'0'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.028084</td>\n",
       "      <td>0.242310</td>\n",
       "      <td>0.432240</td>\n",
       "      <td>3.01280</td>\n",
       "      <td>47.9350</td>\n",
       "      <td>0.021598</td>\n",
       "      <td>0.039729</td>\n",
       "      <td>3.103700</td>\n",
       "      <td>1.01250</td>\n",
       "      <td>0.752060</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012367</td>\n",
       "      <td>0.037342</td>\n",
       "      <td>0.98763</td>\n",
       "      <td>0.036647</td>\n",
       "      <td>10.0740</td>\n",
       "      <td>12.3780</td>\n",
       "      <td>41.48500</td>\n",
       "      <td>8.79840</td>\n",
       "      <td>5.35230</td>\n",
       "      <td>b'0'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.203930</td>\n",
       "      <td>0.560370</td>\n",
       "      <td>0.134950</td>\n",
       "      <td>1.24080</td>\n",
       "      <td>3.1580</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.242910</td>\n",
       "      <td>0.784520</td>\n",
       "      <td>2.27060</td>\n",
       "      <td>0.439630</td>\n",
       "      <td>...</td>\n",
       "      <td>0.107700</td>\n",
       "      <td>0.463860</td>\n",
       "      <td>0.89419</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>24.9790</td>\n",
       "      <td>10.7670</td>\n",
       "      <td>90.08000</td>\n",
       "      <td>4.05190</td>\n",
       "      <td>7.45250</td>\n",
       "      <td>b'0'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.208760</td>\n",
       "      <td>0.496500</td>\n",
       "      <td>0.425480</td>\n",
       "      <td>2.01900</td>\n",
       "      <td>38.9340</td>\n",
       "      <td>0.005436</td>\n",
       "      <td>0.256190</td>\n",
       "      <td>1.014100</td>\n",
       "      <td>2.28270</td>\n",
       "      <td>0.503500</td>\n",
       "      <td>...</td>\n",
       "      <td>0.112990</td>\n",
       "      <td>0.414620</td>\n",
       "      <td>0.88859</td>\n",
       "      <td>0.046947</td>\n",
       "      <td>12.6730</td>\n",
       "      <td>4.5996</td>\n",
       "      <td>66.76400</td>\n",
       "      <td>5.46700</td>\n",
       "      <td>14.54200</td>\n",
       "      <td>b'0'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.111190</td>\n",
       "      <td>0.631740</td>\n",
       "      <td>0.247960</td>\n",
       "      <td>2.00000</td>\n",
       "      <td>58.1540</td>\n",
       "      <td>0.243470</td>\n",
       "      <td>0.139090</td>\n",
       "      <td>0.582940</td>\n",
       "      <td>0.99034</td>\n",
       "      <td>0.368260</td>\n",
       "      <td>...</td>\n",
       "      <td>0.152540</td>\n",
       "      <td>0.301920</td>\n",
       "      <td>0.86275</td>\n",
       "      <td>1.030700</td>\n",
       "      <td>9.7941</td>\n",
       "      <td>2.6129</td>\n",
       "      <td>91.38800</td>\n",
       "      <td>3.99400</td>\n",
       "      <td>1.96520</td>\n",
       "      <td>b'0'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>-0.305050</td>\n",
       "      <td>1.252300</td>\n",
       "      <td>-0.292220</td>\n",
       "      <td>0.71426</td>\n",
       "      <td>-214.9100</td>\n",
       "      <td>-0.305050</td>\n",
       "      <td>-0.305050</td>\n",
       "      <td>-0.253440</td>\n",
       "      <td>0.77037</td>\n",
       "      <td>-0.317380</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.298090</td>\n",
       "      <td>0.961150</td>\n",
       "      <td>1.29810</td>\n",
       "      <td>-0.723530</td>\n",
       "      <td>2.4756</td>\n",
       "      <td>2.3377</td>\n",
       "      <td>447.81000</td>\n",
       "      <td>0.81507</td>\n",
       "      <td>3.09250</td>\n",
       "      <td>b'0'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.127090</td>\n",
       "      <td>0.530500</td>\n",
       "      <td>0.380690</td>\n",
       "      <td>1.71980</td>\n",
       "      <td>-27.6180</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.156110</td>\n",
       "      <td>0.885000</td>\n",
       "      <td>2.13220</td>\n",
       "      <td>0.469500</td>\n",
       "      <td>...</td>\n",
       "      <td>0.053878</td>\n",
       "      <td>0.270680</td>\n",
       "      <td>0.92918</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.0360</td>\n",
       "      <td>5.9864</td>\n",
       "      <td>90.54000</td>\n",
       "      <td>4.03140</td>\n",
       "      <td>23.58700</td>\n",
       "      <td>b'0'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.126240</td>\n",
       "      <td>0.662860</td>\n",
       "      <td>0.219160</td>\n",
       "      <td>1.35530</td>\n",
       "      <td>21.5880</td>\n",
       "      <td>0.020227</td>\n",
       "      <td>0.156330</td>\n",
       "      <td>0.508620</td>\n",
       "      <td>1.71300</td>\n",
       "      <td>0.337140</td>\n",
       "      <td>...</td>\n",
       "      <td>0.089183</td>\n",
       "      <td>0.374450</td>\n",
       "      <td>0.90914</td>\n",
       "      <td>0.110410</td>\n",
       "      <td>13.5630</td>\n",
       "      <td>2.4747</td>\n",
       "      <td>131.42000</td>\n",
       "      <td>2.77740</td>\n",
       "      <td>10.44000</td>\n",
       "      <td>b'0'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.292640</td>\n",
       "      <td>0.292520</td>\n",
       "      <td>0.344150</td>\n",
       "      <td>2.32700</td>\n",
       "      <td>108.0700</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.361310</td>\n",
       "      <td>2.418500</td>\n",
       "      <td>1.25120</td>\n",
       "      <td>0.707480</td>\n",
       "      <td>...</td>\n",
       "      <td>0.286920</td>\n",
       "      <td>0.413650</td>\n",
       "      <td>0.71408</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.3400</td>\n",
       "      <td>2.4188</td>\n",
       "      <td>75.65300</td>\n",
       "      <td>4.82470</td>\n",
       "      <td>3.15560</td>\n",
       "      <td>b'0'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.079997</td>\n",
       "      <td>0.239670</td>\n",
       "      <td>0.321240</td>\n",
       "      <td>2.47390</td>\n",
       "      <td>8.1087</td>\n",
       "      <td>0.207430</td>\n",
       "      <td>0.069715</td>\n",
       "      <td>2.546600</td>\n",
       "      <td>1.05560</td>\n",
       "      <td>0.610340</td>\n",
       "      <td>...</td>\n",
       "      <td>0.052697</td>\n",
       "      <td>0.131070</td>\n",
       "      <td>0.94730</td>\n",
       "      <td>0.035588</td>\n",
       "      <td>6.1579</td>\n",
       "      <td>10.2190</td>\n",
       "      <td>45.42200</td>\n",
       "      <td>8.03580</td>\n",
       "      <td>3.80070</td>\n",
       "      <td>b'0'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.208170</td>\n",
       "      <td>0.602400</td>\n",
       "      <td>0.479350</td>\n",
       "      <td>2.18830</td>\n",
       "      <td>69.6590</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.259110</td>\n",
       "      <td>0.660020</td>\n",
       "      <td>1.69230</td>\n",
       "      <td>0.397600</td>\n",
       "      <td>...</td>\n",
       "      <td>0.182420</td>\n",
       "      <td>0.523570</td>\n",
       "      <td>0.84731</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.3634</td>\n",
       "      <td>2.6362</td>\n",
       "      <td>87.01000</td>\n",
       "      <td>4.19490</td>\n",
       "      <td>14.43400</td>\n",
       "      <td>b'0'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.263450</td>\n",
       "      <td>0.143180</td>\n",
       "      <td>0.619650</td>\n",
       "      <td>5.32770</td>\n",
       "      <td>198.9500</td>\n",
       "      <td>0.559710</td>\n",
       "      <td>0.321420</td>\n",
       "      <td>5.862400</td>\n",
       "      <td>1.34370</td>\n",
       "      <td>0.839400</td>\n",
       "      <td>...</td>\n",
       "      <td>0.255780</td>\n",
       "      <td>0.313850</td>\n",
       "      <td>0.74422</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.1287</td>\n",
       "      <td>4.0062</td>\n",
       "      <td>43.82100</td>\n",
       "      <td>8.32940</td>\n",
       "      <td>5.02860</td>\n",
       "      <td>b'0'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>-0.054623</td>\n",
       "      <td>0.928570</td>\n",
       "      <td>-0.026927</td>\n",
       "      <td>0.94263</td>\n",
       "      <td>-87.7240</td>\n",
       "      <td>-0.054623</td>\n",
       "      <td>-0.054623</td>\n",
       "      <td>0.074796</td>\n",
       "      <td>0.92301</td>\n",
       "      <td>0.069453</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.083417</td>\n",
       "      <td>-0.786480</td>\n",
       "      <td>1.08340</td>\n",
       "      <td>6.611400</td>\n",
       "      <td>4.7055</td>\n",
       "      <td>3.1662</td>\n",
       "      <td>226.76000</td>\n",
       "      <td>1.60960</td>\n",
       "      <td>1.35510</td>\n",
       "      <td>b'0'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>-0.358840</td>\n",
       "      <td>1.117100</td>\n",
       "      <td>-0.465770</td>\n",
       "      <td>0.46115</td>\n",
       "      <td>-152.2100</td>\n",
       "      <td>-1.175200</td>\n",
       "      <td>-0.358840</td>\n",
       "      <td>-0.112060</td>\n",
       "      <td>0.97154</td>\n",
       "      <td>-0.125180</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.029298</td>\n",
       "      <td>2.866600</td>\n",
       "      <td>1.02930</td>\n",
       "      <td>-2.019000</td>\n",
       "      <td>8.9776</td>\n",
       "      <td>11.5070</td>\n",
       "      <td>197.43000</td>\n",
       "      <td>1.84880</td>\n",
       "      <td>2.65720</td>\n",
       "      <td>b'0'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6997</th>\n",
       "      <td>0.003802</td>\n",
       "      <td>0.141440</td>\n",
       "      <td>0.449040</td>\n",
       "      <td>4.17480</td>\n",
       "      <td>60.8960</td>\n",
       "      <td>0.003802</td>\n",
       "      <td>-0.004791</td>\n",
       "      <td>5.952200</td>\n",
       "      <td>0.99613</td>\n",
       "      <td>0.841880</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003885</td>\n",
       "      <td>0.004516</td>\n",
       "      <td>1.00390</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.5574</td>\n",
       "      <td>4.5153</td>\n",
       "      <td>34.11700</td>\n",
       "      <td>10.69800</td>\n",
       "      <td>3.69500</td>\n",
       "      <td>b'1'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6998</th>\n",
       "      <td>-0.018516</td>\n",
       "      <td>0.328860</td>\n",
       "      <td>0.069952</td>\n",
       "      <td>1.25700</td>\n",
       "      <td>-29.2980</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.018781</td>\n",
       "      <td>2.040800</td>\n",
       "      <td>1.11420</td>\n",
       "      <td>0.671130</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.031889</td>\n",
       "      <td>-0.027589</td>\n",
       "      <td>1.01620</td>\n",
       "      <td>0.039008</td>\n",
       "      <td>9.3983</td>\n",
       "      <td>6.3408</td>\n",
       "      <td>89.16300</td>\n",
       "      <td>4.09360</td>\n",
       "      <td>1.69370</td>\n",
       "      <td>b'1'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6999</th>\n",
       "      <td>0.022830</td>\n",
       "      <td>0.711320</td>\n",
       "      <td>-0.060759</td>\n",
       "      <td>0.69120</td>\n",
       "      <td>-14.0810</td>\n",
       "      <td>-0.048958</td>\n",
       "      <td>0.024774</td>\n",
       "      <td>0.405840</td>\n",
       "      <td>2.34630</td>\n",
       "      <td>0.288680</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004260</td>\n",
       "      <td>0.079084</td>\n",
       "      <td>0.97892</td>\n",
       "      <td>0.654850</td>\n",
       "      <td>83.1690</td>\n",
       "      <td>25.4370</td>\n",
       "      <td>30.60800</td>\n",
       "      <td>11.92500</td>\n",
       "      <td>2.71560</td>\n",
       "      <td>b'1'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7000</th>\n",
       "      <td>-0.000153</td>\n",
       "      <td>0.505820</td>\n",
       "      <td>-0.421270</td>\n",
       "      <td>0.15367</td>\n",
       "      <td>-883.0300</td>\n",
       "      <td>-0.000153</td>\n",
       "      <td>-0.004183</td>\n",
       "      <td>0.451220</td>\n",
       "      <td>0.91488</td>\n",
       "      <td>0.228240</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.093040</td>\n",
       "      <td>-0.000669</td>\n",
       "      <td>1.09300</td>\n",
       "      <td>0.035313</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.2655</td>\n",
       "      <td>1094.00000</td>\n",
       "      <td>0.33365</td>\n",
       "      <td>0.17983</td>\n",
       "      <td>b'1'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7001</th>\n",
       "      <td>-0.289710</td>\n",
       "      <td>1.187400</td>\n",
       "      <td>-0.465320</td>\n",
       "      <td>0.53814</td>\n",
       "      <td>-223.2800</td>\n",
       "      <td>-0.289710</td>\n",
       "      <td>-0.289710</td>\n",
       "      <td>-0.158310</td>\n",
       "      <td>0.81801</td>\n",
       "      <td>-0.187980</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.222480</td>\n",
       "      <td>1.541200</td>\n",
       "      <td>1.22250</td>\n",
       "      <td>-0.956790</td>\n",
       "      <td>3.7491</td>\n",
       "      <td>4.1109</td>\n",
       "      <td>354.26000</td>\n",
       "      <td>1.03030</td>\n",
       "      <td>2.28260</td>\n",
       "      <td>b'1'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7002</th>\n",
       "      <td>0.009313</td>\n",
       "      <td>0.617610</td>\n",
       "      <td>-0.222330</td>\n",
       "      <td>0.60119</td>\n",
       "      <td>-62.6300</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014544</td>\n",
       "      <td>0.619150</td>\n",
       "      <td>2.08020</td>\n",
       "      <td>0.382390</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008170</td>\n",
       "      <td>0.024354</td>\n",
       "      <td>1.00190</td>\n",
       "      <td>0.071883</td>\n",
       "      <td>20.0000</td>\n",
       "      <td>10.0800</td>\n",
       "      <td>97.81900</td>\n",
       "      <td>3.73140</td>\n",
       "      <td>3.12880</td>\n",
       "      <td>b'1'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7003</th>\n",
       "      <td>0.167730</td>\n",
       "      <td>0.773890</td>\n",
       "      <td>0.166630</td>\n",
       "      <td>1.24140</td>\n",
       "      <td>-28.0390</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.217420</td>\n",
       "      <td>0.292170</td>\n",
       "      <td>4.88180</td>\n",
       "      <td>0.226110</td>\n",
       "      <td>...</td>\n",
       "      <td>0.043982</td>\n",
       "      <td>0.741830</td>\n",
       "      <td>0.95170</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.3158</td>\n",
       "      <td>15.9310</td>\n",
       "      <td>51.61100</td>\n",
       "      <td>7.07220</td>\n",
       "      <td>34.13400</td>\n",
       "      <td>b'1'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7004</th>\n",
       "      <td>-0.330330</td>\n",
       "      <td>0.932660</td>\n",
       "      <td>-0.476930</td>\n",
       "      <td>0.48863</td>\n",
       "      <td>-350.8300</td>\n",
       "      <td>-0.330330</td>\n",
       "      <td>-0.330330</td>\n",
       "      <td>0.068734</td>\n",
       "      <td>0.70565</td>\n",
       "      <td>0.064105</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.417130</td>\n",
       "      <td>-5.153000</td>\n",
       "      <td>1.41710</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.8878</td>\n",
       "      <td>7.5244</td>\n",
       "      <td>504.65000</td>\n",
       "      <td>0.72328</td>\n",
       "      <td>1.23940</td>\n",
       "      <td>b'1'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7005</th>\n",
       "      <td>0.046080</td>\n",
       "      <td>0.003824</td>\n",
       "      <td>0.996180</td>\n",
       "      <td>261.50000</td>\n",
       "      <td>174.3300</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.046080</td>\n",
       "      <td>260.500000</td>\n",
       "      <td>2.08780</td>\n",
       "      <td>0.996180</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019324</td>\n",
       "      <td>0.046257</td>\n",
       "      <td>0.97800</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.4192</td>\n",
       "      <td>0.66856</td>\n",
       "      <td>545.95000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>b'1'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7006</th>\n",
       "      <td>0.133310</td>\n",
       "      <td>0.164510</td>\n",
       "      <td>0.400840</td>\n",
       "      <td>3.43660</td>\n",
       "      <td>2.8373</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.169280</td>\n",
       "      <td>5.078600</td>\n",
       "      <td>3.65580</td>\n",
       "      <td>0.835490</td>\n",
       "      <td>...</td>\n",
       "      <td>0.066065</td>\n",
       "      <td>0.159550</td>\n",
       "      <td>0.94701</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.8104</td>\n",
       "      <td>21.3280</td>\n",
       "      <td>16.42500</td>\n",
       "      <td>22.22200</td>\n",
       "      <td>8.41110</td>\n",
       "      <td>b'1'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7007</th>\n",
       "      <td>0.038665</td>\n",
       "      <td>0.071884</td>\n",
       "      <td>0.488840</td>\n",
       "      <td>7.80040</td>\n",
       "      <td>221.0100</td>\n",
       "      <td>0.038665</td>\n",
       "      <td>0.045892</td>\n",
       "      <td>11.068000</td>\n",
       "      <td>1.07650</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>...</td>\n",
       "      <td>0.071070</td>\n",
       "      <td>0.048599</td>\n",
       "      <td>0.92893</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.1470</td>\n",
       "      <td>1.4679</td>\n",
       "      <td>53.73500</td>\n",
       "      <td>6.79260</td>\n",
       "      <td>1.11160</td>\n",
       "      <td>b'1'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7008</th>\n",
       "      <td>0.001091</td>\n",
       "      <td>0.851600</td>\n",
       "      <td>0.003463</td>\n",
       "      <td>1.00860</td>\n",
       "      <td>-44.4670</td>\n",
       "      <td>0.086248</td>\n",
       "      <td>0.001091</td>\n",
       "      <td>0.174290</td>\n",
       "      <td>1.02970</td>\n",
       "      <td>0.148420</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.198900</td>\n",
       "      <td>0.007349</td>\n",
       "      <td>0.97403</td>\n",
       "      <td>2.033100</td>\n",
       "      <td>6.8515</td>\n",
       "      <td>4.1096</td>\n",
       "      <td>142.83000</td>\n",
       "      <td>2.55560</td>\n",
       "      <td>1.73460</td>\n",
       "      <td>b'1'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7009</th>\n",
       "      <td>-0.091442</td>\n",
       "      <td>0.705500</td>\n",
       "      <td>-0.047216</td>\n",
       "      <td>0.92568</td>\n",
       "      <td>-7.2952</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.090374</td>\n",
       "      <td>0.417440</td>\n",
       "      <td>9.13450</td>\n",
       "      <td>0.294500</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000966</td>\n",
       "      <td>-0.310490</td>\n",
       "      <td>1.00740</td>\n",
       "      <td>0.077583</td>\n",
       "      <td>72.8930</td>\n",
       "      <td>20.7900</td>\n",
       "      <td>25.38400</td>\n",
       "      <td>14.37900</td>\n",
       "      <td>22.18000</td>\n",
       "      <td>b'1'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7010</th>\n",
       "      <td>0.138090</td>\n",
       "      <td>3.335700</td>\n",
       "      <td>-2.364000</td>\n",
       "      <td>0.29128</td>\n",
       "      <td>-88.3820</td>\n",
       "      <td>-3.396300</td>\n",
       "      <td>0.138090</td>\n",
       "      <td>-0.700210</td>\n",
       "      <td>9.98520</td>\n",
       "      <td>-2.335700</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011347</td>\n",
       "      <td>-0.059122</td>\n",
       "      <td>0.97866</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>231.9000</td>\n",
       "      <td>12.6510</td>\n",
       "      <td>121.93000</td>\n",
       "      <td>2.99350</td>\n",
       "      <td>351.85000</td>\n",
       "      <td>b'1'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7011</th>\n",
       "      <td>0.098271</td>\n",
       "      <td>0.833300</td>\n",
       "      <td>0.000426</td>\n",
       "      <td>1.00050</td>\n",
       "      <td>-43.1910</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.128380</td>\n",
       "      <td>0.200190</td>\n",
       "      <td>2.51440</td>\n",
       "      <td>0.166820</td>\n",
       "      <td>...</td>\n",
       "      <td>0.150320</td>\n",
       "      <td>0.589090</td>\n",
       "      <td>0.85877</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.2728</td>\n",
       "      <td>4.9207</td>\n",
       "      <td>116.48000</td>\n",
       "      <td>3.13360</td>\n",
       "      <td>12.74800</td>\n",
       "      <td>b'1'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7012</th>\n",
       "      <td>0.006404</td>\n",
       "      <td>0.674770</td>\n",
       "      <td>-0.008438</td>\n",
       "      <td>0.98668</td>\n",
       "      <td>-45.5990</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009958</td>\n",
       "      <td>0.482000</td>\n",
       "      <td>1.91090</td>\n",
       "      <td>0.325230</td>\n",
       "      <td>...</td>\n",
       "      <td>0.180200</td>\n",
       "      <td>0.019690</td>\n",
       "      <td>0.81928</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.9144</td>\n",
       "      <td>4.9967</td>\n",
       "      <td>121.00000</td>\n",
       "      <td>3.01640</td>\n",
       "      <td>5.09650</td>\n",
       "      <td>b'1'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7013</th>\n",
       "      <td>0.037953</td>\n",
       "      <td>0.122390</td>\n",
       "      <td>0.230840</td>\n",
       "      <td>2.88600</td>\n",
       "      <td>19.5810</td>\n",
       "      <td>0.037953</td>\n",
       "      <td>0.040082</td>\n",
       "      <td>6.886400</td>\n",
       "      <td>0.98825</td>\n",
       "      <td>0.842840</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.011887</td>\n",
       "      <td>0.045030</td>\n",
       "      <td>1.01190</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.8809</td>\n",
       "      <td>34.6410</td>\n",
       "      <td>60.06000</td>\n",
       "      <td>6.07730</td>\n",
       "      <td>1.15040</td>\n",
       "      <td>b'1'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7014</th>\n",
       "      <td>0.161170</td>\n",
       "      <td>0.072135</td>\n",
       "      <td>0.636490</td>\n",
       "      <td>12.02500</td>\n",
       "      <td>178.9000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.199860</td>\n",
       "      <td>12.863000</td>\n",
       "      <td>1.29080</td>\n",
       "      <td>0.927860</td>\n",
       "      <td>...</td>\n",
       "      <td>0.150780</td>\n",
       "      <td>0.173710</td>\n",
       "      <td>0.84152</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.2700</td>\n",
       "      <td>6.3165</td>\n",
       "      <td>16.32500</td>\n",
       "      <td>22.35800</td>\n",
       "      <td>4.22300</td>\n",
       "      <td>b'1'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7015</th>\n",
       "      <td>0.019432</td>\n",
       "      <td>0.616330</td>\n",
       "      <td>0.057718</td>\n",
       "      <td>1.10620</td>\n",
       "      <td>-162.8800</td>\n",
       "      <td>0.019432</td>\n",
       "      <td>0.008018</td>\n",
       "      <td>-0.232850</td>\n",
       "      <td>1.06560</td>\n",
       "      <td>-0.143510</td>\n",
       "      <td>...</td>\n",
       "      <td>0.061544</td>\n",
       "      <td>-0.135400</td>\n",
       "      <td>0.93846</td>\n",
       "      <td>-0.508510</td>\n",
       "      <td>2.1849</td>\n",
       "      <td>16.6580</td>\n",
       "      <td>179.33000</td>\n",
       "      <td>2.03540</td>\n",
       "      <td>2.77270</td>\n",
       "      <td>b'1'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7016</th>\n",
       "      <td>0.022707</td>\n",
       "      <td>0.598550</td>\n",
       "      <td>-0.152680</td>\n",
       "      <td>0.65004</td>\n",
       "      <td>-55.4880</td>\n",
       "      <td>0.022707</td>\n",
       "      <td>0.023087</td>\n",
       "      <td>0.666780</td>\n",
       "      <td>0.99912</td>\n",
       "      <td>0.399100</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000878</td>\n",
       "      <td>0.056896</td>\n",
       "      <td>1.00090</td>\n",
       "      <td>0.406580</td>\n",
       "      <td>15.1100</td>\n",
       "      <td>13.1450</td>\n",
       "      <td>83.39800</td>\n",
       "      <td>4.37660</td>\n",
       "      <td>2.66530</td>\n",
       "      <td>b'1'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7017</th>\n",
       "      <td>0.015903</td>\n",
       "      <td>0.730140</td>\n",
       "      <td>0.083767</td>\n",
       "      <td>1.28650</td>\n",
       "      <td>-19.8070</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020462</td>\n",
       "      <td>0.369610</td>\n",
       "      <td>2.19550</td>\n",
       "      <td>0.269860</td>\n",
       "      <td>...</td>\n",
       "      <td>0.031304</td>\n",
       "      <td>0.058928</td>\n",
       "      <td>1.00160</td>\n",
       "      <td>1.470900</td>\n",
       "      <td>11.0380</td>\n",
       "      <td>12.5620</td>\n",
       "      <td>48.60000</td>\n",
       "      <td>7.51020</td>\n",
       "      <td>3.51900</td>\n",
       "      <td>b'1'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7018</th>\n",
       "      <td>-0.051896</td>\n",
       "      <td>0.536300</td>\n",
       "      <td>0.054014</td>\n",
       "      <td>1.10610</td>\n",
       "      <td>-9.7422</td>\n",
       "      <td>-0.051896</td>\n",
       "      <td>-0.051896</td>\n",
       "      <td>0.686440</td>\n",
       "      <td>0.98574</td>\n",
       "      <td>0.368140</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.014470</td>\n",
       "      <td>-0.140970</td>\n",
       "      <td>1.01450</td>\n",
       "      <td>0.073630</td>\n",
       "      <td>21.6900</td>\n",
       "      <td>13.7120</td>\n",
       "      <td>66.22400</td>\n",
       "      <td>5.51160</td>\n",
       "      <td>6.42520</td>\n",
       "      <td>b'1'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7019</th>\n",
       "      <td>-0.031617</td>\n",
       "      <td>0.811750</td>\n",
       "      <td>-0.202300</td>\n",
       "      <td>0.61087</td>\n",
       "      <td>-44.0110</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.031617</td>\n",
       "      <td>0.231900</td>\n",
       "      <td>3.03470</td>\n",
       "      <td>0.188250</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018387</td>\n",
       "      <td>-0.167950</td>\n",
       "      <td>0.98125</td>\n",
       "      <td>1.101500</td>\n",
       "      <td>20.3810</td>\n",
       "      <td>20.3950</td>\n",
       "      <td>62.52900</td>\n",
       "      <td>5.83730</td>\n",
       "      <td>4.44690</td>\n",
       "      <td>b'1'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7020</th>\n",
       "      <td>0.025664</td>\n",
       "      <td>0.762040</td>\n",
       "      <td>0.091220</td>\n",
       "      <td>1.12630</td>\n",
       "      <td>-62.1550</td>\n",
       "      <td>0.025664</td>\n",
       "      <td>0.037809</td>\n",
       "      <td>0.213700</td>\n",
       "      <td>1.06870</td>\n",
       "      <td>0.162850</td>\n",
       "      <td>...</td>\n",
       "      <td>0.064291</td>\n",
       "      <td>0.157590</td>\n",
       "      <td>0.93571</td>\n",
       "      <td>0.245150</td>\n",
       "      <td>4.8872</td>\n",
       "      <td>3.6692</td>\n",
       "      <td>158.21000</td>\n",
       "      <td>2.30710</td>\n",
       "      <td>8.92490</td>\n",
       "      <td>b'1'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7021</th>\n",
       "      <td>0.069049</td>\n",
       "      <td>0.682320</td>\n",
       "      <td>0.069829</td>\n",
       "      <td>1.12710</td>\n",
       "      <td>-54.5330</td>\n",
       "      <td>0.069049</td>\n",
       "      <td>0.075024</td>\n",
       "      <td>0.424540</td>\n",
       "      <td>1.05380</td>\n",
       "      <td>0.289670</td>\n",
       "      <td>...</td>\n",
       "      <td>0.051048</td>\n",
       "      <td>0.238370</td>\n",
       "      <td>0.94895</td>\n",
       "      <td>0.458290</td>\n",
       "      <td>5.5224</td>\n",
       "      <td>3.9048</td>\n",
       "      <td>145.40000</td>\n",
       "      <td>2.51040</td>\n",
       "      <td>3.62500</td>\n",
       "      <td>b'1'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7022</th>\n",
       "      <td>0.018371</td>\n",
       "      <td>0.474100</td>\n",
       "      <td>-0.136190</td>\n",
       "      <td>0.60839</td>\n",
       "      <td>-18.4490</td>\n",
       "      <td>0.018371</td>\n",
       "      <td>0.018371</td>\n",
       "      <td>0.972030</td>\n",
       "      <td>1.01210</td>\n",
       "      <td>0.460840</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011909</td>\n",
       "      <td>0.039866</td>\n",
       "      <td>0.98809</td>\n",
       "      <td>0.274140</td>\n",
       "      <td>73.5050</td>\n",
       "      <td>79.2370</td>\n",
       "      <td>31.26800</td>\n",
       "      <td>11.67300</td>\n",
       "      <td>5.14890</td>\n",
       "      <td>b'1'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7023</th>\n",
       "      <td>-0.013359</td>\n",
       "      <td>0.583540</td>\n",
       "      <td>-0.022650</td>\n",
       "      <td>0.92896</td>\n",
       "      <td>-42.2320</td>\n",
       "      <td>-0.013359</td>\n",
       "      <td>-0.015036</td>\n",
       "      <td>0.562890</td>\n",
       "      <td>0.98904</td>\n",
       "      <td>0.328470</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.011082</td>\n",
       "      <td>-0.040671</td>\n",
       "      <td>1.01110</td>\n",
       "      <td>0.805920</td>\n",
       "      <td>10.5990</td>\n",
       "      <td>7.1740</td>\n",
       "      <td>94.09200</td>\n",
       "      <td>3.87920</td>\n",
       "      <td>1.75720</td>\n",
       "      <td>b'1'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7024</th>\n",
       "      <td>0.006338</td>\n",
       "      <td>0.502760</td>\n",
       "      <td>0.439230</td>\n",
       "      <td>1.87360</td>\n",
       "      <td>9.7417</td>\n",
       "      <td>0.006338</td>\n",
       "      <td>0.012022</td>\n",
       "      <td>0.983560</td>\n",
       "      <td>1.00830</td>\n",
       "      <td>0.494490</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008258</td>\n",
       "      <td>0.012817</td>\n",
       "      <td>0.99174</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.4700</td>\n",
       "      <td>6.0759</td>\n",
       "      <td>51.01900</td>\n",
       "      <td>7.15420</td>\n",
       "      <td>62.00100</td>\n",
       "      <td>b'1'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7025</th>\n",
       "      <td>-0.041643</td>\n",
       "      <td>0.848100</td>\n",
       "      <td>-0.128520</td>\n",
       "      <td>0.57485</td>\n",
       "      <td>-121.9200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.036795</td>\n",
       "      <td>0.179010</td>\n",
       "      <td>0.42138</td>\n",
       "      <td>0.151820</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.232720</td>\n",
       "      <td>-0.274290</td>\n",
       "      <td>0.98788</td>\n",
       "      <td>3.593100</td>\n",
       "      <td>39.7030</td>\n",
       "      <td>3.1420</td>\n",
       "      <td>261.85000</td>\n",
       "      <td>1.39390</td>\n",
       "      <td>0.51005</td>\n",
       "      <td>b'1'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7026</th>\n",
       "      <td>0.014946</td>\n",
       "      <td>0.946480</td>\n",
       "      <td>0.032110</td>\n",
       "      <td>1.03630</td>\n",
       "      <td>-20.5810</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015260</td>\n",
       "      <td>0.056357</td>\n",
       "      <td>2.96940</td>\n",
       "      <td>0.053341</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015705</td>\n",
       "      <td>0.280210</td>\n",
       "      <td>0.97443</td>\n",
       "      <td>1.179200</td>\n",
       "      <td>15.0360</td>\n",
       "      <td>4.1741</td>\n",
       "      <td>108.64000</td>\n",
       "      <td>3.35990</td>\n",
       "      <td>35.11800</td>\n",
       "      <td>b'1'</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7027 rows × 65 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Attr1     Attr2     Attr3      Attr4     Attr5     Attr6     Attr7  \\\n",
       "0     0.200550  0.379510  0.396410    2.04720   32.3510  0.388250  0.249760   \n",
       "1     0.209120  0.499880  0.472250    1.94470   14.7860  0.000000  0.258340   \n",
       "2     0.248660  0.695920  0.267130    1.55480   -1.1523  0.000000  0.309060   \n",
       "3     0.081483  0.307340  0.458790    2.49280   51.9520  0.149880  0.092704   \n",
       "4     0.187320  0.613230  0.229600    1.40630   -7.3128  0.187320  0.187320   \n",
       "5     0.228220  0.497940  0.359690    1.75020  -47.7170  0.000000  0.281390   \n",
       "6     0.111090  0.647440  0.289710    1.47050    2.5349  0.000000  0.111090   \n",
       "7     0.532320  0.027059  0.705540   53.95400  299.5800  0.000000  0.652400   \n",
       "8     0.009020  0.632020  0.053735    1.12630  -37.8420  0.000000  0.014434   \n",
       "9     0.124080  0.838370  0.142040    1.16940  -91.8830  0.000000  0.153280   \n",
       "10    0.240010  0.443550  0.188350    1.44000  -21.1650 -0.931900  0.240010   \n",
       "11   -0.027117  0.111480  0.119890    2.07540  -31.6430 -0.084883 -0.024300   \n",
       "12    0.266690  0.349940  0.611470    3.02430   43.0870  0.559830  0.332070   \n",
       "13    0.067731  0.198850  0.081562    2.95760   90.6060  0.212650  0.078063   \n",
       "14   -0.029182  0.211310  0.452640    7.57460   57.8440  0.010387 -0.034653   \n",
       "15   -0.033801  1.154000 -0.205990    0.82150  -74.4510 -0.104130 -0.033801   \n",
       "16    0.270530  0.299130  0.468700    2.56690   73.3950  0.727930  0.336190   \n",
       "17    0.028084  0.242310  0.432240    3.01280   47.9350  0.021598  0.039729   \n",
       "18    0.203930  0.560370  0.134950    1.24080    3.1580  0.000000  0.242910   \n",
       "19    0.208760  0.496500  0.425480    2.01900   38.9340  0.005436  0.256190   \n",
       "20    0.111190  0.631740  0.247960    2.00000   58.1540  0.243470  0.139090   \n",
       "21   -0.305050  1.252300 -0.292220    0.71426 -214.9100 -0.305050 -0.305050   \n",
       "22    0.127090  0.530500  0.380690    1.71980  -27.6180  0.000000  0.156110   \n",
       "23    0.126240  0.662860  0.219160    1.35530   21.5880  0.020227  0.156330   \n",
       "24    0.292640  0.292520  0.344150    2.32700  108.0700  0.000000  0.361310   \n",
       "25    0.079997  0.239670  0.321240    2.47390    8.1087  0.207430  0.069715   \n",
       "26    0.208170  0.602400  0.479350    2.18830   69.6590  0.000000  0.259110   \n",
       "27    0.263450  0.143180  0.619650    5.32770  198.9500  0.559710  0.321420   \n",
       "28   -0.054623  0.928570 -0.026927    0.94263  -87.7240 -0.054623 -0.054623   \n",
       "29   -0.358840  1.117100 -0.465770    0.46115 -152.2100 -1.175200 -0.358840   \n",
       "...        ...       ...       ...        ...       ...       ...       ...   \n",
       "6997  0.003802  0.141440  0.449040    4.17480   60.8960  0.003802 -0.004791   \n",
       "6998 -0.018516  0.328860  0.069952    1.25700  -29.2980  0.000000 -0.018781   \n",
       "6999  0.022830  0.711320 -0.060759    0.69120  -14.0810 -0.048958  0.024774   \n",
       "7000 -0.000153  0.505820 -0.421270    0.15367 -883.0300 -0.000153 -0.004183   \n",
       "7001 -0.289710  1.187400 -0.465320    0.53814 -223.2800 -0.289710 -0.289710   \n",
       "7002  0.009313  0.617610 -0.222330    0.60119  -62.6300  0.000000  0.014544   \n",
       "7003  0.167730  0.773890  0.166630    1.24140  -28.0390  0.000000  0.217420   \n",
       "7004 -0.330330  0.932660 -0.476930    0.48863 -350.8300 -0.330330 -0.330330   \n",
       "7005  0.046080  0.003824  0.996180  261.50000  174.3300  0.000000  0.046080   \n",
       "7006  0.133310  0.164510  0.400840    3.43660    2.8373  0.000000  0.169280   \n",
       "7007  0.038665  0.071884  0.488840    7.80040  221.0100  0.038665  0.045892   \n",
       "7008  0.001091  0.851600  0.003463    1.00860  -44.4670  0.086248  0.001091   \n",
       "7009 -0.091442  0.705500 -0.047216    0.92568   -7.2952  0.000000 -0.090374   \n",
       "7010  0.138090  3.335700 -2.364000    0.29128  -88.3820 -3.396300  0.138090   \n",
       "7011  0.098271  0.833300  0.000426    1.00050  -43.1910  0.000000  0.128380   \n",
       "7012  0.006404  0.674770 -0.008438    0.98668  -45.5990  0.000000  0.009958   \n",
       "7013  0.037953  0.122390  0.230840    2.88600   19.5810  0.037953  0.040082   \n",
       "7014  0.161170  0.072135  0.636490   12.02500  178.9000  0.000000  0.199860   \n",
       "7015  0.019432  0.616330  0.057718    1.10620 -162.8800  0.019432  0.008018   \n",
       "7016  0.022707  0.598550 -0.152680    0.65004  -55.4880  0.022707  0.023087   \n",
       "7017  0.015903  0.730140  0.083767    1.28650  -19.8070  0.000000  0.020462   \n",
       "7018 -0.051896  0.536300  0.054014    1.10610   -9.7422 -0.051896 -0.051896   \n",
       "7019 -0.031617  0.811750 -0.202300    0.61087  -44.0110  0.000000 -0.031617   \n",
       "7020  0.025664  0.762040  0.091220    1.12630  -62.1550  0.025664  0.037809   \n",
       "7021  0.069049  0.682320  0.069829    1.12710  -54.5330  0.069049  0.075024   \n",
       "7022  0.018371  0.474100 -0.136190    0.60839  -18.4490  0.018371  0.018371   \n",
       "7023 -0.013359  0.583540 -0.022650    0.92896  -42.2320 -0.013359 -0.015036   \n",
       "7024  0.006338  0.502760  0.439230    1.87360    9.7417  0.006338  0.012022   \n",
       "7025 -0.041643  0.848100 -0.128520    0.57485 -121.9200  0.000000 -0.036795   \n",
       "7026  0.014946  0.946480  0.032110    1.03630  -20.5810  0.000000  0.015260   \n",
       "\n",
       "           Attr8    Attr9    Attr10  ...      Attr56    Attr57   Attr58  \\\n",
       "0       1.330500  1.13890  0.504940  ...    0.121960  0.397180  0.87804   \n",
       "1       0.996010  1.69960  0.497880  ...    0.121300  0.420020  0.85300   \n",
       "2       0.436950  1.30900  0.304080  ...    0.241140  0.817740  0.76599   \n",
       "3       1.866100  1.05710  0.573530  ...    0.054015  0.142070  0.94598   \n",
       "4       0.630700  1.15590  0.386770  ...    0.134850  0.484310  0.86515   \n",
       "5       1.008300  1.97860  0.502060  ...    0.139320  0.454570  0.85891   \n",
       "6       0.544540  1.73480  0.352560  ...    0.605900  0.315100  0.40871   \n",
       "7      35.957000  0.65273  0.972940  ...    0.086730  0.547130  0.49521   \n",
       "8       0.582230  1.33320  0.367980  ...    0.180110  0.024512  0.84165   \n",
       "9       0.192790  2.11560  0.161630  ...    0.079665  0.767680  0.92847   \n",
       "10      1.254500  4.74470  0.556450  ...    0.353590  0.431320  0.64794   \n",
       "11      7.674100  0.90732  0.855510  ...   -0.102140 -0.031697  1.10210   \n",
       "12      1.857700  1.12680  0.650060  ...    0.112500  0.410250  0.88750   \n",
       "13      4.029000  1.25700  0.801150  ...    0.204440  0.084542  0.79556   \n",
       "14      3.732400  1.02410  0.788690  ...    0.023565 -0.037001  0.97644   \n",
       "15     -0.159000  0.97767 -0.183490  ...   -0.022837  0.184210  1.02280   \n",
       "16      2.231500  1.22140  0.667500  ...    0.181270  0.405290  0.81873   \n",
       "17      3.103700  1.01250  0.752060  ...    0.012367  0.037342  0.98763   \n",
       "18      0.784520  2.27060  0.439630  ...    0.107700  0.463860  0.89419   \n",
       "19      1.014100  2.28270  0.503500  ...    0.112990  0.414620  0.88859   \n",
       "20      0.582940  0.99034  0.368260  ...    0.152540  0.301920  0.86275   \n",
       "21     -0.253440  0.77037 -0.317380  ...   -0.298090  0.961150  1.29810   \n",
       "22      0.885000  2.13220  0.469500  ...    0.053878  0.270680  0.92918   \n",
       "23      0.508620  1.71300  0.337140  ...    0.089183  0.374450  0.90914   \n",
       "24      2.418500  1.25120  0.707480  ...    0.286920  0.413650  0.71408   \n",
       "25      2.546600  1.05560  0.610340  ...    0.052697  0.131070  0.94730   \n",
       "26      0.660020  1.69230  0.397600  ...    0.182420  0.523570  0.84731   \n",
       "27      5.862400  1.34370  0.839400  ...    0.255780  0.313850  0.74422   \n",
       "28      0.074796  0.92301  0.069453  ...   -0.083417 -0.786480  1.08340   \n",
       "29     -0.112060  0.97154 -0.125180  ...   -0.029298  2.866600  1.02930   \n",
       "...          ...      ...       ...  ...         ...       ...      ...   \n",
       "6997    5.952200  0.99613  0.841880  ...   -0.003885  0.004516  1.00390   \n",
       "6998    2.040800  1.11420  0.671130  ...   -0.031889 -0.027589  1.01620   \n",
       "6999    0.405840  2.34630  0.288680  ...    0.004260  0.079084  0.97892   \n",
       "7000    0.451220  0.91488  0.228240  ...   -0.093040 -0.000669  1.09300   \n",
       "7001   -0.158310  0.81801 -0.187980  ...   -0.222480  1.541200  1.22250   \n",
       "7002    0.619150  2.08020  0.382390  ...    0.008170  0.024354  1.00190   \n",
       "7003    0.292170  4.88180  0.226110  ...    0.043982  0.741830  0.95170   \n",
       "7004    0.068734  0.70565  0.064105  ...   -0.417130 -5.153000  1.41710   \n",
       "7005  260.500000  2.08780  0.996180  ...    0.019324  0.046257  0.97800   \n",
       "7006    5.078600  3.65580  0.835490  ...    0.066065  0.159550  0.94701   \n",
       "7007   11.068000  1.07650  0.795600  ...    0.071070  0.048599  0.92893   \n",
       "7008    0.174290  1.02970  0.148420  ...   -0.198900  0.007349  0.97403   \n",
       "7009    0.417440  9.13450  0.294500  ...    0.000966 -0.310490  1.00740   \n",
       "7010   -0.700210  9.98520 -2.335700  ...    0.011347 -0.059122  0.97866   \n",
       "7011    0.200190  2.51440  0.166820  ...    0.150320  0.589090  0.85877   \n",
       "7012    0.482000  1.91090  0.325230  ...    0.180200  0.019690  0.81928   \n",
       "7013    6.886400  0.98825  0.842840  ...   -0.011887  0.045030  1.01190   \n",
       "7014   12.863000  1.29080  0.927860  ...    0.150780  0.173710  0.84152   \n",
       "7015   -0.232850  1.06560 -0.143510  ...    0.061544 -0.135400  0.93846   \n",
       "7016    0.666780  0.99912  0.399100  ...   -0.000878  0.056896  1.00090   \n",
       "7017    0.369610  2.19550  0.269860  ...    0.031304  0.058928  1.00160   \n",
       "7018    0.686440  0.98574  0.368140  ...   -0.014470 -0.140970  1.01450   \n",
       "7019    0.231900  3.03470  0.188250  ...    0.018387 -0.167950  0.98125   \n",
       "7020    0.213700  1.06870  0.162850  ...    0.064291  0.157590  0.93571   \n",
       "7021    0.424540  1.05380  0.289670  ...    0.051048  0.238370  0.94895   \n",
       "7022    0.972030  1.01210  0.460840  ...    0.011909  0.039866  0.98809   \n",
       "7023    0.562890  0.98904  0.328470  ...   -0.011082 -0.040671  1.01110   \n",
       "7024    0.983560  1.00830  0.494490  ...    0.008258  0.012817  0.99174   \n",
       "7025    0.179010  0.42138  0.151820  ...   -0.232720 -0.274290  0.98788   \n",
       "7026    0.056357  2.96940  0.053341  ...    0.015705  0.280210  0.97443   \n",
       "\n",
       "        Attr59    Attr60   Attr61      Attr62     Attr63     Attr64  class  \n",
       "0     0.001924    8.4160   5.1372    82.65800    4.41580    7.42770   b'0'  \n",
       "1     0.000000    4.1486   3.2732   107.35000    3.40000   60.98700   b'0'  \n",
       "2     0.694840    4.9909   3.9510   134.27000    2.71850    5.20780   b'0'  \n",
       "3     0.000000    4.5746   3.6147    86.43500    4.22280    5.54970   b'0'  \n",
       "4     0.124440    6.3985   4.3158   127.21000    2.86920    7.89800   b'0'  \n",
       "5     0.023002    3.4028   8.9949    88.44400    4.12690   12.29900   b'0'  \n",
       "6     0.000000    6.3222   2.9098   129.55000    2.81730   18.35200   b'0'  \n",
       "7     0.013194    9.1300  82.0500     7.45030   48.99100    2.32170   b'0'  \n",
       "8     0.340940    9.9665   4.2382   116.50000    3.13300    2.56030   b'0'  \n",
       "9     0.000000    3.3192   6.4994   144.63000    2.52360  107.67000   b'0'  \n",
       "10    0.000000   16.5710  17.0870    32.92800   11.08500   12.36900   b'0'  \n",
       "11    0.000000    3.6683  13.4610    76.73200    4.75680    0.68991   b'0'  \n",
       "12    0.073630    9.5593   5.6298    38.16800    9.56290   33.41300   b'0'  \n",
       "13    0.196190    8.2122   2.7917    60.21800    6.06130    0.28803   b'0'  \n",
       "14    0.180630    3.4646  11.3380    31.80700   11.47500    1.65110   b'0'  \n",
       "15    0.000000    8.2894   3.9519   182.81000    1.99660   44.29600   b'0'  \n",
       "16    0.000000   11.0660   4.1474    60.29700    6.05340    7.79900   b'0'  \n",
       "17    0.036647   10.0740  12.3780    41.48500    8.79840    5.35230   b'0'  \n",
       "18    0.000000   24.9790  10.7670    90.08000    4.05190    7.45250   b'0'  \n",
       "19    0.046947   12.6730   4.5996    66.76400    5.46700   14.54200   b'0'  \n",
       "20    1.030700    9.7941   2.6129    91.38800    3.99400    1.96520   b'0'  \n",
       "21   -0.723530    2.4756   2.3377   447.81000    0.81507    3.09250   b'0'  \n",
       "22    0.000000    4.0360   5.9864    90.54000    4.03140   23.58700   b'0'  \n",
       "23    0.110410   13.5630   2.4747   131.42000    2.77740   10.44000   b'0'  \n",
       "24    0.000000   15.3400   2.4188    75.65300    4.82470    3.15560   b'0'  \n",
       "25    0.035588    6.1579  10.2190    45.42200    8.03580    3.80070   b'0'  \n",
       "26    0.000000    8.3634   2.6362    87.01000    4.19490   14.43400   b'0'  \n",
       "27    0.000000    8.1287   4.0062    43.82100    8.32940    5.02860   b'0'  \n",
       "28    6.611400    4.7055   3.1662   226.76000    1.60960    1.35510   b'0'  \n",
       "29   -2.019000    8.9776  11.5070   197.43000    1.84880    2.65720   b'0'  \n",
       "...        ...       ...      ...         ...        ...        ...    ...  \n",
       "6997  0.000000    7.5574   4.5153    34.11700   10.69800    3.69500   b'1'  \n",
       "6998  0.039008    9.3983   6.3408    89.16300    4.09360    1.69370   b'1'  \n",
       "6999  0.654850   83.1690  25.4370    30.60800   11.92500    2.71560   b'1'  \n",
       "7000  0.035313       NaN   2.2655  1094.00000    0.33365    0.17983   b'1'  \n",
       "7001 -0.956790    3.7491   4.1109   354.26000    1.03030    2.28260   b'1'  \n",
       "7002  0.071883   20.0000  10.0800    97.81900    3.73140    3.12880   b'1'  \n",
       "7003  0.000000    9.3158  15.9310    51.61100    7.07220   34.13400   b'1'  \n",
       "7004  0.000000    1.8878   7.5244   504.65000    0.72328    1.23940   b'1'  \n",
       "7005  0.000000       NaN   6.4192     0.66856  545.95000        NaN   b'1'  \n",
       "7006  0.000000    9.8104  21.3280    16.42500   22.22200    8.41110   b'1'  \n",
       "7007  0.000000    2.1470   1.4679    53.73500    6.79260    1.11160   b'1'  \n",
       "7008  2.033100    6.8515   4.1096   142.83000    2.55560    1.73460   b'1'  \n",
       "7009  0.077583   72.8930  20.7900    25.38400   14.37900   22.18000   b'1'  \n",
       "7010  0.000000  231.9000  12.6510   121.93000    2.99350  351.85000   b'1'  \n",
       "7011  0.000000    9.2728   4.9207   116.48000    3.13360   12.74800   b'1'  \n",
       "7012  0.000000    8.9144   4.9967   121.00000    3.01640    5.09650   b'1'  \n",
       "7013  0.000000    3.8809  34.6410    60.06000    6.07730    1.15040   b'1'  \n",
       "7014  0.000000   12.2700   6.3165    16.32500   22.35800    4.22300   b'1'  \n",
       "7015 -0.508510    2.1849  16.6580   179.33000    2.03540    2.77270   b'1'  \n",
       "7016  0.406580   15.1100  13.1450    83.39800    4.37660    2.66530   b'1'  \n",
       "7017  1.470900   11.0380  12.5620    48.60000    7.51020    3.51900   b'1'  \n",
       "7018  0.073630   21.6900  13.7120    66.22400    5.51160    6.42520   b'1'  \n",
       "7019  1.101500   20.3810  20.3950    62.52900    5.83730    4.44690   b'1'  \n",
       "7020  0.245150    4.8872   3.6692   158.21000    2.30710    8.92490   b'1'  \n",
       "7021  0.458290    5.5224   3.9048   145.40000    2.51040    3.62500   b'1'  \n",
       "7022  0.274140   73.5050  79.2370    31.26800   11.67300    5.14890   b'1'  \n",
       "7023  0.805920   10.5990   7.1740    94.09200    3.87920    1.75720   b'1'  \n",
       "7024  0.000000   10.4700   6.0759    51.01900    7.15420   62.00100   b'1'  \n",
       "7025  3.593100   39.7030   3.1420   261.85000    1.39390    0.51005   b'1'  \n",
       "7026  1.179200   15.0360   4.1741   108.64000    3.35990   35.11800   b'1'  \n",
       "\n",
       "[7027 rows x 65 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1 = arff.loadarff('1year.arff')\n",
    "df1 = pd.DataFrame(data1[0])\n",
    "data2 = arff.loadarff('2year.arff')\n",
    "df2 = pd.DataFrame(data2[0])\n",
    "data3 = arff.loadarff('3year.arff')\n",
    "df3 = pd.DataFrame(data3[0])\n",
    "data4 = arff.loadarff('4year.arff')\n",
    "df4 = pd.DataFrame(data4[0])\n",
    "data5 = arff.loadarff('5year.arff')\n",
    "df5 = pd.DataFrame(data5[0])\n",
    "print(df1.shape)\n",
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z9Tp68E61FtB"
   },
   "source": [
    "Παρατηρούμε ότι τα δεδομένα μας **δεν έχουν αρίθμηση γραμμών ούτε επικεφαλίδες στην πρώτη γραμμή**. Άρα μπορούμε να τα ενώσουμε χωρίς να χρειαστεί να τα επεξεργαστούμε κάπως.\n",
    "\n",
    "Τα **χαρακτηριστικά** βρίσκονται στις **64 πρώτες στήλες** και το **target στην τελευταία**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1918
    },
    "colab_type": "code",
    "id": "XRPmHUoTukQT",
    "outputId": "bfe07cf3-3b34-4e48-e81f-32696e7cd8a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(43405, 65)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Attr1</th>\n",
       "      <th>Attr2</th>\n",
       "      <th>Attr3</th>\n",
       "      <th>Attr4</th>\n",
       "      <th>Attr5</th>\n",
       "      <th>Attr6</th>\n",
       "      <th>Attr7</th>\n",
       "      <th>Attr8</th>\n",
       "      <th>Attr9</th>\n",
       "      <th>Attr10</th>\n",
       "      <th>...</th>\n",
       "      <th>Attr56</th>\n",
       "      <th>Attr57</th>\n",
       "      <th>Attr58</th>\n",
       "      <th>Attr59</th>\n",
       "      <th>Attr60</th>\n",
       "      <th>Attr61</th>\n",
       "      <th>Attr62</th>\n",
       "      <th>Attr63</th>\n",
       "      <th>Attr64</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.200550</td>\n",
       "      <td>0.379510</td>\n",
       "      <td>0.396410</td>\n",
       "      <td>2.04720</td>\n",
       "      <td>3.235100e+01</td>\n",
       "      <td>0.388250</td>\n",
       "      <td>0.249760</td>\n",
       "      <td>1.330500</td>\n",
       "      <td>1.13890</td>\n",
       "      <td>0.504940</td>\n",
       "      <td>...</td>\n",
       "      <td>0.121960</td>\n",
       "      <td>0.397180</td>\n",
       "      <td>0.87804</td>\n",
       "      <td>0.001924</td>\n",
       "      <td>8.4160</td>\n",
       "      <td>5.1372</td>\n",
       "      <td>82.6580</td>\n",
       "      <td>4.415800</td>\n",
       "      <td>7.42770</td>\n",
       "      <td>b'0'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.209120</td>\n",
       "      <td>0.499880</td>\n",
       "      <td>0.472250</td>\n",
       "      <td>1.94470</td>\n",
       "      <td>1.478600e+01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.258340</td>\n",
       "      <td>0.996010</td>\n",
       "      <td>1.69960</td>\n",
       "      <td>0.497880</td>\n",
       "      <td>...</td>\n",
       "      <td>0.121300</td>\n",
       "      <td>0.420020</td>\n",
       "      <td>0.85300</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.1486</td>\n",
       "      <td>3.2732</td>\n",
       "      <td>107.3500</td>\n",
       "      <td>3.400000</td>\n",
       "      <td>60.98700</td>\n",
       "      <td>b'0'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.248660</td>\n",
       "      <td>0.695920</td>\n",
       "      <td>0.267130</td>\n",
       "      <td>1.55480</td>\n",
       "      <td>-1.152300e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.309060</td>\n",
       "      <td>0.436950</td>\n",
       "      <td>1.30900</td>\n",
       "      <td>0.304080</td>\n",
       "      <td>...</td>\n",
       "      <td>0.241140</td>\n",
       "      <td>0.817740</td>\n",
       "      <td>0.76599</td>\n",
       "      <td>0.694840</td>\n",
       "      <td>4.9909</td>\n",
       "      <td>3.9510</td>\n",
       "      <td>134.2700</td>\n",
       "      <td>2.718500</td>\n",
       "      <td>5.20780</td>\n",
       "      <td>b'0'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.081483</td>\n",
       "      <td>0.307340</td>\n",
       "      <td>0.458790</td>\n",
       "      <td>2.49280</td>\n",
       "      <td>5.195200e+01</td>\n",
       "      <td>0.149880</td>\n",
       "      <td>0.092704</td>\n",
       "      <td>1.866100</td>\n",
       "      <td>1.05710</td>\n",
       "      <td>0.573530</td>\n",
       "      <td>...</td>\n",
       "      <td>0.054015</td>\n",
       "      <td>0.142070</td>\n",
       "      <td>0.94598</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.5746</td>\n",
       "      <td>3.6147</td>\n",
       "      <td>86.4350</td>\n",
       "      <td>4.222800</td>\n",
       "      <td>5.54970</td>\n",
       "      <td>b'0'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.187320</td>\n",
       "      <td>0.613230</td>\n",
       "      <td>0.229600</td>\n",
       "      <td>1.40630</td>\n",
       "      <td>-7.312800e+00</td>\n",
       "      <td>0.187320</td>\n",
       "      <td>0.187320</td>\n",
       "      <td>0.630700</td>\n",
       "      <td>1.15590</td>\n",
       "      <td>0.386770</td>\n",
       "      <td>...</td>\n",
       "      <td>0.134850</td>\n",
       "      <td>0.484310</td>\n",
       "      <td>0.86515</td>\n",
       "      <td>0.124440</td>\n",
       "      <td>6.3985</td>\n",
       "      <td>4.3158</td>\n",
       "      <td>127.2100</td>\n",
       "      <td>2.869200</td>\n",
       "      <td>7.89800</td>\n",
       "      <td>b'0'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.228220</td>\n",
       "      <td>0.497940</td>\n",
       "      <td>0.359690</td>\n",
       "      <td>1.75020</td>\n",
       "      <td>-4.771700e+01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.281390</td>\n",
       "      <td>1.008300</td>\n",
       "      <td>1.97860</td>\n",
       "      <td>0.502060</td>\n",
       "      <td>...</td>\n",
       "      <td>0.139320</td>\n",
       "      <td>0.454570</td>\n",
       "      <td>0.85891</td>\n",
       "      <td>0.023002</td>\n",
       "      <td>3.4028</td>\n",
       "      <td>8.9949</td>\n",
       "      <td>88.4440</td>\n",
       "      <td>4.126900</td>\n",
       "      <td>12.29900</td>\n",
       "      <td>b'0'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.111090</td>\n",
       "      <td>0.647440</td>\n",
       "      <td>0.289710</td>\n",
       "      <td>1.47050</td>\n",
       "      <td>2.534900e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.111090</td>\n",
       "      <td>0.544540</td>\n",
       "      <td>1.73480</td>\n",
       "      <td>0.352560</td>\n",
       "      <td>...</td>\n",
       "      <td>0.605900</td>\n",
       "      <td>0.315100</td>\n",
       "      <td>0.40871</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.3222</td>\n",
       "      <td>2.9098</td>\n",
       "      <td>129.5500</td>\n",
       "      <td>2.817300</td>\n",
       "      <td>18.35200</td>\n",
       "      <td>b'0'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.532320</td>\n",
       "      <td>0.027059</td>\n",
       "      <td>0.705540</td>\n",
       "      <td>53.95400</td>\n",
       "      <td>2.995800e+02</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.652400</td>\n",
       "      <td>35.957000</td>\n",
       "      <td>0.65273</td>\n",
       "      <td>0.972940</td>\n",
       "      <td>...</td>\n",
       "      <td>0.086730</td>\n",
       "      <td>0.547130</td>\n",
       "      <td>0.49521</td>\n",
       "      <td>0.013194</td>\n",
       "      <td>9.1300</td>\n",
       "      <td>82.0500</td>\n",
       "      <td>7.4503</td>\n",
       "      <td>48.991000</td>\n",
       "      <td>2.32170</td>\n",
       "      <td>b'0'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.009020</td>\n",
       "      <td>0.632020</td>\n",
       "      <td>0.053735</td>\n",
       "      <td>1.12630</td>\n",
       "      <td>-3.784200e+01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014434</td>\n",
       "      <td>0.582230</td>\n",
       "      <td>1.33320</td>\n",
       "      <td>0.367980</td>\n",
       "      <td>...</td>\n",
       "      <td>0.180110</td>\n",
       "      <td>0.024512</td>\n",
       "      <td>0.84165</td>\n",
       "      <td>0.340940</td>\n",
       "      <td>9.9665</td>\n",
       "      <td>4.2382</td>\n",
       "      <td>116.5000</td>\n",
       "      <td>3.133000</td>\n",
       "      <td>2.56030</td>\n",
       "      <td>b'0'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.124080</td>\n",
       "      <td>0.838370</td>\n",
       "      <td>0.142040</td>\n",
       "      <td>1.16940</td>\n",
       "      <td>-9.188300e+01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.153280</td>\n",
       "      <td>0.192790</td>\n",
       "      <td>2.11560</td>\n",
       "      <td>0.161630</td>\n",
       "      <td>...</td>\n",
       "      <td>0.079665</td>\n",
       "      <td>0.767680</td>\n",
       "      <td>0.92847</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.3192</td>\n",
       "      <td>6.4994</td>\n",
       "      <td>144.6300</td>\n",
       "      <td>2.523600</td>\n",
       "      <td>107.67000</td>\n",
       "      <td>b'0'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.240010</td>\n",
       "      <td>0.443550</td>\n",
       "      <td>0.188350</td>\n",
       "      <td>1.44000</td>\n",
       "      <td>-2.116500e+01</td>\n",
       "      <td>-0.931900</td>\n",
       "      <td>0.240010</td>\n",
       "      <td>1.254500</td>\n",
       "      <td>4.74470</td>\n",
       "      <td>0.556450</td>\n",
       "      <td>...</td>\n",
       "      <td>0.353590</td>\n",
       "      <td>0.431320</td>\n",
       "      <td>0.64794</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>16.5710</td>\n",
       "      <td>17.0870</td>\n",
       "      <td>32.9280</td>\n",
       "      <td>11.085000</td>\n",
       "      <td>12.36900</td>\n",
       "      <td>b'0'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-0.027117</td>\n",
       "      <td>0.111480</td>\n",
       "      <td>0.119890</td>\n",
       "      <td>2.07540</td>\n",
       "      <td>-3.164300e+01</td>\n",
       "      <td>-0.084883</td>\n",
       "      <td>-0.024300</td>\n",
       "      <td>7.674100</td>\n",
       "      <td>0.90732</td>\n",
       "      <td>0.855510</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.102140</td>\n",
       "      <td>-0.031697</td>\n",
       "      <td>1.10210</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.6683</td>\n",
       "      <td>13.4610</td>\n",
       "      <td>76.7320</td>\n",
       "      <td>4.756800</td>\n",
       "      <td>0.68991</td>\n",
       "      <td>b'0'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.266690</td>\n",
       "      <td>0.349940</td>\n",
       "      <td>0.611470</td>\n",
       "      <td>3.02430</td>\n",
       "      <td>4.308700e+01</td>\n",
       "      <td>0.559830</td>\n",
       "      <td>0.332070</td>\n",
       "      <td>1.857700</td>\n",
       "      <td>1.12680</td>\n",
       "      <td>0.650060</td>\n",
       "      <td>...</td>\n",
       "      <td>0.112500</td>\n",
       "      <td>0.410250</td>\n",
       "      <td>0.88750</td>\n",
       "      <td>0.073630</td>\n",
       "      <td>9.5593</td>\n",
       "      <td>5.6298</td>\n",
       "      <td>38.1680</td>\n",
       "      <td>9.562900</td>\n",
       "      <td>33.41300</td>\n",
       "      <td>b'0'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.067731</td>\n",
       "      <td>0.198850</td>\n",
       "      <td>0.081562</td>\n",
       "      <td>2.95760</td>\n",
       "      <td>9.060600e+01</td>\n",
       "      <td>0.212650</td>\n",
       "      <td>0.078063</td>\n",
       "      <td>4.029000</td>\n",
       "      <td>1.25700</td>\n",
       "      <td>0.801150</td>\n",
       "      <td>...</td>\n",
       "      <td>0.204440</td>\n",
       "      <td>0.084542</td>\n",
       "      <td>0.79556</td>\n",
       "      <td>0.196190</td>\n",
       "      <td>8.2122</td>\n",
       "      <td>2.7917</td>\n",
       "      <td>60.2180</td>\n",
       "      <td>6.061300</td>\n",
       "      <td>0.28803</td>\n",
       "      <td>b'0'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.029182</td>\n",
       "      <td>0.211310</td>\n",
       "      <td>0.452640</td>\n",
       "      <td>7.57460</td>\n",
       "      <td>5.784400e+01</td>\n",
       "      <td>0.010387</td>\n",
       "      <td>-0.034653</td>\n",
       "      <td>3.732400</td>\n",
       "      <td>1.02410</td>\n",
       "      <td>0.788690</td>\n",
       "      <td>...</td>\n",
       "      <td>0.023565</td>\n",
       "      <td>-0.037001</td>\n",
       "      <td>0.97644</td>\n",
       "      <td>0.180630</td>\n",
       "      <td>3.4646</td>\n",
       "      <td>11.3380</td>\n",
       "      <td>31.8070</td>\n",
       "      <td>11.475000</td>\n",
       "      <td>1.65110</td>\n",
       "      <td>b'0'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-0.033801</td>\n",
       "      <td>1.154000</td>\n",
       "      <td>-0.205990</td>\n",
       "      <td>0.82150</td>\n",
       "      <td>-7.445100e+01</td>\n",
       "      <td>-0.104130</td>\n",
       "      <td>-0.033801</td>\n",
       "      <td>-0.159000</td>\n",
       "      <td>0.97767</td>\n",
       "      <td>-0.183490</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.022837</td>\n",
       "      <td>0.184210</td>\n",
       "      <td>1.02280</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.2894</td>\n",
       "      <td>3.9519</td>\n",
       "      <td>182.8100</td>\n",
       "      <td>1.996600</td>\n",
       "      <td>44.29600</td>\n",
       "      <td>b'0'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.270530</td>\n",
       "      <td>0.299130</td>\n",
       "      <td>0.468700</td>\n",
       "      <td>2.56690</td>\n",
       "      <td>7.339500e+01</td>\n",
       "      <td>0.727930</td>\n",
       "      <td>0.336190</td>\n",
       "      <td>2.231500</td>\n",
       "      <td>1.22140</td>\n",
       "      <td>0.667500</td>\n",
       "      <td>...</td>\n",
       "      <td>0.181270</td>\n",
       "      <td>0.405290</td>\n",
       "      <td>0.81873</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.0660</td>\n",
       "      <td>4.1474</td>\n",
       "      <td>60.2970</td>\n",
       "      <td>6.053400</td>\n",
       "      <td>7.79900</td>\n",
       "      <td>b'0'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.028084</td>\n",
       "      <td>0.242310</td>\n",
       "      <td>0.432240</td>\n",
       "      <td>3.01280</td>\n",
       "      <td>4.793500e+01</td>\n",
       "      <td>0.021598</td>\n",
       "      <td>0.039729</td>\n",
       "      <td>3.103700</td>\n",
       "      <td>1.01250</td>\n",
       "      <td>0.752060</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012367</td>\n",
       "      <td>0.037342</td>\n",
       "      <td>0.98763</td>\n",
       "      <td>0.036647</td>\n",
       "      <td>10.0740</td>\n",
       "      <td>12.3780</td>\n",
       "      <td>41.4850</td>\n",
       "      <td>8.798400</td>\n",
       "      <td>5.35230</td>\n",
       "      <td>b'0'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.203930</td>\n",
       "      <td>0.560370</td>\n",
       "      <td>0.134950</td>\n",
       "      <td>1.24080</td>\n",
       "      <td>3.158000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.242910</td>\n",
       "      <td>0.784520</td>\n",
       "      <td>2.27060</td>\n",
       "      <td>0.439630</td>\n",
       "      <td>...</td>\n",
       "      <td>0.107700</td>\n",
       "      <td>0.463860</td>\n",
       "      <td>0.89419</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>24.9790</td>\n",
       "      <td>10.7670</td>\n",
       "      <td>90.0800</td>\n",
       "      <td>4.051900</td>\n",
       "      <td>7.45250</td>\n",
       "      <td>b'0'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.208760</td>\n",
       "      <td>0.496500</td>\n",
       "      <td>0.425480</td>\n",
       "      <td>2.01900</td>\n",
       "      <td>3.893400e+01</td>\n",
       "      <td>0.005436</td>\n",
       "      <td>0.256190</td>\n",
       "      <td>1.014100</td>\n",
       "      <td>2.28270</td>\n",
       "      <td>0.503500</td>\n",
       "      <td>...</td>\n",
       "      <td>0.112990</td>\n",
       "      <td>0.414620</td>\n",
       "      <td>0.88859</td>\n",
       "      <td>0.046947</td>\n",
       "      <td>12.6730</td>\n",
       "      <td>4.5996</td>\n",
       "      <td>66.7640</td>\n",
       "      <td>5.467000</td>\n",
       "      <td>14.54200</td>\n",
       "      <td>b'0'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.111190</td>\n",
       "      <td>0.631740</td>\n",
       "      <td>0.247960</td>\n",
       "      <td>2.00000</td>\n",
       "      <td>5.815400e+01</td>\n",
       "      <td>0.243470</td>\n",
       "      <td>0.139090</td>\n",
       "      <td>0.582940</td>\n",
       "      <td>0.99034</td>\n",
       "      <td>0.368260</td>\n",
       "      <td>...</td>\n",
       "      <td>0.152540</td>\n",
       "      <td>0.301920</td>\n",
       "      <td>0.86275</td>\n",
       "      <td>1.030700</td>\n",
       "      <td>9.7941</td>\n",
       "      <td>2.6129</td>\n",
       "      <td>91.3880</td>\n",
       "      <td>3.994000</td>\n",
       "      <td>1.96520</td>\n",
       "      <td>b'0'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>-0.305050</td>\n",
       "      <td>1.252300</td>\n",
       "      <td>-0.292220</td>\n",
       "      <td>0.71426</td>\n",
       "      <td>-2.149100e+02</td>\n",
       "      <td>-0.305050</td>\n",
       "      <td>-0.305050</td>\n",
       "      <td>-0.253440</td>\n",
       "      <td>0.77037</td>\n",
       "      <td>-0.317380</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.298090</td>\n",
       "      <td>0.961150</td>\n",
       "      <td>1.29810</td>\n",
       "      <td>-0.723530</td>\n",
       "      <td>2.4756</td>\n",
       "      <td>2.3377</td>\n",
       "      <td>447.8100</td>\n",
       "      <td>0.815070</td>\n",
       "      <td>3.09250</td>\n",
       "      <td>b'0'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.127090</td>\n",
       "      <td>0.530500</td>\n",
       "      <td>0.380690</td>\n",
       "      <td>1.71980</td>\n",
       "      <td>-2.761800e+01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.156110</td>\n",
       "      <td>0.885000</td>\n",
       "      <td>2.13220</td>\n",
       "      <td>0.469500</td>\n",
       "      <td>...</td>\n",
       "      <td>0.053878</td>\n",
       "      <td>0.270680</td>\n",
       "      <td>0.92918</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.0360</td>\n",
       "      <td>5.9864</td>\n",
       "      <td>90.5400</td>\n",
       "      <td>4.031400</td>\n",
       "      <td>23.58700</td>\n",
       "      <td>b'0'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.126240</td>\n",
       "      <td>0.662860</td>\n",
       "      <td>0.219160</td>\n",
       "      <td>1.35530</td>\n",
       "      <td>2.158800e+01</td>\n",
       "      <td>0.020227</td>\n",
       "      <td>0.156330</td>\n",
       "      <td>0.508620</td>\n",
       "      <td>1.71300</td>\n",
       "      <td>0.337140</td>\n",
       "      <td>...</td>\n",
       "      <td>0.089183</td>\n",
       "      <td>0.374450</td>\n",
       "      <td>0.90914</td>\n",
       "      <td>0.110410</td>\n",
       "      <td>13.5630</td>\n",
       "      <td>2.4747</td>\n",
       "      <td>131.4200</td>\n",
       "      <td>2.777400</td>\n",
       "      <td>10.44000</td>\n",
       "      <td>b'0'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.292640</td>\n",
       "      <td>0.292520</td>\n",
       "      <td>0.344150</td>\n",
       "      <td>2.32700</td>\n",
       "      <td>1.080700e+02</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.361310</td>\n",
       "      <td>2.418500</td>\n",
       "      <td>1.25120</td>\n",
       "      <td>0.707480</td>\n",
       "      <td>...</td>\n",
       "      <td>0.286920</td>\n",
       "      <td>0.413650</td>\n",
       "      <td>0.71408</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.3400</td>\n",
       "      <td>2.4188</td>\n",
       "      <td>75.6530</td>\n",
       "      <td>4.824700</td>\n",
       "      <td>3.15560</td>\n",
       "      <td>b'0'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.079997</td>\n",
       "      <td>0.239670</td>\n",
       "      <td>0.321240</td>\n",
       "      <td>2.47390</td>\n",
       "      <td>8.108700e+00</td>\n",
       "      <td>0.207430</td>\n",
       "      <td>0.069715</td>\n",
       "      <td>2.546600</td>\n",
       "      <td>1.05560</td>\n",
       "      <td>0.610340</td>\n",
       "      <td>...</td>\n",
       "      <td>0.052697</td>\n",
       "      <td>0.131070</td>\n",
       "      <td>0.94730</td>\n",
       "      <td>0.035588</td>\n",
       "      <td>6.1579</td>\n",
       "      <td>10.2190</td>\n",
       "      <td>45.4220</td>\n",
       "      <td>8.035800</td>\n",
       "      <td>3.80070</td>\n",
       "      <td>b'0'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.208170</td>\n",
       "      <td>0.602400</td>\n",
       "      <td>0.479350</td>\n",
       "      <td>2.18830</td>\n",
       "      <td>6.965900e+01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.259110</td>\n",
       "      <td>0.660020</td>\n",
       "      <td>1.69230</td>\n",
       "      <td>0.397600</td>\n",
       "      <td>...</td>\n",
       "      <td>0.182420</td>\n",
       "      <td>0.523570</td>\n",
       "      <td>0.84731</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.3634</td>\n",
       "      <td>2.6362</td>\n",
       "      <td>87.0100</td>\n",
       "      <td>4.194900</td>\n",
       "      <td>14.43400</td>\n",
       "      <td>b'0'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.263450</td>\n",
       "      <td>0.143180</td>\n",
       "      <td>0.619650</td>\n",
       "      <td>5.32770</td>\n",
       "      <td>1.989500e+02</td>\n",
       "      <td>0.559710</td>\n",
       "      <td>0.321420</td>\n",
       "      <td>5.862400</td>\n",
       "      <td>1.34370</td>\n",
       "      <td>0.839400</td>\n",
       "      <td>...</td>\n",
       "      <td>0.255780</td>\n",
       "      <td>0.313850</td>\n",
       "      <td>0.74422</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.1287</td>\n",
       "      <td>4.0062</td>\n",
       "      <td>43.8210</td>\n",
       "      <td>8.329400</td>\n",
       "      <td>5.02860</td>\n",
       "      <td>b'0'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>-0.054623</td>\n",
       "      <td>0.928570</td>\n",
       "      <td>-0.026927</td>\n",
       "      <td>0.94263</td>\n",
       "      <td>-8.772400e+01</td>\n",
       "      <td>-0.054623</td>\n",
       "      <td>-0.054623</td>\n",
       "      <td>0.074796</td>\n",
       "      <td>0.92301</td>\n",
       "      <td>0.069453</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.083417</td>\n",
       "      <td>-0.786480</td>\n",
       "      <td>1.08340</td>\n",
       "      <td>6.611400</td>\n",
       "      <td>4.7055</td>\n",
       "      <td>3.1662</td>\n",
       "      <td>226.7600</td>\n",
       "      <td>1.609600</td>\n",
       "      <td>1.35510</td>\n",
       "      <td>b'0'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>-0.358840</td>\n",
       "      <td>1.117100</td>\n",
       "      <td>-0.465770</td>\n",
       "      <td>0.46115</td>\n",
       "      <td>-1.522100e+02</td>\n",
       "      <td>-1.175200</td>\n",
       "      <td>-0.358840</td>\n",
       "      <td>-0.112060</td>\n",
       "      <td>0.97154</td>\n",
       "      <td>-0.125180</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.029298</td>\n",
       "      <td>2.866600</td>\n",
       "      <td>1.02930</td>\n",
       "      <td>-2.019000</td>\n",
       "      <td>8.9776</td>\n",
       "      <td>11.5070</td>\n",
       "      <td>197.4300</td>\n",
       "      <td>1.848800</td>\n",
       "      <td>2.65720</td>\n",
       "      <td>b'0'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5880</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-1.076400e+06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.25330</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.439610</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.13787</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.8356</td>\n",
       "      <td>NaN</td>\n",
       "      <td>25077.0000</td>\n",
       "      <td>0.014555</td>\n",
       "      <td>NaN</td>\n",
       "      <td>b'1'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5881</th>\n",
       "      <td>-0.118560</td>\n",
       "      <td>0.266680</td>\n",
       "      <td>0.659110</td>\n",
       "      <td>3.71540</td>\n",
       "      <td>-1.307100e+01</td>\n",
       "      <td>-1.109900</td>\n",
       "      <td>-0.118560</td>\n",
       "      <td>2.749800</td>\n",
       "      <td>1.19140</td>\n",
       "      <td>0.733320</td>\n",
       "      <td>...</td>\n",
       "      <td>0.270790</td>\n",
       "      <td>-0.161680</td>\n",
       "      <td>0.72950</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.8075</td>\n",
       "      <td>6.9348</td>\n",
       "      <td>74.3660</td>\n",
       "      <td>4.908100</td>\n",
       "      <td>12.13200</td>\n",
       "      <td>b'1'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5882</th>\n",
       "      <td>-0.377700</td>\n",
       "      <td>5.119600</td>\n",
       "      <td>-4.544700</td>\n",
       "      <td>0.11229</td>\n",
       "      <td>-6.138600e+02</td>\n",
       "      <td>-0.377700</td>\n",
       "      <td>-0.377700</td>\n",
       "      <td>-0.866960</td>\n",
       "      <td>0.98755</td>\n",
       "      <td>-4.438500</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.012610</td>\n",
       "      <td>0.085096</td>\n",
       "      <td>1.01260</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.7382</td>\n",
       "      <td>9.9700</td>\n",
       "      <td>660.1600</td>\n",
       "      <td>0.552900</td>\n",
       "      <td>6.65840</td>\n",
       "      <td>b'1'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5883</th>\n",
       "      <td>-0.365550</td>\n",
       "      <td>0.381560</td>\n",
       "      <td>0.119930</td>\n",
       "      <td>1.32920</td>\n",
       "      <td>-2.163300e+01</td>\n",
       "      <td>-0.365550</td>\n",
       "      <td>-0.367600</td>\n",
       "      <td>1.481700</td>\n",
       "      <td>0.82275</td>\n",
       "      <td>0.565350</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.215440</td>\n",
       "      <td>-0.646600</td>\n",
       "      <td>1.21540</td>\n",
       "      <td>0.030513</td>\n",
       "      <td>8.6829</td>\n",
       "      <td>9.0708</td>\n",
       "      <td>68.5620</td>\n",
       "      <td>5.323700</td>\n",
       "      <td>3.76040</td>\n",
       "      <td>b'1'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5884</th>\n",
       "      <td>-0.907080</td>\n",
       "      <td>0.265180</td>\n",
       "      <td>-0.015367</td>\n",
       "      <td>0.90474</td>\n",
       "      <td>-1.269100e+01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.907080</td>\n",
       "      <td>2.771000</td>\n",
       "      <td>0.67575</td>\n",
       "      <td>0.734820</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.478580</td>\n",
       "      <td>-1.234400</td>\n",
       "      <td>2.25890</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>30.5840</td>\n",
       "      <td>9.7620</td>\n",
       "      <td>87.1300</td>\n",
       "      <td>4.189100</td>\n",
       "      <td>0.79123</td>\n",
       "      <td>b'1'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5885</th>\n",
       "      <td>-2.409900</td>\n",
       "      <td>5.536900</td>\n",
       "      <td>-4.536900</td>\n",
       "      <td>0.18061</td>\n",
       "      <td>-2.395200e+02</td>\n",
       "      <td>-3.722700</td>\n",
       "      <td>-2.409900</td>\n",
       "      <td>-0.819390</td>\n",
       "      <td>5.56310</td>\n",
       "      <td>-4.536900</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.328990</td>\n",
       "      <td>0.531180</td>\n",
       "      <td>1.38560</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.6222</td>\n",
       "      <td>21.3370</td>\n",
       "      <td>363.2800</td>\n",
       "      <td>1.004700</td>\n",
       "      <td>NaN</td>\n",
       "      <td>b'1'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5886</th>\n",
       "      <td>-0.131760</td>\n",
       "      <td>0.852120</td>\n",
       "      <td>-0.140900</td>\n",
       "      <td>0.71140</td>\n",
       "      <td>-4.453700e+01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.131960</td>\n",
       "      <td>0.173550</td>\n",
       "      <td>1.51000</td>\n",
       "      <td>0.147880</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.126080</td>\n",
       "      <td>-0.891000</td>\n",
       "      <td>1.07400</td>\n",
       "      <td>1.090500</td>\n",
       "      <td>29.8170</td>\n",
       "      <td>5.4249</td>\n",
       "      <td>118.0200</td>\n",
       "      <td>3.092800</td>\n",
       "      <td>2.31370</td>\n",
       "      <td>b'1'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5887</th>\n",
       "      <td>0.022228</td>\n",
       "      <td>0.437040</td>\n",
       "      <td>-0.345450</td>\n",
       "      <td>0.20957</td>\n",
       "      <td>-6.259200e+02</td>\n",
       "      <td>0.022228</td>\n",
       "      <td>0.022228</td>\n",
       "      <td>1.127200</td>\n",
       "      <td>1.09980</td>\n",
       "      <td>0.492650</td>\n",
       "      <td>...</td>\n",
       "      <td>0.090725</td>\n",
       "      <td>0.045119</td>\n",
       "      <td>0.90928</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.4112</td>\n",
       "      <td>681.9000</td>\n",
       "      <td>0.535270</td>\n",
       "      <td>0.25750</td>\n",
       "      <td>b'1'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5888</th>\n",
       "      <td>-0.131350</td>\n",
       "      <td>0.912090</td>\n",
       "      <td>-0.046074</td>\n",
       "      <td>0.94943</td>\n",
       "      <td>-9.408900e+01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.139140</td>\n",
       "      <td>0.096391</td>\n",
       "      <td>2.15460</td>\n",
       "      <td>0.087917</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.050162</td>\n",
       "      <td>-1.494000</td>\n",
       "      <td>1.05050</td>\n",
       "      <td>0.010299</td>\n",
       "      <td>4.1145</td>\n",
       "      <td>7.2471</td>\n",
       "      <td>154.3600</td>\n",
       "      <td>2.364700</td>\n",
       "      <td>15.97200</td>\n",
       "      <td>b'1'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5889</th>\n",
       "      <td>0.002669</td>\n",
       "      <td>0.476590</td>\n",
       "      <td>0.238260</td>\n",
       "      <td>1.49990</td>\n",
       "      <td>-1.919200e+01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004038</td>\n",
       "      <td>1.098200</td>\n",
       "      <td>2.82420</td>\n",
       "      <td>0.523410</td>\n",
       "      <td>...</td>\n",
       "      <td>0.401280</td>\n",
       "      <td>0.005099</td>\n",
       "      <td>0.59856</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.2839</td>\n",
       "      <td>8.6686</td>\n",
       "      <td>61.5950</td>\n",
       "      <td>5.925800</td>\n",
       "      <td>9.90450</td>\n",
       "      <td>b'1'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5890</th>\n",
       "      <td>0.097567</td>\n",
       "      <td>0.178560</td>\n",
       "      <td>0.465540</td>\n",
       "      <td>3.60720</td>\n",
       "      <td>4.934900e+01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.097567</td>\n",
       "      <td>4.600200</td>\n",
       "      <td>2.67580</td>\n",
       "      <td>0.821410</td>\n",
       "      <td>...</td>\n",
       "      <td>0.056797</td>\n",
       "      <td>0.118780</td>\n",
       "      <td>0.96327</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>24.1030</td>\n",
       "      <td>8.1366</td>\n",
       "      <td>24.3570</td>\n",
       "      <td>14.986000</td>\n",
       "      <td>7.51850</td>\n",
       "      <td>b'1'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5891</th>\n",
       "      <td>-0.100040</td>\n",
       "      <td>0.204980</td>\n",
       "      <td>0.538520</td>\n",
       "      <td>3.62710</td>\n",
       "      <td>-5.841100e+01</td>\n",
       "      <td>-0.100040</td>\n",
       "      <td>-0.077304</td>\n",
       "      <td>3.723700</td>\n",
       "      <td>0.51043</td>\n",
       "      <td>0.763300</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.959120</td>\n",
       "      <td>-0.131070</td>\n",
       "      <td>1.95910</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.5671</td>\n",
       "      <td>2.3404</td>\n",
       "      <td>270.3100</td>\n",
       "      <td>1.350300</td>\n",
       "      <td>1.07930</td>\n",
       "      <td>b'1'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5892</th>\n",
       "      <td>0.112350</td>\n",
       "      <td>1.739900</td>\n",
       "      <td>-0.514250</td>\n",
       "      <td>0.43445</td>\n",
       "      <td>-8.229700e+01</td>\n",
       "      <td>-0.322010</td>\n",
       "      <td>0.112350</td>\n",
       "      <td>-0.425240</td>\n",
       "      <td>3.41440</td>\n",
       "      <td>-0.739870</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.020766</td>\n",
       "      <td>-0.151850</td>\n",
       "      <td>0.96072</td>\n",
       "      <td>-1.122600</td>\n",
       "      <td>12.9690</td>\n",
       "      <td>32.4040</td>\n",
       "      <td>97.2050</td>\n",
       "      <td>3.754900</td>\n",
       "      <td>5.64380</td>\n",
       "      <td>b'1'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5893</th>\n",
       "      <td>-0.089739</td>\n",
       "      <td>1.330000</td>\n",
       "      <td>-0.314680</td>\n",
       "      <td>0.67454</td>\n",
       "      <td>-1.245300e+02</td>\n",
       "      <td>-1.260700</td>\n",
       "      <td>-0.089739</td>\n",
       "      <td>-0.247990</td>\n",
       "      <td>2.23270</td>\n",
       "      <td>-0.329810</td>\n",
       "      <td>...</td>\n",
       "      <td>0.220300</td>\n",
       "      <td>0.272090</td>\n",
       "      <td>0.79435</td>\n",
       "      <td>-0.428730</td>\n",
       "      <td>4.9425</td>\n",
       "      <td>15.4680</td>\n",
       "      <td>158.0600</td>\n",
       "      <td>2.309200</td>\n",
       "      <td>6.41950</td>\n",
       "      <td>b'1'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5894</th>\n",
       "      <td>-0.043676</td>\n",
       "      <td>1.094700</td>\n",
       "      <td>-0.105800</td>\n",
       "      <td>0.90335</td>\n",
       "      <td>-1.842800e+02</td>\n",
       "      <td>-0.074932</td>\n",
       "      <td>-0.043657</td>\n",
       "      <td>-0.086284</td>\n",
       "      <td>0.79372</td>\n",
       "      <td>-0.094454</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.043822</td>\n",
       "      <td>0.462410</td>\n",
       "      <td>1.04670</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.4797</td>\n",
       "      <td>1.8274</td>\n",
       "      <td>503.4000</td>\n",
       "      <td>0.725070</td>\n",
       "      <td>71.93500</td>\n",
       "      <td>b'1'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5895</th>\n",
       "      <td>-0.254870</td>\n",
       "      <td>0.745120</td>\n",
       "      <td>-0.092301</td>\n",
       "      <td>0.87613</td>\n",
       "      <td>-3.419100e+01</td>\n",
       "      <td>0.129500</td>\n",
       "      <td>-0.254870</td>\n",
       "      <td>0.342070</td>\n",
       "      <td>2.96310</td>\n",
       "      <td>0.254880</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.086068</td>\n",
       "      <td>-0.999940</td>\n",
       "      <td>1.08610</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13.8890</td>\n",
       "      <td>7.0327</td>\n",
       "      <td>91.7860</td>\n",
       "      <td>3.976600</td>\n",
       "      <td>8.53460</td>\n",
       "      <td>b'1'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5896</th>\n",
       "      <td>-0.098161</td>\n",
       "      <td>1.202600</td>\n",
       "      <td>-0.414880</td>\n",
       "      <td>0.59535</td>\n",
       "      <td>-4.642800e+01</td>\n",
       "      <td>-0.382780</td>\n",
       "      <td>-0.087177</td>\n",
       "      <td>-0.168300</td>\n",
       "      <td>3.37090</td>\n",
       "      <td>-0.202390</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.044658</td>\n",
       "      <td>0.485010</td>\n",
       "      <td>1.02520</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>101.4200</td>\n",
       "      <td>6.4053</td>\n",
       "      <td>111.0200</td>\n",
       "      <td>3.287700</td>\n",
       "      <td>8.65230</td>\n",
       "      <td>b'1'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5897</th>\n",
       "      <td>-0.120980</td>\n",
       "      <td>0.648020</td>\n",
       "      <td>0.351980</td>\n",
       "      <td>1.54320</td>\n",
       "      <td>8.746000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.120980</td>\n",
       "      <td>0.543050</td>\n",
       "      <td>8.40550</td>\n",
       "      <td>0.351910</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.022887</td>\n",
       "      <td>-0.343780</td>\n",
       "      <td>1.01460</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>64.6560</td>\n",
       "      <td>10.8470</td>\n",
       "      <td>28.1400</td>\n",
       "      <td>12.971000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>b'1'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5898</th>\n",
       "      <td>-3.046300</td>\n",
       "      <td>6.906900</td>\n",
       "      <td>-5.997000</td>\n",
       "      <td>0.13174</td>\n",
       "      <td>-1.372700e+02</td>\n",
       "      <td>-3.737600</td>\n",
       "      <td>-3.046300</td>\n",
       "      <td>-0.854960</td>\n",
       "      <td>13.47700</td>\n",
       "      <td>-5.905100</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.191050</td>\n",
       "      <td>0.515870</td>\n",
       "      <td>1.20160</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>86.6640</td>\n",
       "      <td>18.4510</td>\n",
       "      <td>187.0600</td>\n",
       "      <td>1.951200</td>\n",
       "      <td>149.58000</td>\n",
       "      <td>b'1'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5899</th>\n",
       "      <td>-0.370470</td>\n",
       "      <td>1.299200</td>\n",
       "      <td>-0.677770</td>\n",
       "      <td>0.47802</td>\n",
       "      <td>-2.070200e+02</td>\n",
       "      <td>-0.476540</td>\n",
       "      <td>-0.370470</td>\n",
       "      <td>-0.230370</td>\n",
       "      <td>0.95927</td>\n",
       "      <td>-0.299290</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.386530</td>\n",
       "      <td>1.237800</td>\n",
       "      <td>1.36420</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.0960</td>\n",
       "      <td>1.7817</td>\n",
       "      <td>494.0600</td>\n",
       "      <td>0.738780</td>\n",
       "      <td>2.52570</td>\n",
       "      <td>b'1'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5900</th>\n",
       "      <td>0.361470</td>\n",
       "      <td>0.255900</td>\n",
       "      <td>0.562760</td>\n",
       "      <td>9.75930</td>\n",
       "      <td>2.059800e+02</td>\n",
       "      <td>-0.325970</td>\n",
       "      <td>0.516340</td>\n",
       "      <td>2.907000</td>\n",
       "      <td>0.59282</td>\n",
       "      <td>0.743910</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.155780</td>\n",
       "      <td>0.485910</td>\n",
       "      <td>0.57244</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.3617</td>\n",
       "      <td>8.7628</td>\n",
       "      <td>39.5570</td>\n",
       "      <td>9.227200</td>\n",
       "      <td>1.59020</td>\n",
       "      <td>b'1'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5901</th>\n",
       "      <td>-0.084381</td>\n",
       "      <td>0.224850</td>\n",
       "      <td>0.097051</td>\n",
       "      <td>1.43160</td>\n",
       "      <td>-1.192200e+02</td>\n",
       "      <td>-0.084381</td>\n",
       "      <td>-0.084381</td>\n",
       "      <td>2.741800</td>\n",
       "      <td>0.76553</td>\n",
       "      <td>0.616500</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.306280</td>\n",
       "      <td>-0.136870</td>\n",
       "      <td>1.30630</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.7854</td>\n",
       "      <td>8.8316</td>\n",
       "      <td>226.3700</td>\n",
       "      <td>1.612400</td>\n",
       "      <td>0.53491</td>\n",
       "      <td>b'1'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5902</th>\n",
       "      <td>0.059436</td>\n",
       "      <td>0.541610</td>\n",
       "      <td>0.376720</td>\n",
       "      <td>2.01920</td>\n",
       "      <td>2.666500e+01</td>\n",
       "      <td>-0.275010</td>\n",
       "      <td>0.059436</td>\n",
       "      <td>0.846500</td>\n",
       "      <td>2.26430</td>\n",
       "      <td>0.458470</td>\n",
       "      <td>...</td>\n",
       "      <td>0.170090</td>\n",
       "      <td>0.129640</td>\n",
       "      <td>0.82727</td>\n",
       "      <td>0.375110</td>\n",
       "      <td>10.6280</td>\n",
       "      <td>4.3093</td>\n",
       "      <td>59.5840</td>\n",
       "      <td>6.125800</td>\n",
       "      <td>8.93020</td>\n",
       "      <td>b'1'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5903</th>\n",
       "      <td>-0.076836</td>\n",
       "      <td>0.511940</td>\n",
       "      <td>0.196600</td>\n",
       "      <td>1.38590</td>\n",
       "      <td>-1.333800e+02</td>\n",
       "      <td>-0.076836</td>\n",
       "      <td>-0.074938</td>\n",
       "      <td>-0.245900</td>\n",
       "      <td>1.03000</td>\n",
       "      <td>-0.125890</td>\n",
       "      <td>...</td>\n",
       "      <td>0.029134</td>\n",
       "      <td>0.610360</td>\n",
       "      <td>0.97087</td>\n",
       "      <td>-0.020175</td>\n",
       "      <td>1.9645</td>\n",
       "      <td>21.3700</td>\n",
       "      <td>155.1000</td>\n",
       "      <td>2.353400</td>\n",
       "      <td>4.07770</td>\n",
       "      <td>b'1'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5904</th>\n",
       "      <td>-0.283380</td>\n",
       "      <td>0.977940</td>\n",
       "      <td>-0.125140</td>\n",
       "      <td>0.64902</td>\n",
       "      <td>-5.612600e+01</td>\n",
       "      <td>-1.329800</td>\n",
       "      <td>-0.283380</td>\n",
       "      <td>0.022514</td>\n",
       "      <td>1.39430</td>\n",
       "      <td>0.022018</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032555</td>\n",
       "      <td>-12.871000</td>\n",
       "      <td>0.97208</td>\n",
       "      <td>24.893000</td>\n",
       "      <td>9.8129</td>\n",
       "      <td>16.6910</td>\n",
       "      <td>93.3380</td>\n",
       "      <td>3.910500</td>\n",
       "      <td>1.81430</td>\n",
       "      <td>b'1'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5905</th>\n",
       "      <td>0.012898</td>\n",
       "      <td>0.706210</td>\n",
       "      <td>0.038857</td>\n",
       "      <td>1.17220</td>\n",
       "      <td>-1.890700e+01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013981</td>\n",
       "      <td>0.416000</td>\n",
       "      <td>1.67680</td>\n",
       "      <td>0.293790</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020169</td>\n",
       "      <td>0.043904</td>\n",
       "      <td>1.01220</td>\n",
       "      <td>1.259400</td>\n",
       "      <td>13.4720</td>\n",
       "      <td>12.4320</td>\n",
       "      <td>49.1170</td>\n",
       "      <td>7.431300</td>\n",
       "      <td>2.27990</td>\n",
       "      <td>b'1'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5906</th>\n",
       "      <td>-0.578050</td>\n",
       "      <td>0.967020</td>\n",
       "      <td>-0.800850</td>\n",
       "      <td>0.16576</td>\n",
       "      <td>-6.736500e+01</td>\n",
       "      <td>-0.578050</td>\n",
       "      <td>-0.578050</td>\n",
       "      <td>-0.403340</td>\n",
       "      <td>0.93979</td>\n",
       "      <td>-0.390040</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.064073</td>\n",
       "      <td>1.482000</td>\n",
       "      <td>1.06410</td>\n",
       "      <td>-0.018084</td>\n",
       "      <td>110.7200</td>\n",
       "      <td>44.7590</td>\n",
       "      <td>81.2200</td>\n",
       "      <td>4.494000</td>\n",
       "      <td>5.13050</td>\n",
       "      <td>b'1'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5907</th>\n",
       "      <td>-0.179050</td>\n",
       "      <td>1.255300</td>\n",
       "      <td>-0.275990</td>\n",
       "      <td>0.74554</td>\n",
       "      <td>-1.204400e+02</td>\n",
       "      <td>-0.179050</td>\n",
       "      <td>-0.154930</td>\n",
       "      <td>-0.260180</td>\n",
       "      <td>1.17490</td>\n",
       "      <td>-0.326590</td>\n",
       "      <td>...</td>\n",
       "      <td>0.148880</td>\n",
       "      <td>0.548240</td>\n",
       "      <td>0.85112</td>\n",
       "      <td>-0.522430</td>\n",
       "      <td>9.8526</td>\n",
       "      <td>3.4892</td>\n",
       "      <td>207.8700</td>\n",
       "      <td>1.755900</td>\n",
       "      <td>9.95270</td>\n",
       "      <td>b'1'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5908</th>\n",
       "      <td>-0.108860</td>\n",
       "      <td>0.743940</td>\n",
       "      <td>0.015449</td>\n",
       "      <td>1.08780</td>\n",
       "      <td>-1.700300e+01</td>\n",
       "      <td>-0.108860</td>\n",
       "      <td>-0.109180</td>\n",
       "      <td>0.125310</td>\n",
       "      <td>0.84516</td>\n",
       "      <td>0.093224</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.183200</td>\n",
       "      <td>-1.167700</td>\n",
       "      <td>1.18320</td>\n",
       "      <td>6.092400</td>\n",
       "      <td>13.8860</td>\n",
       "      <td>6.0769</td>\n",
       "      <td>83.1220</td>\n",
       "      <td>4.391100</td>\n",
       "      <td>0.95575</td>\n",
       "      <td>b'1'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5909</th>\n",
       "      <td>-0.105370</td>\n",
       "      <td>0.536290</td>\n",
       "      <td>-0.045578</td>\n",
       "      <td>0.91478</td>\n",
       "      <td>-5.606800e+01</td>\n",
       "      <td>-0.105370</td>\n",
       "      <td>-0.109940</td>\n",
       "      <td>0.864600</td>\n",
       "      <td>0.95040</td>\n",
       "      <td>0.463670</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.052186</td>\n",
       "      <td>-0.227250</td>\n",
       "      <td>1.05220</td>\n",
       "      <td>0.003196</td>\n",
       "      <td>7.7332</td>\n",
       "      <td>4.7174</td>\n",
       "      <td>136.8500</td>\n",
       "      <td>2.667200</td>\n",
       "      <td>2.79270</td>\n",
       "      <td>b'1'</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>43405 rows × 65 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Attr1     Attr2     Attr3     Attr4         Attr5     Attr6  \\\n",
       "0     0.200550  0.379510  0.396410   2.04720  3.235100e+01  0.388250   \n",
       "1     0.209120  0.499880  0.472250   1.94470  1.478600e+01  0.000000   \n",
       "2     0.248660  0.695920  0.267130   1.55480 -1.152300e+00  0.000000   \n",
       "3     0.081483  0.307340  0.458790   2.49280  5.195200e+01  0.149880   \n",
       "4     0.187320  0.613230  0.229600   1.40630 -7.312800e+00  0.187320   \n",
       "5     0.228220  0.497940  0.359690   1.75020 -4.771700e+01  0.000000   \n",
       "6     0.111090  0.647440  0.289710   1.47050  2.534900e+00  0.000000   \n",
       "7     0.532320  0.027059  0.705540  53.95400  2.995800e+02  0.000000   \n",
       "8     0.009020  0.632020  0.053735   1.12630 -3.784200e+01  0.000000   \n",
       "9     0.124080  0.838370  0.142040   1.16940 -9.188300e+01  0.000000   \n",
       "10    0.240010  0.443550  0.188350   1.44000 -2.116500e+01 -0.931900   \n",
       "11   -0.027117  0.111480  0.119890   2.07540 -3.164300e+01 -0.084883   \n",
       "12    0.266690  0.349940  0.611470   3.02430  4.308700e+01  0.559830   \n",
       "13    0.067731  0.198850  0.081562   2.95760  9.060600e+01  0.212650   \n",
       "14   -0.029182  0.211310  0.452640   7.57460  5.784400e+01  0.010387   \n",
       "15   -0.033801  1.154000 -0.205990   0.82150 -7.445100e+01 -0.104130   \n",
       "16    0.270530  0.299130  0.468700   2.56690  7.339500e+01  0.727930   \n",
       "17    0.028084  0.242310  0.432240   3.01280  4.793500e+01  0.021598   \n",
       "18    0.203930  0.560370  0.134950   1.24080  3.158000e+00  0.000000   \n",
       "19    0.208760  0.496500  0.425480   2.01900  3.893400e+01  0.005436   \n",
       "20    0.111190  0.631740  0.247960   2.00000  5.815400e+01  0.243470   \n",
       "21   -0.305050  1.252300 -0.292220   0.71426 -2.149100e+02 -0.305050   \n",
       "22    0.127090  0.530500  0.380690   1.71980 -2.761800e+01  0.000000   \n",
       "23    0.126240  0.662860  0.219160   1.35530  2.158800e+01  0.020227   \n",
       "24    0.292640  0.292520  0.344150   2.32700  1.080700e+02  0.000000   \n",
       "25    0.079997  0.239670  0.321240   2.47390  8.108700e+00  0.207430   \n",
       "26    0.208170  0.602400  0.479350   2.18830  6.965900e+01  0.000000   \n",
       "27    0.263450  0.143180  0.619650   5.32770  1.989500e+02  0.559710   \n",
       "28   -0.054623  0.928570 -0.026927   0.94263 -8.772400e+01 -0.054623   \n",
       "29   -0.358840  1.117100 -0.465770   0.46115 -1.522100e+02 -1.175200   \n",
       "...        ...       ...       ...       ...           ...       ...   \n",
       "5880       NaN       NaN       NaN   0.00000 -1.076400e+06       NaN   \n",
       "5881 -0.118560  0.266680  0.659110   3.71540 -1.307100e+01 -1.109900   \n",
       "5882 -0.377700  5.119600 -4.544700   0.11229 -6.138600e+02 -0.377700   \n",
       "5883 -0.365550  0.381560  0.119930   1.32920 -2.163300e+01 -0.365550   \n",
       "5884 -0.907080  0.265180 -0.015367   0.90474 -1.269100e+01  0.000000   \n",
       "5885 -2.409900  5.536900 -4.536900   0.18061 -2.395200e+02 -3.722700   \n",
       "5886 -0.131760  0.852120 -0.140900   0.71140 -4.453700e+01  0.000000   \n",
       "5887  0.022228  0.437040 -0.345450   0.20957 -6.259200e+02  0.022228   \n",
       "5888 -0.131350  0.912090 -0.046074   0.94943 -9.408900e+01  0.000000   \n",
       "5889  0.002669  0.476590  0.238260   1.49990 -1.919200e+01  0.000000   \n",
       "5890  0.097567  0.178560  0.465540   3.60720  4.934900e+01  0.000000   \n",
       "5891 -0.100040  0.204980  0.538520   3.62710 -5.841100e+01 -0.100040   \n",
       "5892  0.112350  1.739900 -0.514250   0.43445 -8.229700e+01 -0.322010   \n",
       "5893 -0.089739  1.330000 -0.314680   0.67454 -1.245300e+02 -1.260700   \n",
       "5894 -0.043676  1.094700 -0.105800   0.90335 -1.842800e+02 -0.074932   \n",
       "5895 -0.254870  0.745120 -0.092301   0.87613 -3.419100e+01  0.129500   \n",
       "5896 -0.098161  1.202600 -0.414880   0.59535 -4.642800e+01 -0.382780   \n",
       "5897 -0.120980  0.648020  0.351980   1.54320  8.746000e+00  0.000000   \n",
       "5898 -3.046300  6.906900 -5.997000   0.13174 -1.372700e+02 -3.737600   \n",
       "5899 -0.370470  1.299200 -0.677770   0.47802 -2.070200e+02 -0.476540   \n",
       "5900  0.361470  0.255900  0.562760   9.75930  2.059800e+02 -0.325970   \n",
       "5901 -0.084381  0.224850  0.097051   1.43160 -1.192200e+02 -0.084381   \n",
       "5902  0.059436  0.541610  0.376720   2.01920  2.666500e+01 -0.275010   \n",
       "5903 -0.076836  0.511940  0.196600   1.38590 -1.333800e+02 -0.076836   \n",
       "5904 -0.283380  0.977940 -0.125140   0.64902 -5.612600e+01 -1.329800   \n",
       "5905  0.012898  0.706210  0.038857   1.17220 -1.890700e+01  0.000000   \n",
       "5906 -0.578050  0.967020 -0.800850   0.16576 -6.736500e+01 -0.578050   \n",
       "5907 -0.179050  1.255300 -0.275990   0.74554 -1.204400e+02 -0.179050   \n",
       "5908 -0.108860  0.743940  0.015449   1.08780 -1.700300e+01 -0.108860   \n",
       "5909 -0.105370  0.536290 -0.045578   0.91478 -5.606800e+01 -0.105370   \n",
       "\n",
       "         Attr7      Attr8     Attr9    Attr10  ...      Attr56     Attr57  \\\n",
       "0     0.249760   1.330500   1.13890  0.504940  ...    0.121960   0.397180   \n",
       "1     0.258340   0.996010   1.69960  0.497880  ...    0.121300   0.420020   \n",
       "2     0.309060   0.436950   1.30900  0.304080  ...    0.241140   0.817740   \n",
       "3     0.092704   1.866100   1.05710  0.573530  ...    0.054015   0.142070   \n",
       "4     0.187320   0.630700   1.15590  0.386770  ...    0.134850   0.484310   \n",
       "5     0.281390   1.008300   1.97860  0.502060  ...    0.139320   0.454570   \n",
       "6     0.111090   0.544540   1.73480  0.352560  ...    0.605900   0.315100   \n",
       "7     0.652400  35.957000   0.65273  0.972940  ...    0.086730   0.547130   \n",
       "8     0.014434   0.582230   1.33320  0.367980  ...    0.180110   0.024512   \n",
       "9     0.153280   0.192790   2.11560  0.161630  ...    0.079665   0.767680   \n",
       "10    0.240010   1.254500   4.74470  0.556450  ...    0.353590   0.431320   \n",
       "11   -0.024300   7.674100   0.90732  0.855510  ...   -0.102140  -0.031697   \n",
       "12    0.332070   1.857700   1.12680  0.650060  ...    0.112500   0.410250   \n",
       "13    0.078063   4.029000   1.25700  0.801150  ...    0.204440   0.084542   \n",
       "14   -0.034653   3.732400   1.02410  0.788690  ...    0.023565  -0.037001   \n",
       "15   -0.033801  -0.159000   0.97767 -0.183490  ...   -0.022837   0.184210   \n",
       "16    0.336190   2.231500   1.22140  0.667500  ...    0.181270   0.405290   \n",
       "17    0.039729   3.103700   1.01250  0.752060  ...    0.012367   0.037342   \n",
       "18    0.242910   0.784520   2.27060  0.439630  ...    0.107700   0.463860   \n",
       "19    0.256190   1.014100   2.28270  0.503500  ...    0.112990   0.414620   \n",
       "20    0.139090   0.582940   0.99034  0.368260  ...    0.152540   0.301920   \n",
       "21   -0.305050  -0.253440   0.77037 -0.317380  ...   -0.298090   0.961150   \n",
       "22    0.156110   0.885000   2.13220  0.469500  ...    0.053878   0.270680   \n",
       "23    0.156330   0.508620   1.71300  0.337140  ...    0.089183   0.374450   \n",
       "24    0.361310   2.418500   1.25120  0.707480  ...    0.286920   0.413650   \n",
       "25    0.069715   2.546600   1.05560  0.610340  ...    0.052697   0.131070   \n",
       "26    0.259110   0.660020   1.69230  0.397600  ...    0.182420   0.523570   \n",
       "27    0.321420   5.862400   1.34370  0.839400  ...    0.255780   0.313850   \n",
       "28   -0.054623   0.074796   0.92301  0.069453  ...   -0.083417  -0.786480   \n",
       "29   -0.358840  -0.112060   0.97154 -0.125180  ...   -0.029298   2.866600   \n",
       "...        ...        ...       ...       ...  ...         ...        ...   \n",
       "5880       NaN   0.000000   7.25330       NaN  ...    0.439610        NaN   \n",
       "5881 -0.118560   2.749800   1.19140  0.733320  ...    0.270790  -0.161680   \n",
       "5882 -0.377700  -0.866960   0.98755 -4.438500  ...   -0.012610   0.085096   \n",
       "5883 -0.367600   1.481700   0.82275  0.565350  ...   -0.215440  -0.646600   \n",
       "5884 -0.907080   2.771000   0.67575  0.734820  ...   -0.478580  -1.234400   \n",
       "5885 -2.409900  -0.819390   5.56310 -4.536900  ...   -0.328990   0.531180   \n",
       "5886 -0.131960   0.173550   1.51000  0.147880  ...   -0.126080  -0.891000   \n",
       "5887  0.022228   1.127200   1.09980  0.492650  ...    0.090725   0.045119   \n",
       "5888 -0.139140   0.096391   2.15460  0.087917  ...   -0.050162  -1.494000   \n",
       "5889  0.004038   1.098200   2.82420  0.523410  ...    0.401280   0.005099   \n",
       "5890  0.097567   4.600200   2.67580  0.821410  ...    0.056797   0.118780   \n",
       "5891 -0.077304   3.723700   0.51043  0.763300  ...   -0.959120  -0.131070   \n",
       "5892  0.112350  -0.425240   3.41440 -0.739870  ...   -0.020766  -0.151850   \n",
       "5893 -0.089739  -0.247990   2.23270 -0.329810  ...    0.220300   0.272090   \n",
       "5894 -0.043657  -0.086284   0.79372 -0.094454  ...   -0.043822   0.462410   \n",
       "5895 -0.254870   0.342070   2.96310  0.254880  ...   -0.086068  -0.999940   \n",
       "5896 -0.087177  -0.168300   3.37090 -0.202390  ...   -0.044658   0.485010   \n",
       "5897 -0.120980   0.543050   8.40550  0.351910  ...   -0.022887  -0.343780   \n",
       "5898 -3.046300  -0.854960  13.47700 -5.905100  ...   -0.191050   0.515870   \n",
       "5899 -0.370470  -0.230370   0.95927 -0.299290  ...   -0.386530   1.237800   \n",
       "5900  0.516340   2.907000   0.59282  0.743910  ...   -0.155780   0.485910   \n",
       "5901 -0.084381   2.741800   0.76553  0.616500  ...   -0.306280  -0.136870   \n",
       "5902  0.059436   0.846500   2.26430  0.458470  ...    0.170090   0.129640   \n",
       "5903 -0.074938  -0.245900   1.03000 -0.125890  ...    0.029134   0.610360   \n",
       "5904 -0.283380   0.022514   1.39430  0.022018  ...    0.032555 -12.871000   \n",
       "5905  0.013981   0.416000   1.67680  0.293790  ...    0.020169   0.043904   \n",
       "5906 -0.578050  -0.403340   0.93979 -0.390040  ...   -0.064073   1.482000   \n",
       "5907 -0.154930  -0.260180   1.17490 -0.326590  ...    0.148880   0.548240   \n",
       "5908 -0.109180   0.125310   0.84516  0.093224  ...   -0.183200  -1.167700   \n",
       "5909 -0.109940   0.864600   0.95040  0.463670  ...   -0.052186  -0.227250   \n",
       "\n",
       "       Attr58     Attr59    Attr60   Attr61      Attr62     Attr63     Attr64  \\\n",
       "0     0.87804   0.001924    8.4160   5.1372     82.6580   4.415800    7.42770   \n",
       "1     0.85300   0.000000    4.1486   3.2732    107.3500   3.400000   60.98700   \n",
       "2     0.76599   0.694840    4.9909   3.9510    134.2700   2.718500    5.20780   \n",
       "3     0.94598   0.000000    4.5746   3.6147     86.4350   4.222800    5.54970   \n",
       "4     0.86515   0.124440    6.3985   4.3158    127.2100   2.869200    7.89800   \n",
       "5     0.85891   0.023002    3.4028   8.9949     88.4440   4.126900   12.29900   \n",
       "6     0.40871   0.000000    6.3222   2.9098    129.5500   2.817300   18.35200   \n",
       "7     0.49521   0.013194    9.1300  82.0500      7.4503  48.991000    2.32170   \n",
       "8     0.84165   0.340940    9.9665   4.2382    116.5000   3.133000    2.56030   \n",
       "9     0.92847   0.000000    3.3192   6.4994    144.6300   2.523600  107.67000   \n",
       "10    0.64794   0.000000   16.5710  17.0870     32.9280  11.085000   12.36900   \n",
       "11    1.10210   0.000000    3.6683  13.4610     76.7320   4.756800    0.68991   \n",
       "12    0.88750   0.073630    9.5593   5.6298     38.1680   9.562900   33.41300   \n",
       "13    0.79556   0.196190    8.2122   2.7917     60.2180   6.061300    0.28803   \n",
       "14    0.97644   0.180630    3.4646  11.3380     31.8070  11.475000    1.65110   \n",
       "15    1.02280   0.000000    8.2894   3.9519    182.8100   1.996600   44.29600   \n",
       "16    0.81873   0.000000   11.0660   4.1474     60.2970   6.053400    7.79900   \n",
       "17    0.98763   0.036647   10.0740  12.3780     41.4850   8.798400    5.35230   \n",
       "18    0.89419   0.000000   24.9790  10.7670     90.0800   4.051900    7.45250   \n",
       "19    0.88859   0.046947   12.6730   4.5996     66.7640   5.467000   14.54200   \n",
       "20    0.86275   1.030700    9.7941   2.6129     91.3880   3.994000    1.96520   \n",
       "21    1.29810  -0.723530    2.4756   2.3377    447.8100   0.815070    3.09250   \n",
       "22    0.92918   0.000000    4.0360   5.9864     90.5400   4.031400   23.58700   \n",
       "23    0.90914   0.110410   13.5630   2.4747    131.4200   2.777400   10.44000   \n",
       "24    0.71408   0.000000   15.3400   2.4188     75.6530   4.824700    3.15560   \n",
       "25    0.94730   0.035588    6.1579  10.2190     45.4220   8.035800    3.80070   \n",
       "26    0.84731   0.000000    8.3634   2.6362     87.0100   4.194900   14.43400   \n",
       "27    0.74422   0.000000    8.1287   4.0062     43.8210   8.329400    5.02860   \n",
       "28    1.08340   6.611400    4.7055   3.1662    226.7600   1.609600    1.35510   \n",
       "29    1.02930  -2.019000    8.9776  11.5070    197.4300   1.848800    2.65720   \n",
       "...       ...        ...       ...      ...         ...        ...        ...   \n",
       "5880  0.13787        NaN    4.8356      NaN  25077.0000   0.014555        NaN   \n",
       "5881  0.72950   0.000000    1.8075   6.9348     74.3660   4.908100   12.13200   \n",
       "5882  1.01260   0.000000    9.7382   9.9700    660.1600   0.552900    6.65840   \n",
       "5883  1.21540   0.030513    8.6829   9.0708     68.5620   5.323700    3.76040   \n",
       "5884  2.25890   0.000000   30.5840   9.7620     87.1300   4.189100    0.79123   \n",
       "5885  1.38560   0.000000    7.6222  21.3370    363.2800   1.004700        NaN   \n",
       "5886  1.07400   1.090500   29.8170   5.4249    118.0200   3.092800    2.31370   \n",
       "5887  0.90928   0.000000       NaN   4.4112    681.9000   0.535270    0.25750   \n",
       "5888  1.05050   0.010299    4.1145   7.2471    154.3600   2.364700   15.97200   \n",
       "5889  0.59856   0.000000    7.2839   8.6686     61.5950   5.925800    9.90450   \n",
       "5890  0.96327   0.000000   24.1030   8.1366     24.3570  14.986000    7.51850   \n",
       "5891  1.95910   0.000000    2.5671   2.3404    270.3100   1.350300    1.07930   \n",
       "5892  0.96072  -1.122600   12.9690  32.4040     97.2050   3.754900    5.64380   \n",
       "5893  0.79435  -0.428730    4.9425  15.4680    158.0600   2.309200    6.41950   \n",
       "5894  1.04670   0.000000    2.4797   1.8274    503.4000   0.725070   71.93500   \n",
       "5895  1.08610   0.000000   13.8890   7.0327     91.7860   3.976600    8.53460   \n",
       "5896  1.02520   0.000000  101.4200   6.4053    111.0200   3.287700    8.65230   \n",
       "5897  1.01460   0.000000   64.6560  10.8470     28.1400  12.971000        NaN   \n",
       "5898  1.20160   0.000000   86.6640  18.4510    187.0600   1.951200  149.58000   \n",
       "5899  1.36420   0.000000   12.0960   1.7817    494.0600   0.738780    2.52570   \n",
       "5900  0.57244   0.000000    3.3617   8.7628     39.5570   9.227200    1.59020   \n",
       "5901  1.30630   0.000000    1.7854   8.8316    226.3700   1.612400    0.53491   \n",
       "5902  0.82727   0.375110   10.6280   4.3093     59.5840   6.125800    8.93020   \n",
       "5903  0.97087  -0.020175    1.9645  21.3700    155.1000   2.353400    4.07770   \n",
       "5904  0.97208  24.893000    9.8129  16.6910     93.3380   3.910500    1.81430   \n",
       "5905  1.01220   1.259400   13.4720  12.4320     49.1170   7.431300    2.27990   \n",
       "5906  1.06410  -0.018084  110.7200  44.7590     81.2200   4.494000    5.13050   \n",
       "5907  0.85112  -0.522430    9.8526   3.4892    207.8700   1.755900    9.95270   \n",
       "5908  1.18320   6.092400   13.8860   6.0769     83.1220   4.391100    0.95575   \n",
       "5909  1.05220   0.003196    7.7332   4.7174    136.8500   2.667200    2.79270   \n",
       "\n",
       "      class  \n",
       "0      b'0'  \n",
       "1      b'0'  \n",
       "2      b'0'  \n",
       "3      b'0'  \n",
       "4      b'0'  \n",
       "5      b'0'  \n",
       "6      b'0'  \n",
       "7      b'0'  \n",
       "8      b'0'  \n",
       "9      b'0'  \n",
       "10     b'0'  \n",
       "11     b'0'  \n",
       "12     b'0'  \n",
       "13     b'0'  \n",
       "14     b'0'  \n",
       "15     b'0'  \n",
       "16     b'0'  \n",
       "17     b'0'  \n",
       "18     b'0'  \n",
       "19     b'0'  \n",
       "20     b'0'  \n",
       "21     b'0'  \n",
       "22     b'0'  \n",
       "23     b'0'  \n",
       "24     b'0'  \n",
       "25     b'0'  \n",
       "26     b'0'  \n",
       "27     b'0'  \n",
       "28     b'0'  \n",
       "29     b'0'  \n",
       "...     ...  \n",
       "5880   b'1'  \n",
       "5881   b'1'  \n",
       "5882   b'1'  \n",
       "5883   b'1'  \n",
       "5884   b'1'  \n",
       "5885   b'1'  \n",
       "5886   b'1'  \n",
       "5887   b'1'  \n",
       "5888   b'1'  \n",
       "5889   b'1'  \n",
       "5890   b'1'  \n",
       "5891   b'1'  \n",
       "5892   b'1'  \n",
       "5893   b'1'  \n",
       "5894   b'1'  \n",
       "5895   b'1'  \n",
       "5896   b'1'  \n",
       "5897   b'1'  \n",
       "5898   b'1'  \n",
       "5899   b'1'  \n",
       "5900   b'1'  \n",
       "5901   b'1'  \n",
       "5902   b'1'  \n",
       "5903   b'1'  \n",
       "5904   b'1'  \n",
       "5905   b'1'  \n",
       "5906   b'1'  \n",
       "5907   b'1'  \n",
       "5908   b'1'  \n",
       "5909   b'1'  \n",
       "\n",
       "[43405 rows x 65 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs = [df1, df2, df3, df4, df5]\n",
    "\n",
    "dataset = pd.concat(dfs)\n",
    "print(dataset.shape)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XwmhFT562JZO"
   },
   "source": [
    "##Απουσιάζουσες Τιμές\n",
    "\n",
    "Ξέρουμε ήδη από την περιγραφή πως στο σύνολο δεδομένων μας υπάρχουν αποσιάζουσες τιμές. Παρακάτω εξετάζουμε πόσες είναι αυτές συνολικά αλλά και σε πόσες γραμμές (δείγματα) απουσιάζει τουλάχιστον ένα χαρακτηριστικό. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "colab_type": "code",
    "id": "YCPr-M9lx3nl",
    "outputId": "746cf460-2e0c-467f-f48d-25f21abd58cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sum of missing values is:  41322\n",
      "Number of rows with at least one missing value is:  23438\n",
      "Percentage of missing values: 0.5399838728257114\n"
     ]
    }
   ],
   "source": [
    "summ = pd.isnull(dataset).sum().sum()\n",
    "print(\"Total sum of missing values is: \", summ)\n",
    "\n",
    "rows = dataset.isnull().any(axis=1).sum()\n",
    "print(\"Number of rows with at least one missing value is: \", rows)\n",
    "\n",
    "percentage = rows/len(dataset.index)\n",
    "print(\"Percentage of missing values:\", percentage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G0EBq9nA6iz2"
   },
   "source": [
    "Παρατηρούμε ότι **σε παραπάνω από τα μισά δείγματά μας (54%) λείπει κάποιο χαρακτηριστικό**. Συνεπώς δε μπορούμε να διαγράψουμε τα δείγματα με απουσιάζουσες τιμές, αφού χάνουμε μεγάλο μέρος της πληροφορίας.\n",
    "\n",
    "Αντί για αυτό, θα χρησιμοποιήσουμε τον Simple Imputer για να γεμίσουμε αυτά τα κενά με το **μέσο όρο** των τιμών που δεν απουσιάζουν, για κάθε χαρακτηριστικό. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "colab_type": "code",
    "id": "c6gv_HMRIV_1",
    "outputId": "5b708bf1-390b-4017-f635-75f1d09e6489"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.0055e-01,  3.7951e-01,  3.9641e-01, ...,  4.4158e+00,\n",
       "         7.4277e+00,  0.0000e+00],\n",
       "       [ 2.0912e-01,  4.9988e-01,  4.7225e-01, ...,  3.4000e+00,\n",
       "         6.0987e+01,  0.0000e+00],\n",
       "       [ 2.4866e-01,  6.9592e-01,  2.6713e-01, ...,  2.7185e+00,\n",
       "         5.2078e+00,  0.0000e+00],\n",
       "       ...,\n",
       "       [-1.7905e-01,  1.2553e+00, -2.7599e-01, ...,  1.7559e+00,\n",
       "         9.9527e+00,  1.0000e+00],\n",
       "       [-1.0886e-01,  7.4394e-01,  1.5449e-02, ...,  4.3911e+00,\n",
       "         9.5575e-01,  1.0000e+00],\n",
       "       [-1.0537e-01,  5.3629e-01, -4.5578e-02, ...,  2.6672e+00,\n",
       "         2.7927e+00,  1.0000e+00]])"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "data = dataset.values  #μετατροπή του dataframe σε numpy array\n",
    "imr = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "imr = imr.fit(data)\n",
    "imputed_data = imr.transform(data)\n",
    "imputed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jy1N4vi3-bVh"
   },
   "source": [
    "##Διαχωρισμός features, labels\n",
    "\n",
    "Όπως αναφέραμε τα χαρατκηριστικά βρίσκονται στις πρώτες 64 στήλες και το target στην 65η (τελευταία) στήλη. \n",
    "\n",
    "Για λόγους συμβατότητας με τη scikit-learn, μετατρέπουμε τα features σε float και τα labels σε integer (εφαρμόζουμε και τη μέθοδο flatten έτσι ώστε τα ο πίινακας των labels να είναι μονοδιάστατος). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 252
    },
    "colab_type": "code",
    "id": "r7jW774gQK0X",
    "outputId": "6f3b8da9-ba0d-4c09-a237-58bb3df57de6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.0055e-01  3.7951e-01  3.9641e-01 ...  8.2658e+01  4.4158e+00\n",
      "   7.4277e+00]\n",
      " [ 2.0912e-01  4.9988e-01  4.7225e-01 ...  1.0735e+02  3.4000e+00\n",
      "   6.0987e+01]\n",
      " [ 2.4866e-01  6.9592e-01  2.6713e-01 ...  1.3427e+02  2.7185e+00\n",
      "   5.2078e+00]\n",
      " ...\n",
      " [-1.7905e-01  1.2553e+00 -2.7599e-01 ...  2.0787e+02  1.7559e+00\n",
      "   9.9527e+00]\n",
      " [-1.0886e-01  7.4394e-01  1.5449e-02 ...  8.3122e+01  4.3911e+00\n",
      "   9.5575e-01]\n",
      " [-1.0537e-01  5.3629e-01 -4.5578e-02 ...  1.3685e+02  2.6672e+00\n",
      "   2.7927e+00]]\n",
      "[0 0 0 ... 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "features = imputed_data[:, :-1].astype(float)\n",
    "labels = imputed_data[:, -1:].astype(int).flatten()\n",
    "print(features)\n",
    "\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BalW6XD5DUd1"
   },
   "source": [
    "#Ισορροπία, διαχωρισμός σε train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "1rtcbcOSa5I7",
    "outputId": "784203ff-7de4-48ff-eec1-1d4244d2f3dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frequencies: [41314  2091]\n"
     ]
    }
   ],
   "source": [
    "print(\"frequencies:\", np.bincount(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WQeMM_qODjtb"
   },
   "source": [
    "Βλέπουμε λοιπόν πως πράγματι το dataset μας **δεν** είναι ισορροπημένο. (Αξίζει εδώ να σημειώσουμε πως το label 0 αντιστοιχεί σε μη χρεοκοπημένη εταιρία, ενώ το label 1 αντιστοιχεί σε χρεοκοπία. Αυτό το αντιλαμβανόμαστε επειδή ξέρουμε από την περιγραφή πως οι χρεοκοπημένες εταιρίες είναι λιγότερες από τις μη χρεοκοπημένες.)\n",
    "\n",
    "Η **κλάση 0** (μη χρεοκοπημένη εταιρία) αντιστοιχεί στο 41314/43450=0.951 ή **95%** του συνόλου, ενώ η **κλάση 1** (χρεοκοπία) μόλις στο 2091/43450=0.048 ή **0.05** επί του συνόλου.\n",
    "\n",
    "Χωρίζουμε τα δεδομένα μας σε train και test με το test να αποτελεί το 30% του συνόλου."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hOeo37A8VQZq"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test, train_labels, test_labels = train_test_split(features, labels, test_size=0.30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XAZTDY2fE_J8"
   },
   "source": [
    "#Γ. Baseline Classifiaction\n",
    "\n",
    "Παρακάτω θα εκπαιδεύσουμε τους Dummy Classifiers (για όλες τις στρατηγικές που έχουμε διδαχτεί στο εργαστήριο) καθώς και τους Gaussian Naive Bayes Classifier, KNeighbors Classifier και Multi Layer Perceptron με τις default τιμές τους. Δε θα κάνουμε ακόμα καμία προεπεξεργασία στα δεδομένα μας. Για τον καθένα θα εκτυπώσουμε το classification report (όπου φαίνονται οι μετρικές f1 marco average και f1 micro average) και τον πίνακα σύγχυσης. Τέλος, θα κρατήσουμε την κάθε averaged μετρική σε ένα dictionary για όλα τα μοντέλα, έτσι ώστε να μπορέσουμε να τις απεικονίσουμε με διαγράμματα και να συγκρίνουμε τις επιδόσεις των μοντέλων."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZDYpaGTnOf8O"
   },
   "source": [
    "##Dummy Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1112
    },
    "colab_type": "code",
    "id": "QpTo6uwHVa0Y",
    "outputId": "7f125142-5b5b-44db-b174-00e38546bfc2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dummy Uniform:\n",
      "[[6191 6214]\n",
      " [ 305  312]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.50      0.66     12405\n",
      "           1       0.05      0.51      0.09       617\n",
      "\n",
      "   micro avg       0.50      0.50      0.50     13022\n",
      "   macro avg       0.50      0.50      0.37     13022\n",
      "weighted avg       0.91      0.50      0.63     13022\n",
      "\n",
      "Dummy Constant 0:\n",
      "[[12405     0]\n",
      " [  617     0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.98     12405\n",
      "           1       0.00      0.00      0.00       617\n",
      "\n",
      "   micro avg       0.95      0.95      0.95     13022\n",
      "   macro avg       0.48      0.50      0.49     13022\n",
      "weighted avg       0.91      0.95      0.93     13022\n",
      "\n",
      "Dummy Constant_1:\n",
      "[[    0 12405]\n",
      " [    0   617]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00     12405\n",
      "           1       0.05      1.00      0.09       617\n",
      "\n",
      "   micro avg       0.05      0.05      0.05     13022\n",
      "   macro avg       0.02      0.50      0.05     13022\n",
      "weighted avg       0.00      0.05      0.00     13022\n",
      "\n",
      "Dummy Most Frequent:\n",
      "[[12405     0]\n",
      " [  617     0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.98     12405\n",
      "           1       0.00      0.00      0.00       617\n",
      "\n",
      "   micro avg       0.95      0.95      0.95     13022\n",
      "   macro avg       0.48      0.50      0.49     13022\n",
      "weighted avg       0.91      0.95      0.93     13022\n",
      "\n",
      "Dummy Stratified:\n",
      "[[11803   602]\n",
      " [  586    31]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.95      0.95     12405\n",
      "           1       0.05      0.05      0.05       617\n",
      "\n",
      "   micro avg       0.91      0.91      0.91     13022\n",
      "   macro avg       0.50      0.50      0.50     13022\n",
      "weighted avg       0.91      0.91      0.91     13022\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "uniform = DummyClassifier(strategy=\"uniform\")\n",
    "constant_0 = DummyClassifier(strategy=\"constant\", constant=0)\n",
    "constant_1 = DummyClassifier(strategy=\"constant\", constant=1)\n",
    "most_frequent = DummyClassifier(strategy=\"most_frequent\")\n",
    "stratified = DummyClassifier(strategy=\"stratified\")\n",
    "\n",
    "model = uniform.fit(train, train_labels)\n",
    "model = constant_0.fit(train, train_labels)\n",
    "model = constant_1.fit(train, train_labels)\n",
    "model = most_frequent.fit(train, train_labels)\n",
    "model = stratified.fit(train, train_labels)\n",
    "\n",
    "f1_macro = {}\n",
    "f1_micro = {}\n",
    "\n",
    "print('Dummy Uniform:')\n",
    "pred = uniform.predict(test)\n",
    "f1_micro[\"Uniform\"] = f1_score(test_labels, pred, average = 'micro')\n",
    "f1_macro[\"Uniform\"] = f1_score(test_labels, pred, average = 'macro')\n",
    "print(confusion_matrix(test_labels, pred))\n",
    "print(classification_report(test_labels, pred))\n",
    "\n",
    "print('Dummy Constant 0:')\n",
    "pred = constant_0.predict(test)\n",
    "f1_micro[\"Constant 0\"] = f1_score(test_labels, pred, average = 'micro')\n",
    "f1_macro[\"Constant 0\"] = f1_score(test_labels, pred, average = 'macro')\n",
    "print(confusion_matrix(test_labels, pred))\n",
    "print(classification_report(test_labels, pred))\n",
    "\n",
    "print('Dummy Constant_1:')\n",
    "pred = constant_1.predict(test)\n",
    "f1_micro[\"Constant 1\"] = f1_score(test_labels, pred, average = 'micro')\n",
    "f1_macro[\"Constant 1\"] = f1_score(test_labels, pred, average = 'macro')\n",
    "print(confusion_matrix(test_labels, pred))\n",
    "print(classification_report(test_labels, pred))\n",
    "\n",
    "print('Dummy Most Frequent:')\n",
    "pred = most_frequent.predict(test)\n",
    "f1_micro[\"Most Frequent\"] = f1_score(test_labels, pred, average = 'micro')\n",
    "f1_macro[\"Most Frequent\"] = f1_score(test_labels, pred, average = 'macro')\n",
    "print(confusion_matrix(test_labels, pred))\n",
    "print(classification_report(test_labels, pred))\n",
    "\n",
    "print('Dummy Stratified:')\n",
    "pred = stratified.predict(test)\n",
    "f1_micro[\"Stratified\"] = f1_score(test_labels, pred, average = 'micro')\n",
    "f1_macro[\"Stratified\"] = f1_score(test_labels, pred, average = 'macro')\n",
    "print(confusion_matrix(test_labels, pred))\n",
    "print(classification_report(test_labels, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tqO4PsY0XKbi"
   },
   "source": [
    "##Gaussian Naive Bayes Clasiffier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 218
    },
    "colab_type": "code",
    "id": "FWNQtURIK_Lc",
    "outputId": "57bebb67-d5ba-4d6b-be0c-43bcc8195a92"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GNB:\n",
      "[[  270 12135]\n",
      " [   15   602]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.02      0.04     12405\n",
      "           1       0.05      0.98      0.09       617\n",
      "\n",
      "   micro avg       0.07      0.07      0.07     13022\n",
      "   macro avg       0.50      0.50      0.07     13022\n",
      "weighted avg       0.90      0.07      0.04     13022\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(train, train_labels)\n",
    "\n",
    "pred = gnb.predict(test)\n",
    "f1_micro[\"GNB\"] = f1_score(test_labels, pred, average = 'micro')\n",
    "f1_macro[\"GNB\"] = f1_score(test_labels, pred, average = 'macro')\n",
    "print(\"GNB:\")\n",
    "print(confusion_matrix(test_labels, pred))\n",
    "print(classification_report(test_labels, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mMKnPUvkQoy-"
   },
   "source": [
    "##KNeighbors Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 218
    },
    "colab_type": "code",
    "id": "OBmMK4olfQX9",
    "outputId": "5fc77c36-cb53-494e-91f7-a1d1f83719d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Default:\n",
      "[[12343    62]\n",
      " [  570    47]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98     12405\n",
      "           1       0.43      0.08      0.13       617\n",
      "\n",
      "   micro avg       0.95      0.95      0.95     13022\n",
      "   macro avg       0.69      0.54      0.55     13022\n",
      "weighted avg       0.93      0.95      0.93     13022\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(train, train_labels)\n",
    "pred = knn.predict(test)\n",
    "f1_micro[\"KNN_default\"] = f1_score(test_labels, pred, average = 'micro')\n",
    "f1_macro[\"KNN_default\"] = f1_score(test_labels, pred, average = 'macro')\n",
    "print(\"KNN Default:\")\n",
    "print(confusion_matrix(test_labels, pred))\n",
    "print(classification_report(test_labels, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nv-QFYnERSik"
   },
   "source": [
    "##Multi Layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 218
    },
    "colab_type": "code",
    "id": "2XyAHGpWQsx3",
    "outputId": "33510504-224f-4109-c5bc-1ef08e1accba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP Default:\n",
      "[[11604   801]\n",
      " [  419   198]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.94      0.95     12405\n",
      "           1       0.20      0.32      0.25       617\n",
      "\n",
      "   micro avg       0.91      0.91      0.91     13022\n",
      "   macro avg       0.58      0.63      0.60     13022\n",
      "weighted avg       0.93      0.91      0.92     13022\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "mlp = MLPClassifier()\n",
    "mlp.fit(train, train_labels)\n",
    "pred = mlp.predict(test)\n",
    "f1_micro[\"MLP_default\"] = f1_score(test_labels, pred, average = 'micro')\n",
    "f1_macro[\"MLP_default\"] = f1_score(test_labels, pred, average = 'macro')\n",
    "print(\"MLP Default:\")\n",
    "print(confusion_matrix(test_labels, pred))\n",
    "print(classification_report(test_labels, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mrqoUGKER-dG"
   },
   "source": [
    "##Plots, Σχολιασμός"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RlI0ftYuSFXP"
   },
   "source": [
    "###f1_micro average bar plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 324
    },
    "colab_type": "code",
    "id": "Eq0zTfjCSMxd",
    "outputId": "9f64c349-a0f1-4ee4-b72f-29ef5c3d14cd"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlEAAAEzCAYAAAARqFYSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkiRV+P/NbYk675et+QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff2abad3470>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(1, figsize=(10, 5))\n",
    "plt.bar(range(len(f1_micro)), list(f1_micro.values()), align='center')\n",
    "plt.xticks(range(len(f1_micro)), list(f1_micro.keys()))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tzYENxttSdQ1"
   },
   "source": [
    "###f1_macro average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 320
    },
    "colab_type": "code",
    "id": "xQ-Y8DtySiZu",
    "outputId": "96374b01-8dff-4476-d7a2-f6ea50d1cd34"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlEAAAEvCAYAAABlvJTyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAHOFJREFUeJzt3Xu8XGV97/FPzAZqjgE3shXwWjWv\nnzeqAnqSIrek3rUWjQVBbQRtj0SN51g1ao8ivtR4KCclXmqxRarVesEKIhdjRaiYYjEoRYs/K4Ki\nibqpOTGKyiX7/PGsSSbDvkyePXvPZOfzfr3yysxaa2Z+69lz+T7PembNvLGxMSRJkrR77tXvAiRJ\nkvZEhihJkqQKhihJkqQKhihJkqQKhihJkqQKhihJkqQKQ7P9gKOj2+bUORWGhxewZcvt/S6jr2wD\n2wBsA7ANwDYA2wDmVhuMjCycN9E6R6KmaWhofr9L6DvbwDYA2wBsA7ANwDaAvacNDFGSJEkVDFGS\nJEkVDFGSJEkVDFGSJEkVDFGSJEkVDFGSJEkVujpPVESsBRYDY8CqzLy2bd2DgX8E9gWuy8z/MROF\nSpIkDZIpR6Ii4lhgUWYuAU4D1nVscjZwdmY+Gbg7Ih7S+zIlSZIGSzeH85YBFwJk5o3AcETsDxAR\n9wKOBj7XrF+ZmT+coVolSZIGRjch6mBgtO36aLMMYATYBqyNiKsj4t09rk+SJGkg1fx23ryOyw8E\nzgFuAS6JiGdn5iUT3Xh4eMGcOx38yMjCfpfQd7aBbQC2AdgGYBuAbQB7Rxt0E6I2sXPkCeBQYHNz\n+TbgB5l5E0BEfAl4LDBhiJorP0jYMjKykNHRbf0uo69sA9sAbAOwDcA2gMFug1PXXNHvEnrqvNVL\nZ/wxJguD3RzOWw8sB4iIw4FNmbkNIDPvAr4fEYuabY8AclrVSpIk7QGmHInKzA0RsTEiNgDbgZUR\nsQLYmpmfBV4LnN9MMr8BuHgmC5YkSRoEXc2JyszVHYuub1v3PeApvSxKkiRp0HnGckmSpAqGKEmS\npAqGKEmSpAqGKEmSpAqGKEmSpAqGKEmSpAqGKEmSpAqGKEmSpAqGKEmSpAqGKEmSpAqGKEmSpAqG\nKEmSpAqGKEmSpAqGKEmSpAqGKEmSpAqGKEmSpApD/S5AkqTZcOqaK/pdQk+dt3ppv0vY6zkSJUmS\nVMEQJUmSVMEQJUmSVMEQJUmSVMEQJUmSVMEQJUmSVMEQJUmSVMEQJUmSVMEQJUmSVMEQJUmSVMEQ\nJUmSVMEQJUmSVMEQJUmSVMEQJUmSVMEQJUmSVMEQJUmSVMEQJUmSVGGom40iYi2wGBgDVmXmtW3r\nbgFuBe5uFp2SmT/ubZmSJEmDZcoQFRHHAosyc0lEPBo4D1jSsdkzM/OXM1GgJEnSIOrmcN4y4EKA\nzLwRGI6I/We0KkmSpAHXzeG8g4GNbddHm2W/aFv2wYh4GHA18KbMHOtZhZIkSQOoqzlRHeZ1XH8r\ncDnwc8qI1QuACya68fDwAoaG5lc87OAaGVnY7xL6zjawDWBw2+C5r7uo3yX01MVnP6/fJUxqUJ8H\nc43t3P826CZEbaKMPLUcCmxuXcnMj7QuR8SlwGFMEqK2bLl996scYCMjCxkd3dbvMvrKNrANwDaY\nTYPczj4PZo/tPDttMFlQ62ZO1HpgOUBEHA5sysxtzfUDIuILEbFvs+2xwLemV64kSdLgm3IkKjM3\nRMTGiNgAbAdWRsQKYGtmfrYZfbomIn4NfINJRqEkSZLmiq7mRGXm6o5F17etOwc4p5dFSZIkDbqa\nieXaQ5y65op+l9BT561e2u8S9kg+DyRpZvizL5IkSRUMUZIkSRUMUZIkSRUMUZIkSRUMUZIkSRUM\nUZIkSRUMUZIkSRUMUZIkSRUMUZIkSRUMUZIkSRUMUZIkSRUMUZIkSRUMUZIkSRUMUZIkSRUMUZIk\nSRUMUZIkSRUMUZIkSRUMUZIkSRUMUZIkSRUMUZIkSRUMUZIkSRUMUZIkSRUMUZIkSRUMUZIkSRUM\nUZIkSRUMUZIkSRUMUZIkSRUMUZIkSRUMUZIkSRUMUZIkSRUMUZIkSRUMUZIkSRUMUZIkSRUMUZIk\nSRWGutkoItYCi4ExYFVmXjvONu8GlmTmcT2tUJIkaQBNORIVEccCizJzCXAasG6cbR4DHNP78iRJ\nkgZTN4fzlgEXAmTmjcBwROzfsc3ZwFt6XJskSdLA6iZEHQyMtl0fbZYBEBErgKuAW3pZmCRJ0iDr\nak5Uh3mtCxFxIPAy4A+AB3Zz4+HhBQwNza942ME1MrKw3yXsFQa9nQe9vrnCdh78Nhj0+uYK27n/\nbdBNiNpE28gTcCiwubm8FBgBvgLsBzwiItZm5v+c6M62bLm9stTBNDKykNHRbf0uY68wyO3s82D2\n2M6D3Qa+FmaP7Tw7bTBZUOvmcN56YDlARBwObMrMbQCZeUFmPiYzFwMnANdNFqAkSZLmiilDVGZu\nADZGxAbKN/NWRsSKiDhhxquTJEkaUF3NicrM1R2Lrh9nm1uA46ZfkiRJ0uDzjOWSJEkVDFGSJEkV\nDFGSJEkVDFGSJEkVDFGSJEkVDFGSJEkVDFGSJEkVan47b49w6por+l1CT523emm/S5AkSW0ciZIk\nSapgiJIkSapgiJIkSapgiJIkSapgiJIkSapgiJIkSapgiJIkSapgiJIkSapgiJIkSapgiJIkSapg\niJIkSapgiJIkSapgiJIkSapgiJIkSapgiJIkSapgiJIkSapgiJIkSapgiJIkSapgiJIkSapgiJIk\nSapgiJIkSapgiJIkSapgiJIkSapgiJIkSapgiJIkSapgiJIkSaow1M1GEbEWWAyMAasy89q2da8A\nTgPuBq4HVmbm2AzUKkmSNDCmHImKiGOBRZm5hBKW1rWtWwCcBBydmUcBjwKWzFCtkiRJA6Obw3nL\ngAsBMvNGYDgi9m+u356ZyzLzziZQHQD8ZMaqlSRJGhDdhKiDgdG266PNsh0iYjVwE/CpzPx+78qT\nJEkaTF3Nieowr3NBZq6JiHOASyPi6sz86kQ3Hh5ewNDQ/IqH3buNjCzsdwl9N+htMOj1zRW28+C3\nwaDXN1fYzv1vg25C1CZ2HXk6FNgMEBEHAo/LzH/JzF9HxGXAUcCEIWrLltunUe7ea3R0W79L6LtB\nboORkYUDXd9cYjsPdhv4Wpg9tvPstMFkQa2bw3nrgeUAEXE4sCkzW1XvA5wfEfdprj8ZyPpSJUmS\n9gxTjkRl5oaI2BgRG4DtwMqIWAFszczPRsSZwJcj4i7KKQ4+N6MVS5IkDYCu5kRl5uqORde3rTsf\nOL93JUmSJA0+z1guSZJUwRAlSZJUwRAlSZJUwRAlSZJUwRAlSZJUwRAlSZJUwRAlSZJUwRAlSZJU\nwRAlSZJUwRAlSZJUwRAlSZJUwRAlSZJUwRAlSZJUwRAlSZJUwRAlSZJUwRAlSZJUwRAlSZJUwRAl\nSZJUwRAlSZJUwRAlSZJUwRAlSZJUwRAlSZJUwRAlSZJUwRAlSZJUwRAlSZJUwRAlSZJUwRAlSZJU\nwRAlSZJUwRAlSZJUwRAlSZJUwRAlSZJUwRAlSZJUwRAlSZJUwRAlSZJUYaibjSJiLbAYGANWZea1\nbeuOB94N3A0k8PLM3D4DtUqSJA2MKUeiIuJYYFFmLgFOA9Z1bHIusDwzjwIWAs/oeZWSJEkDppvD\necuACwEy80ZgOCL2b1t/RGb+qLk8CtyvtyVKkiQNnm5C1MGUcNQy2iwDIDN/ARARhwBPAy7tZYGS\nJEmDqKs5UR3mdS6IiPsDFwOnZ+Z/TXbj4eEFDA3Nr3jYvdvIyMJ+l9B3g94Gg17fXGE7D34bDHp9\nc4Xt3P826CZEbaJt5Ak4FNjcutIc2rsMeEtmrp/qzrZsuX13axQwOrqt3yX03SC3wcjIwoGuby6x\nnQe7DXwtzB7beXbaYLKg1s3hvPXAcoCIOBzYlJntVZ8NrM3My6dTpCRJ0p5kypGozNwQERsjYgOw\nHVgZESuArcAXgJcCiyLi5c1NPp6Z585UwZIkSYOgqzlRmbm6Y9H1bZf36105kiRJewbPWC5JklTB\nECVJklTBECVJklTBECVJklTBECVJklTBECVJklTBECVJklTBECVJklTBECVJklTBECVJklTBECVJ\nklTBECVJklTBECVJklTBECVJklTBECVJklTBECVJklTBECVJklTBECVJklTBECVJklTBECVJklTB\nECVJklTBECVJklTBECVJklTBECVJklTBECVJklTBECVJklTBECVJklTBECVJklTBECVJklTBECVJ\nklTBECVJklTBECVJklTBECVJklTBECVJklTBECVJklRhqJuNImItsBgYA1Zl5rVt634H+BvgsZl5\n5IxUKUmSNGCmHImKiGOBRZm5BDgNWNexyVnAN2egNkmSpIHVzeG8ZcCFAJl5IzAcEfu3rX8z8NkZ\nqE2SJGlgdXM472BgY9v10WbZLwAyc1tE3K/bBxweXsDQ0PzdKlIwMrKw3yX03aC3waDXN1fYzoPf\nBoNe31xhO/e/DbqaE9Vh3nQecMuW26dz873W6Oi2fpfQd4PcBiMjCwe6vrnEdh7sNvC1MHts59lp\ng8mCWjeH8zZRRp5aDgU2T7MmSZKkPVo3IWo9sBwgIg4HNmWm8VeSJO3VpgxRmbkB2BgRGyjfzFsZ\nESsi4gSAiPg08IlyMa6MiJNntGJJkqQB0NWcqMxc3bHo+rZ1L+xpRZIkSXsAz1guSZJUwRAlSZJU\nwRAlSZJUwRAlSZJUwRAlSZJUwRAlSZJUwRAlSZJUwRAlSZJUwRAlSZJUoaszlkuS9mynrrmi3yX0\n1Hmrl/a7BMmRKEmSpBqGKEmSpAqGKEmSpAqGKEmSpAqGKEmSpAqGKEmSpAqGKEmSpAqGKEmSpAqe\nbFNzmicYlCTNFEeiJEmSKhiiJEmSKhiiJEmSKhiiJEmSKhiiJEmSKhiiJEmSKhiiJEmSKhiiJEmS\nKhiiJEmSKhiiJEmSKhiiJEmSKhiiJEmSKhiiJEmSKhiiJEmSKhiiJEmSKgx1s1FErAUWA2PAqsy8\ntm3dHwDvAu4GLs3Md8xEoZIkSYNkypGoiDgWWJSZS4DTgHUdm6wDXgAcBTwtIh7T8yolSZIGTDeH\n85YBFwJk5o3AcETsDxARDwd+npm3ZuZ24NJme0mSpDmtmxB1MDDadn20WTbeup8Bh/SmNEmSpME1\nb2xsbNINIuJc4JLMvKi5fjVwamZ+NyJ+H3h9Zp7QrHs58PDMfPMM1y1JktRX3YxEbWLnyBPAocDm\nCdY9sFkmSZI0p3UTotYDywEi4nBgU2ZuA8jMW4D9I+JhETEEPKfZXpIkaU6b8nAeQESsAY4BtgMr\ngScCWzPzsxFxDPCeZtPPZOZfzlSxkiRJg6KrECVJkqRdecZySZKkCoYoSZKkCl397MtcFRHHAa/K\nzOVty84AbsvM942z/WrgKuDrwNXAdzLzT2an2noRsQj4K2AEmA9sAP48M3/bg/tenpkX9OJ2EXEA\n8HHgAOCXwMmZ+fPp1tjc9x7RBs3yFwIfBhZn5re6vK+HATcDSzLzmrbl1wLfzswVu1nbCzLzMx3L\nzgBOAX7ctnhNZl6+O/fdSxHxEODgzPy3adzHSuAlwG+BewNvppzz7jeZ+d0u72N5Zl4QEU8ATsjM\nt0XEG4CXAq8EXpyZf9blfd2WmQfV7MtMiohHAv8XeECz6AfA6ZQvFL2D8ssWv2m2PR84o9nuBmAj\n5WfDfodyWpyrZ61waQbt1SFqd2XmGtjxxr3fHhKg5gOfAV6dmVdFxDzKT/W8FXhLDx5iNbBbASIi\n9gX+1zi3ey1wZWaeFRF/Cryx+Tcte1IbND+z9Ezg3yvq+D7wIuCa5r4eCQzv7p00gexFlDbrdM54\nHYw+WgrcB6gKUc2+vgJ4Umbe2YTtvwW+TOksdRWiaJ4DmflN4JvNsmdQwtM3ga/U1Dco2l5DK1sB\nKCLeSHkdrQe2AKvY+SWjdpmZxzW3OQb438DTZ6HsHZq/8wWZeWRz/XnA64CPAX/BxAHwJuCJmfnv\nzboVAJl5fheP+Tjgfa19H2f9PlR0xts7+rvTgZuqo9VcvSAzP99xuzuBrzZX7w18ODM/2OVj/iXw\nrYnaqxnI+BDw5sz8dDf32dzuFuBxwIFMsxM1XYaoCUTEVZQX0OOBb2Tmy5sX1wXAy4BHRMSHKR/8\n5wP3BfYBXpOZ10XEfwLXUd5gXkJ5U34q5RuOfw+soPxo87LMvHsGd+WplBfpVQCZOdb0kLc3+7kK\nOKnZ9sLMfE+zn5uAI4CHUEYfbgD+gXJG+v2AtwGHAY+PiH8C/rjZrwcB/w04IzM/HxFXAl+kfNgd\nBDyXEowOi4gPZObpbbUuA05tLl8M7PJi3kva4Lom6F1ZsZ/XAE+NiPnNc+okyvNvQbOfx1F+LPxO\n4EeUtn5As093U94PXgy8H3hyRLw1M8+c6kGbtroDuF/TBucCD6e8Ht6amVc0P1T+V5RzzN0K/BC4\nkraR4NYITPP7m++jjFxso7xW7ktp2x2vSeBNlA+7OyPih5n5uYo2O4AyOrIvcGdm/mdEvIry9xqN\niJ9RPmgvpYxOfb5pnzspz58XUn5TtPUcWAe8CrgIOBz4UES8GPhYZh4ZEUez829wKyXAbaeMwD4Y\n2PHj7gPmqZQPw/YRpLOAeZT3tw8Ap0fEh6YYPX4Au45kzrqIOAw4k/J+8xwmD4D/AawBnjUDpRzC\nNDrjk3RGJ1PT0draFoL3A66LiMsy8we7XfQ9HQO8f3cCVIdpdaJ6wTlREzuCMqz/JOBZEXHftnWv\no/SuXkZ58V2TmcdTAtXaZpuHA2dm5t811zdn5lMoh5IOzMyjm8uHzfB+PIqdPWMohf86M38bEb9L\n+YA6uvl3YkQ8otlsv8x8OnAO5ZDEYcBBmXkMpRd5YGaeRXmBPZ/SI1ifmcdSPkjf3vaQv8jMZcBl\nwPMpb77ZER5g158R6uVPCO0xbdA6B1ulO4GvAcc3159H+fBv+SBwYlPfFuBkyjngvtg8f1dR2vws\n4KpuAlSbn2fmC5r73Nzc3x9RghPAu4GTMvOplBP2Tua9wJ817bWecloV6HhNNvt7PmV0rCZAkZnX\nU96Ab46I8yPij4EbgcuBNzU93H2AyzLzncD9KSOax1N656d0PAda9/tRynPuZZTDhC3rgOdl5lLg\np5QQ9jRgnyw/8v4xShgdNI+idCJ2yMztbR3A31AO9Y03shsRcWVEXNNs07fT4ETEQcBHKM/F25rF\nHwBOiYgDx7nJRuCXEbG0y/t/UET8a9MJekXb8udHxFcj4qqIOLtZvJamM97c7svNv6tb70ERcVvb\nfVzQdIRou/1hEfGBrna+2NHRaq63OlpdaaY/3ED5fBtXRLw4Im6IiMta20XE/Ij4u7b9W9qE2VOB\nlRFxYkScEhHXNO10bnO7Fc1oFhFxn2YEqmWY0olaFRF/2O0+9JohanxjwPcy8ydZflh5E6XHOp4j\nKT1qMvPrwCOb5b/KzG+3bddKypspvWgob6IT3W+vjFHC2nieSAmAd2XmXZQPhcc361qHH35EqfE7\nwMKI+Cgl/X+i4762AE+KiK9SRgzaPwg676sb87rcrht7ahvU+DTwouZQwo8pc8toPiDGMvPWZrsv\nU/Z9PfDS5o19v/Zh/gmsaj4QW/+OaJa3nt+/D/xR8yFyAXDvpsf84Lb5XVdO8RhPpozgXEkZ5WjN\nwen2NblbMvOlwLGU0PMGyihU5/OvtX8/Bd7VjFS/iN0IPBHxAGAR8E/Nvh1P+ZWHx1Dm6JGZXwN+\nXbsvM2g7bUcuIuKi5u//PZqRTko4OSYiHtpx28zM4zJzMWVE65NRTs482/ahHJL8VGbe2LZ8sgBI\ns/ydUaYBTOU1wCeakZtNUD78KYcMlzYdmAdHxFHs2hk/hNLpPh44jzLXbCoTdUYnM1VHa1LN+8gT\n6AjUbevnUUZalwF/yM7Pw3t0rjLzBnZ2gj5JGb1/RmYeBTyqCVmT2cI0O1G9sLcfzhulHCZoNwL8\nArirY/lEL6CxjnWtD+s7Ora7a4LLvQwL4/kO5fDCDs2Q7CLuWfu+NIe46KgxM2+PiMWUD8kVlGHw\nU9u2OZkyEnN08//X29Z1u7+tnxHaSm9/QmhPaoPp+mfKobDN7DrMP+5+Zua3IuLxlNGQd0fEeZRD\nbRO5x5yoiICdz/c7gHdm5j+Os01Lqy06T1K3T/P/7cDxmbljfZT5HN2+JrvWvOnv13yo3hgR76U8\nXzq19u8c4D2ZeXlE/DnlUEK37gB+3DlHJiJez87nHAxm5/bblIAAQGY+D3bMTblXs2x7lPk672DX\n/aHtdt+JiF9TDl3ePKMV31NQgstrI+KjmfmjtnUfAb42TgCkOcR7HXBiF4/xGEpHBkpn4ZnAYylT\nAr7QvA4OAB7Kroc1fwKsi4i3U0ZYNu7Gfu2uVkfrJ7R1tCZxQNv0gu2ULwbcNsG29wO2ZebPAJoO\nJZT3zKMj4inN9Vbnqt3PgYuaNno0gzkiew+D+GKdTd8FHtQcFyYiRigJ/auT3mpX1za3ofmA7erb\nVLPoi8BDI+K5ABFxL8qx/xMpI2JLImKo6Rn+d3aOku0iyk/+nNzMiXgl5c0Cdj6HDgJubkYJnk/5\nkJ7ILr3aNusphzcAXkA5pNILe1IbTEtm3gH8C2WezsVty7cAY1G+FAFl5OXrEXES8LjMvJDSWz5y\nmrV9jdK7JSLuHxHvapb/OMpcJyi9VCidlUOabX8PWNgsv54yKZuIOCkiWtuPZ7rteBpwbtsowwGU\nv+ctE9zvQcBNTQh/Fjv/xlO+lzZ/A1rtEBGvbvY7Ke1OlB913692Z2bQFZQRlOe2FjSvh4WU+XQA\nZOYllDmBvzfenTQjGYfQn3lR38rM91Pm0n2s7ZAWzWv2DEoAHM+ZlC8P7DPB+pZ57AyQrefEHcDG\nZjTuuMx8YmZ+fJz7/0IzVeDtjG+qx+7WP1M+s06iu/lUW9tqX5qZF0+ybfv+w65t8M62+1nUvFcB\nO+Z3vZ+d0w2+1qxq72j1av97aq8OUZl5J2XC8Llthx9eQxmy79Y5wBERcQVlAuKqXtc5Hc2bw9OB\nP42I1qkZtgJvy/Lbh+dSTtvwFeBvJ5kseDPw4oj4CiWUnNUs/0ZE/BtlmPy5EfEl4FfAjyLirRPc\n12Zg34jonEy4DjiyeYzj2x5jWvakNoiI05rn4hOAD0fERyp2+dOUCepbO5a/Avh4c//7UA5Hfhd4\nX/P8fRvw15Q5QYdHxFp236coc0g2UEJc6zDmXwCfiogvUua7QQlLv2q2fQkluEB5Db25OWS2gglC\nbeNfgTdExCkVtUI5lcTPKKMQV1AmhL+GEkTXjRPg3gtcSGnj9wJ/0ozktZ4DUzmN8nf9CvAUSoC6\njNIzv4rywdbXidfjaUYFnwG8JCKubUYY1lC+JNF5+HE15VBxS7QO/1IOHb2q/QN0tmX5NttNlG/n\nti+fMABm5k8pf/epTlOxIxCz85BZAo+OiPsDRMTbI+KBHbdrhfN5lE5IK5yPRcSCiFjArm0KlR2I\niTpaPfJflJGr+0b59uFRzfKJOlctC4G7MvMnEfFgShvuS1tHi/J66TQjndHd4c++SJpVUb79dlBm\nntHvWrR3iHue4uA+lMPta2DnKQsi4smUD/zfbW56RjbnWGtucxPwxpz4K/sPpXQk/h/lNCVPyszj\nIuL5lC9F/JbSKXg15ZDeBVm+tfkcyoT7Wyjh/FzKlxKeApxA+ZbgvpRO+3HAbcDfUDoi387M1gj+\nVG1wRmauiIhnA6dn5rOjTFZf0Wx2BCUIAfxHZp4eu3nesog4ldIRuoUSsC+nfAP4g5TR+/lNHZfF\nrqdrOJ9y6PP6Zn9PoxwG/BLlkOMlwCsz8+Gx8xQHSyjzT1+fmR/rtsZeMkRJmlWGKElzhSFKkqTd\nEOWcYJ2nRNjamnA/1zUjdv9nnFWfzMy/nu16+skQJUmSVGGvnlguSZJUyxAlSZJUwRAlSZJUwRAl\nSZJUwRAlSZJU4f8DKxMH/OVKfJQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff2982aba20>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(1, figsize=(10, 5))\n",
    "plt.bar(range(len(f1_macro)), list(f1_macro.values()), align='center')\n",
    "plt.xticks(range(len(f1_macro)), list(f1_macro.keys()))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oxeGydHmWSJf"
   },
   "source": [
    "##Σχολιασμός\n",
    "Παρατηρούμε από τα διαγράμματα πως καλύτερο f1 micro average έχουμε για τον Constant 0 (λογικό, αφού η συντριπική πλειοψηφία των δεγμάτων μας ανήκουν σε αυτή την κλάση), ενώ για τη μετρική f1 macro average καλύτερα τα πάει ο KNeighbors Classifier.\n",
    "\n",
    "Για το micro average έχουμε πολύ υψηλές τιμές για όλους σχεδόν τους ταξινομητές άρα συμπεραίνουμε πως έχουμε να κάνουμε με ένα εύκολο dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0nRkFp1WUa2S"
   },
   "source": [
    "#Δ. Βελτιστοποίηση Ταξινομητών"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m419yhV-UjoV"
   },
   "source": [
    "Για καθέναν από τους Classifiers θα προσπαθήσουμε να βρούμε τη βέλτιστη αρχιτεκτονική. Για το λόγο αυτό θα χρειαστεί να κάνουμε την κατάλληλη προεπεξεργασία στα δεδομένα μας.\n",
    "\n",
    "Αρχικά θα κάνουμε **επιλογή χαρακτηριστικών** στο train set με βάση τη διακύμανση των τιμών τους. Στη συνέχεια θα **κανονικοποιήσουμε** και θα **εξισορροπήσουμε** τα δεδομένα έτσι ώστε να έχουμε ίσο αριθμό δειγμάτων για κάθε κλάση. Το επόμενο βήμα είναι να κάνουμε **εξαγωγή νέων χαρακτηριστικών με ανάλυση σε κύριες συνιστώσες**. Τέλος θα δοκιμάσουμε τον **ταξινομητή** μας για τις διάφορες υπερπαραμέτρους του."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dihMjc5bXjcf"
   },
   "source": [
    "##Random Sampling\n",
    "\n",
    "Επειδή το dataset μας είναι πολύ μεγάλο θα επιλέξουμε **τυχαία 1000 δείγματα** έτσι ώστε να κάνουμε Gridsearch με βάση αυτά.\n",
    "Μόλις βρούμε τη βέλτιστη αρχιτεκτονική για την κάθε μετρική για αυτά τα 1000 δείγματα και κάνουμε ένα **πιο στενό GriSearch  σε όλο το training set**. Την πλέον βέλτιστη αρχιτεκτονική θα την εφαρμόσουμε για να κάνουμε fit σε **ολόκληρο** το training set και predict στο test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YY216B9tYwRE"
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "sdata, starget = shuffle(features, labels)\n",
    "samples = 1000\n",
    "sfeatures = sdata[0:samples-1,:]\n",
    "slabels = starget[0:samples-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jRBawZpKbel_"
   },
   "outputs": [],
   "source": [
    "#Χωρίζουμε ξανά τα 1000 samples αυτή τη φορά σε train και test set\n",
    "strain, stest, strain_labels, stest_labels = train_test_split(sfeatures, slabels, test_size=0.30)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "34eI-7ARaNrH"
   },
   "source": [
    "##Variance Threshold\n",
    "\n",
    "Θα βρούμε τα min, max της απόκλισης του κάθε χαρακτηριστικού του train set για να προσαρμόσουμε το threshold ανάλογα"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 353
    },
    "colab_type": "code",
    "id": "SW6c8ATYd4n1",
    "outputId": "dc0bd1f6-ff52-48e1-c616-744e5a8e48e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8.59830975e+00 1.78516019e+01 1.77401074e+01 9.13823970e+02\n",
      " 1.86473219e+08 3.59637028e+00 8.60670450e+00 2.41519067e+03\n",
      " 1.07061731e+02 1.78432213e+01 6.65751556e+00 5.08448377e+00\n",
      " 6.10532690e-01 8.60670450e+00 3.54556969e+09 4.07348141e+00\n",
      " 2.41569946e+03 8.60670450e+00 5.87684755e-01 2.95784778e+03\n",
      " 1.11680109e+00 6.64641476e+00 5.60048797e-01 8.31749443e+00\n",
      " 2.29652329e+01 3.50199440e+00 1.03269420e+08 6.29894755e+02\n",
      " 7.03042597e-01 9.59497702e+00 6.04587199e-01 1.44718240e+08\n",
      " 9.66509497e+01 5.46384192e+01 5.85958539e+00 1.07579706e+02\n",
      " 5.20758553e+04 1.78549672e+01 7.98927738e-02 2.75005720e+02\n",
      " 9.91745235e+01 1.76938892e-01 3.04666384e+04 2.81837204e+04\n",
      " 1.20284480e+06 9.09707290e+02 6.11877349e+03 7.87011174e+00\n",
      " 2.99880674e-01 4.24640377e+02 1.78698149e+01 1.08663037e+03\n",
      " 6.29243422e+02 6.29653209e+02 2.87740464e+10 1.04138522e-01\n",
      " 1.34331973e+01 1.12107626e-01 2.15980416e+00 1.29651001e+08\n",
      " 2.93472337e+04 1.45923784e+05 1.51360987e+02 1.20576356e+04]\n",
      "28774046353.680256\n",
      "0.07989277378106281\n",
      "Max scaled variance is  0.006150340496606415\n",
      "Min scaled variance is  1.950956346496108e-05\n"
     ]
    }
   ],
   "source": [
    "variance = strain.var(axis=0)\n",
    "print(variance)\n",
    "print(np.max(variance))\n",
    "print(np.min(variance))\n",
    "\n",
    "#Τα παρπακάτω μας χρειάζονται για να προσδιορίσουμε το threshold του selector στην περίπτωση που χρησιμοποιήσουμε min-max scaler για το grid search\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "train_scaled = scaler.fit_transform(train)\n",
    "variance = np.var(train_scaled, axis=0)\n",
    "\n",
    "maxx = max(variance)\n",
    "print(\"Max scaled variance is \", maxx)\n",
    "\n",
    "minn  = min(variance)\n",
    "print(\"Min scaled variance is \", minn)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C9bBRYXehlx4"
   },
   "source": [
    "##GridSearch\n",
    "\n",
    "Για το Cross Validation χρησιμοποιούμε 5 folds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fvlAsPEBh6sB"
   },
   "source": [
    "##GNB\n",
    "\n",
    "Θα κάνουμε top down ανάλυση, ξεκινώτας με όλα τα στάδια και αφαιρώντας κάθε φορά ένα, μέχρι να αφήσουμε μόνο τον ταξινομητή."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "7FFbIuNgfnOf",
    "outputId": "8480093e-d3d0-4e6b-d758-72c8ad59c849"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "vthreshold = [0, 300, 30000, 300000]\n",
    "n_components = [3,5,7]\n",
    "\n",
    "selector = VarianceThreshold()\n",
    "scaler = StandardScaler()\n",
    "ros = RandomOverSampler() #για αυτό το dataset θα χρησιμοποιήσουμε μόνο τον RandomOverSampler \n",
    "                          #και όχι τον UnderSampler γιατί εφόσον η μια κλάση μας συναντάται στο 0.05%\n",
    "                          #των δειγμάτων, αν κάνουμε UnderSampling θα χάσουμε πολλά δείγματα, άρα και\n",
    "                          #πολλή πληροφορία\n",
    "pca = PCA()\n",
    "gnb = GaussianNB()\n",
    "\n",
    "pipe = Pipeline(steps=[('selector', selector), ('scaler', scaler), ('ros', ros), ('pca', pca), ('gnb', gnb)])\n",
    "estimator = GridSearchCV(pipe, dict(selector__threshold=vthreshold, pca__n_components=n_components), cv=5, scoring='f1_macro', n_jobs=-1)\n",
    "\n",
    "estimator.fit(strain, strain_labels)\n",
    "preds = estimator.predict(stest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 138
    },
    "colab_type": "code",
    "id": "Chr30RLwoIk8",
    "outputId": "6fd97534-93fb-4ff7-c5d4-57add3a7975b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(memory=None,\n",
      "     steps=[('selector', VarianceThreshold(threshold=300)), ('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('ros', RandomOverSampler(random_state=None, ratio=None, return_indices=False,\n",
      "         sampling_strategy='auto')), ('pca', PCA(copy=True, iterated_power='auto', n_components=3, random_state=None,\n",
      "  svd_solver='auto', tol=0.0, whiten=False)), ('gnb', GaussianNB(priors=None, var_smoothing=1e-09))])\n",
      "{'pca__n_components': 3, 'selector__threshold': 300}\n",
      "Best score:  0.33205183167152996\n"
     ]
    }
   ],
   "source": [
    "print(estimator.best_estimator_)\n",
    "print(estimator.best_params_)\n",
    "print(\"Best score: \", f1_score(stest_labels, preds, average = 'macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uoLnYY4oh9tP"
   },
   "source": [
    "##f1_micro average\n",
    "\n",
    "####Στάδια\n",
    "1. Variance Threshold\n",
    "2. StandardScaler\n",
    "3. RandomOverSampler\n",
    "4. PCA\n",
    "5. GNB\n",
    "\n",
    "###Βέλτιστη επίδοση, παράμετροι\n",
    "Επίδοση: **0.9166666666666666**\n",
    "\n",
    "{'pca__n_components': 3, 'selector__threshold': 0}\n",
    "\n",
    "---\n",
    "####Στάδια\n",
    "1. Variance Threshold\n",
    "2. StandardScaler\n",
    "3. RandomOverSampler\n",
    "5. GNB\n",
    "\n",
    "###Βέλτιστη επίδοση, παράμετροι\n",
    "Επίδοση: 0.8733333333333333\n",
    "\n",
    "{ 'selector__threshold': 0}\n",
    "\n",
    "---\n",
    "####Στάδια\n",
    "1. Variance Threshold\n",
    "2. StandardScaler\n",
    "5. GNB\n",
    "\n",
    "###Βέλτιστη επίδοση, παράμετροι\n",
    "Επίδοση: 0.89\n",
    "\n",
    "{'selector__threshold': 0}\n",
    "\n",
    "---\n",
    "####Στάδια\n",
    "1. Variance Threshold\n",
    "5. GNB\n",
    "\n",
    "###Βέλτιστη επίδοση, παράμετροι\n",
    "Επίδοση: 0.5666666666666667\n",
    "\n",
    "{'selector__threshold': 0}\n",
    "\n",
    "---\n",
    "Εδώ δε θα πάρουμε την περίπτωση του να έχουμε μόνο τον ταξινομητή, αφού αυτός δεν έχει παραμέτρους που χρειάζονται βελτιστοποίηση .\n",
    "\n",
    "---\n",
    "---\n",
    "Θα δούμε τώρα τι γίνεται σε περίπτωση που χρησιμοποιήσουμε MinMaxScaler\n",
    "\n",
    "\n",
    "####Στάδια\n",
    "1. Variance Threshold\n",
    "2. StandardScaler\n",
    "3. RandomOverSampler\n",
    "4.PCA\n",
    "5.GNB\n",
    "\n",
    "###Βέλτιστη επίδοση, παράμετροι\n",
    "Επίδοση: 0.91\n",
    "\n",
    "{'pca__n_components': 3, 'selector__threshold': 0.0001}\n",
    "\n",
    "---\n",
    "####Στάδια\n",
    "1. MinMaxScaler\n",
    "1. Variance Threshold\n",
    "3. RandomOverSampler\n",
    "5. GNB\n",
    "\n",
    "###Βέλτιστη επίδοση, παράμετροι\n",
    "Επίδοση: 0.88\n",
    "\n",
    "{'selector__threshold': 0.001}\n",
    "\n",
    "---\n",
    "####Στάδια\n",
    "1. MinMaxScaler\n",
    "1. Variance Threshold\n",
    "5. GNB\n",
    "\n",
    "###Βέλτιστη επίδοση, παράμετροι\n",
    "Επίδοση: 0.89\n",
    "\n",
    "{'selector__threshold': 0}\n",
    "\n",
    "---\n",
    "####Στάδια\n",
    "1. MinMaxScaler\n",
    "5. GNB\n",
    "\n",
    "###Βέλτιστη επίδοση, παράμετροι\n",
    "Επίδοση: 0.89"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZWcj1iioGgfk"
   },
   "source": [
    "##Βέλτιστη Αρχιτεκτονική\n",
    "##Optimized GridSearch\n",
    "\n",
    "####Στάδια\n",
    "1. Variance Threshold\n",
    "2. StandardScaler\n",
    "3. RandomOverSampler\n",
    "4. PCA\n",
    "5. GNB\n",
    "\n",
    "###Βέλτιστη επίδοση, παράμετροι\n",
    "Επίδοση: 0.9166666666666666\n",
    "\n",
    "{'pca__n_components': 3, 'selector__threshold': 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fJGX9HNPSTP-"
   },
   "outputs": [],
   "source": [
    "vthreshold = [0, 0.0001, 0.01, 1]\n",
    "n_components = [1,2,3,4]\n",
    "\n",
    "selector = VarianceThreshold()\n",
    "scaler = StandardScaler()\n",
    "ros = RandomOverSampler()\n",
    "pca = PCA()\n",
    "gnb = GaussianNB()\n",
    "\n",
    "pipe = Pipeline(steps=[('selector', selector), ('scaler', scaler), ('ros', ros), ('pca', pca), ('gnb', gnb)])\n",
    "estimator = GridSearchCV(pipe, dict(selector__threshold=vthreshold, pca__n_components=n_components), cv=5, scoring='f1_micro', n_jobs=-1)\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "times_micro = {}\n",
    "\n",
    "estimator.fit(train, train_labels)\n",
    "preds = estimator.predict(test)\n",
    "\n",
    "end_time = time.time()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 373
    },
    "colab_type": "code",
    "id": "_8poRl9cUapo",
    "outputId": "4d38ea4a-2c95-4a27-87ae-4cfeceb75965"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Συνολικός χρόνος fit και predict: 50.00786256790161 seconds\n",
      "\n",
      "Pipeline(memory=None,\n",
      "     steps=[('selector', VarianceThreshold(threshold=0.0001)), ('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('ros', RandomOverSampler(random_state=None, ratio=None, return_indices=False,\n",
      "         sampling_strategy='auto')), ('pca', PCA(copy=True, iterated_power='auto', n_components=1, random_state=None,\n",
      "  svd_solver='auto', tol=0.0, whiten=False)), ('gnb', GaussianNB(priors=None, var_smoothing=1e-09))])\n",
      "{'pca__n_components': 1, 'selector__threshold': 0.0001}\n",
      "\n",
      " Optimized GNB:\n",
      "[[12385    20]\n",
      " [  613     4]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.98     12405\n",
      "           1       0.17      0.01      0.01       617\n",
      "\n",
      "   micro avg       0.95      0.95      0.95     13022\n",
      "   macro avg       0.56      0.50      0.49     13022\n",
      "weighted avg       0.92      0.95      0.93     13022\n",
      "\n"
     ]
    }
   ],
   "source": [
    "times_micro[\"Optimized GNB\"] = end_time-start_time\n",
    "print(\"Συνολικός χρόνος fit και predict: %s seconds\\n\" %(end_time-start_time))\n",
    "\n",
    "print(estimator.best_estimator_)\n",
    "print(estimator.best_params_)\n",
    "\n",
    "print(\"\\n Optimized GNB:\")\n",
    "print(confusion_matrix(test_labels, preds))\n",
    "print(classification_report(test_labels, preds))\n",
    "\n",
    "f1_micro_opt={}\n",
    "f1_micro_opt[\"GNB_optimized\"] = f1_score(test_labels, preds, average = 'micro')\n",
    "\n",
    "metavoli = {}\n",
    "metavoli['GNB'] = f1_micro_opt['GNB_optimized'] - f1_micro['GNB']\n",
    "\n",
    "best_n = estimator.best_params_['pca__n_components']\n",
    "best_thr = estimator.best_params_['selector__threshold']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BHPhGFeDXdUv"
   },
   "source": [
    "Όπως παρατηρούμε παρακάτω, με 1 μόνο n_component εξηγούμε το 60.8% της διακύμανσης του dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 518
    },
    "colab_type": "code",
    "id": "UKqZWAePWmp_",
    "outputId": "2b255a37-5775-451f-888c-ec1f6fc73448"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.60806761 0.72492757 0.83467708 0.8925854  0.93664769 0.97106003\n",
      " 0.98460409 0.99635576 0.99929347 0.99974854 0.99984454 0.99990659\n",
      " 0.99995457 0.99997726 0.99998485 0.99998851 0.9999919  0.99999457\n",
      " 0.99999615 0.99999738 0.99999811 0.99999864 0.99999913 0.99999931\n",
      " 0.99999949 0.99999962 0.99999973 0.9999998  0.99999987 0.99999991\n",
      " 0.99999994 0.99999996 0.99999997 0.99999998 0.99999999 0.99999999\n",
      " 0.99999999 0.99999999 1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.        ]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAU4AAAE9CAYAAABk5YsGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n4hQRSUiJU0Qkof8PEWNqlnzNxosAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff28ecd69b0>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "kpca_transform = pca.fit_transform(features)\n",
    "explained_variance = np.var(kpca_transform, axis=0)\n",
    "evar = explained_variance / np.sum(explained_variance)\n",
    "\n",
    "#evar = pca.explained_variance_ratio_\n",
    "cum_evar = np.cumsum(evar)\n",
    "print(cum_evar)\n",
    "plt.figure(1, figsize=(5, 5))\n",
    "plt.xlabel(\"Principal Component number\")\n",
    "plt.ylabel('Cumulative Variance')\n",
    "plt.plot(cum_evar, linewidth=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "41awlgNbX9Ac"
   },
   "source": [
    "Θα εφαρμόσουμε την αρχιτεκτονική που βρήκαμε για να κάνουμε fit στους Dummy Classifiers έτσι ώστε να συγκρίνουμε τα αποτελέσματά μας."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qeZow8JOaGW_"
   },
   "outputs": [],
   "source": [
    "selector = VarianceThreshold(best_thr)\n",
    "scaler = StandardScaler()\n",
    "ros = RandomOverSampler()\n",
    "pca = PCA(best_n)\n",
    "gnb = GaussianNB()\n",
    "\n",
    "train_reduced = selector.fit_transform(train)\n",
    "test_reduced = selector.transform(test)\n",
    "\n",
    "train_scaled = scaler.fit_transform(train_reduced)\n",
    "test_scaled = scaler.transform(test_reduced)\n",
    "\n",
    "train_resampled, train_labels_resampled = ros.fit_sample(train_scaled, train_labels)\n",
    "\n",
    "train_pca = pca.fit_transform(train_resampled)\n",
    "test_pca = pca.transform(test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6H6cIGaRdJ6r"
   },
   "outputs": [],
   "source": [
    "uniform = DummyClassifier(strategy=\"uniform\")\n",
    "constant_0 = DummyClassifier(strategy=\"constant\", constant=0)\n",
    "constant_1 = DummyClassifier(strategy=\"constant\", constant=1)\n",
    "most_frequent = DummyClassifier(strategy=\"most_frequent\")\n",
    "stratified = DummyClassifier(strategy=\"stratified\")\n",
    "\n",
    "uniform.fit(train_pca, train_labels_resampled)\n",
    "constant_0.fit(train_pca, train_labels_resampled)\n",
    "constant_1.fit(train_pca, train_labels_resampled)\n",
    "most_frequent.fit(train_pca, train_labels_resampled)\n",
    "stratified.fit(train_pca, train_labels_resampled)\n",
    "\n",
    "\n",
    "#print('Dummy Uniform:')\n",
    "pred = uniform.predict(test_pca)\n",
    "f1_micro_opt[\"Uniform\"] = f1_score(test_labels, pred, average = 'micro')\n",
    "metavoli[\"Uniform\"] = f1_micro_opt[\"Uniform\"] - f1_micro[\"Uniform\"]\n",
    "\n",
    "#print('Dummy Constant 0:')\n",
    "pred = constant_0.predict(test_pca)\n",
    "f1_micro_opt[\"Constant 0\"] = f1_score(test_labels, pred, average = 'micro')\n",
    "metavoli[\"Constant 0\"] = f1_micro_opt[\"Constant 0\"] - f1_micro[\"Constant 0\"]\n",
    "\n",
    "#print('Dummy Constant_1:')\n",
    "pred = constant_1.predict(test_pca)\n",
    "f1_micro_opt[\"Constant 1\"] = f1_score(test_labels, pred, average = 'micro')\n",
    "metavoli[\"Constant 1\"] = f1_micro_opt[\"Constant 1\"] - f1_micro[\"Constant 1\"]\n",
    "\n",
    "#print('Dummy Most Frequent:')\n",
    "pred = most_frequent.predict(test_pca)\n",
    "f1_micro_opt[\"Most Frequent\"] = f1_score(test_labels, pred, average = 'micro')\n",
    "metavoli[\"Most Frequent\"] = f1_micro_opt[\"Most Frequent\"] - f1_micro[\"Most Frequent\"]\n",
    "\n",
    "#print('Dummy Stratified:')\n",
    "pred = stratified.predict(test)\n",
    "f1_micro_opt[\"Stratified\"] = f1_score(test_labels, pred, average = 'micro')\n",
    "metavoli[\"Stratified\"] = f1_micro_opt[\"Stratified\"] - f1_micro[\"Stratified\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "colab_type": "code",
    "id": "fC8Xuy9Ve283",
    "outputId": "594497ea-ba14-4e14-d857-e934ad8e8a17"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAAD8CAYAAABq6S8VAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff28b3b4048>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar(range(len(f1_micro_opt)), list(f1_micro_opt.values()), align='center')\n",
    "plt.xticks(range(len(f1_micro_opt)), list(f1_micro_opt.keys()))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 118
    },
    "colab_type": "code",
    "id": "DDKThzM0fGEq",
    "outputId": "9b25580d-de07-4703-ae37-c38b381cd8d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GNB 0.8844263553985563\n",
      "Uniform 0.0018430348640761096\n",
      "Constant 0 0.0\n",
      "Constant 1 0.0\n",
      "Most Frequent 0.0\n",
      "Stratified -0.4065427737674705\n"
     ]
    }
   ],
   "source": [
    "for header in metavoli.keys():\n",
    "  print(header, metavoli[header])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KJxW_xqbiA4f"
   },
   "source": [
    "###f1_marco average\n",
    "\n",
    "####Στάδια\n",
    "1. Variance Threshold\n",
    "2. StandardScaler\n",
    "3. RandomOverSampler\n",
    "4. PCA\n",
    "5. GNB\n",
    "\n",
    "###Βέλτιστη επίδοση, παράμετροι\n",
    "Επίδοση: 0.5152220283110336\n",
    "\n",
    "{'pca__n_components': 3, 'selector__threshold': 0}\n",
    "\n",
    "---\n",
    "####Στάδια\n",
    "1. Variance Threshold\n",
    "2. StandardScaler\n",
    "3. RandomOverSampler\n",
    "5. GNB\n",
    "\n",
    "###Βέλτιστη επίδοση, παράμετροι\n",
    "Επίδοση: 0.49594354567711585\n",
    "\n",
    "{ 'selector__threshold': 0}\n",
    "\n",
    "--- \n",
    "####Στάδια\n",
    "1. Variance Threshold\n",
    "2. StandardScaler\n",
    "5. GNB\n",
    "\n",
    "###Βέλτιστη επίδοση, παράμετροι\n",
    "Επίδοση: 0.4993678887484197\n",
    "\n",
    "{'selector__threshold': 0}\n",
    "\n",
    "---\n",
    "\n",
    "####Στάδια\n",
    "1. Variance Threshold\n",
    "5. GNB\n",
    "\n",
    "###Βέλτιστη επίδοση, παράμετροι\n",
    "Επίδοση: 0.36868686868686873\n",
    "\n",
    "{'selector__threshold': 0}\n",
    "\n",
    "\n",
    "---\n",
    "---\n",
    "\n",
    "Θα δούμε τώρα τι συμβαίνει αν χρησιμοποιήσουμε MinMaxScaler\n",
    "\n",
    "####Στάδια\n",
    "1. MinMaxScaler\n",
    "1. Variance Threshold\n",
    "3. RandomOverSampler\n",
    "4. PCA\n",
    "5. GNB\n",
    "\n",
    "###Βέλτιστη επίδοση, παράμετροι\n",
    "Επίδοση: **0.5670995670995671**\n",
    "\n",
    "{'pca__n_components': 3, 'selector__threshold': 0}\n",
    "\n",
    "---\n",
    "####Στάδια\n",
    "1. MinMaxScaler\n",
    "1. Variance Threshold\n",
    "3. RandomOverSampler\n",
    "5. GNB\n",
    "\n",
    "###Βέλτιστη επίδοση, παράμετροι\n",
    "Επίδοση: 0.49594354567711585\n",
    "\n",
    "{ 'selector__threshold': 0.0001}\n",
    "\n",
    "--- \n",
    "####Στάδια\n",
    "1. MinMaxScaler\n",
    "1. Variance Threshold\n",
    "5. GNB\n",
    "\n",
    "###Βέλτιστη επίδοση, παράμετροι\n",
    "Επίδοση: 0.4993678887484197\n",
    "\n",
    "{'selector__threshold': 0}\n",
    "\n",
    "---\n",
    "\n",
    "####Στάδια\n",
    "1. MinMaxScaler\n",
    "5. GNB\n",
    "\n",
    "###Βέλτιστη επίδοση, παράμετροι\n",
    "Επίδοση: 0.4993678887484197"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WUEmdCIehhrR"
   },
   "source": [
    "##Βέλτιστη Αρχιτεκτονική\n",
    "####Στάδια\n",
    "1. MinMaxScaler\n",
    "1. Variance Threshold\n",
    "3. RandomOverSampler\n",
    "4. PCA\n",
    "5. GNB\n",
    "\n",
    "###Βέλτιστη επίδοση, παράμετροι\n",
    "Επίδοση: **0.5670995670995671**\n",
    "\n",
    "{'pca__n_components': 3, 'selector__threshold': 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Su3UNSSth4UF"
   },
   "outputs": [],
   "source": [
    "#Progressive GridSearch για ακόμα καλύτερες επιδόσεις\n",
    "\n",
    "vthreshold = [0, 0.000001, 0.00001]\n",
    "n_components = [1,2,3,4]\n",
    "\n",
    "selector = VarianceThreshold()\n",
    "scaler = MinMaxScaler()\n",
    "ros = RandomOverSampler()\n",
    "pca = PCA()\n",
    "gnb = GaussianNB()\n",
    "\n",
    "pipe = Pipeline(steps=[('scaler', scaler), ('selector', selector), ('ros', ros), ('pca', pca), ('gnb', gnb)])\n",
    "estimator = GridSearchCV(pipe, dict(selector__threshold=vthreshold, pca__n_components=n_components), cv=5, scoring='f1_macro', n_jobs=-1)\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "times_macro= {}\n",
    "\n",
    "estimator.fit(train, train_labels)\n",
    "preds = estimator.predict(test)\n",
    "\n",
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 373
    },
    "colab_type": "code",
    "id": "lurJ3ArEixn0",
    "outputId": "3ef07d85-b8f2-4bb3-c9f2-611256bd3e9d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Συνολικός χρόνος fit και predict: 37.076473236083984 seconds\n",
      "\n",
      "Pipeline(memory=None,\n",
      "     steps=[('scaler', MinMaxScaler(copy=True, feature_range=(0, 1))), ('selector', VarianceThreshold(threshold=1e-06)), ('ros', RandomOverSampler(random_state=None, ratio=None, return_indices=False,\n",
      "         sampling_strategy='auto')), ('pca', PCA(copy=True, iterated_power='auto', n_components=2, random_state=None,\n",
      "  svd_solver='auto', tol=0.0, whiten=False)), ('gnb', GaussianNB(priors=None, var_smoothing=1e-09))])\n",
      "{'pca__n_components': 2, 'selector__threshold': 1e-06}\n",
      "\n",
      " Optimized GNB:\n",
      "[[12199   206]\n",
      " [  596    21]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.98      0.97     12405\n",
      "           1       0.09      0.03      0.05       617\n",
      "\n",
      "   micro avg       0.94      0.94      0.94     13022\n",
      "   macro avg       0.52      0.51      0.51     13022\n",
      "weighted avg       0.91      0.94      0.92     13022\n",
      "\n"
     ]
    }
   ],
   "source": [
    "times_macro[\"Optimized GNB\"] = end_time-start_time\n",
    "print(\"Συνολικός χρόνος fit και predict: %s seconds\\n\" %(end_time-start_time))\n",
    "\n",
    "print(estimator.best_estimator_)\n",
    "print(estimator.best_params_)\n",
    "\n",
    "print(\"\\n Optimized GNB:\")\n",
    "print(confusion_matrix(test_labels, preds))\n",
    "print(classification_report(test_labels, preds))\n",
    "\n",
    "f1_macro_opt={}\n",
    "f1_macro_opt[\"GNB_optimized\"] = f1_score(test_labels, preds, average = 'macro')\n",
    "\n",
    "metavoli = {}\n",
    "metavoli['GNB'] = f1_micro_opt['GNB_optimized'] - f1_micro['GNB']\n",
    "\n",
    "best_n = estimator.best_params_['pca__n_components']\n",
    "best_thr = estimator.best_params_['selector__threshold']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TKA1JltQkLOB"
   },
   "outputs": [],
   "source": [
    "selector = VarianceThreshold(best_thr)\n",
    "scaler = MinMaxScaler()\n",
    "ros = RandomOverSampler()\n",
    "pca = PCA(best_n)\n",
    "gnb = GaussianNB()\n",
    "\n",
    "train_scaled = scaler.fit_transform(train)\n",
    "test_scaled = scaler.transform(test)\n",
    "\n",
    "train_reduced = selector.fit_transform(train_scaled)\n",
    "test_reduced = selector.transform(test_scaled)\n",
    "\n",
    "train_resampled, train_labels_resampled = ros.fit_sample(train_reduced, train_labels)\n",
    "\n",
    "train_pca = pca.fit_transform(train_resampled)\n",
    "test_pca = pca.transform(test_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "YbFvh1Npkn3S",
    "outputId": "65383427-080c-4172-f28d-0fed3f2fd043"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "uniform = DummyClassifier(strategy=\"uniform\")\n",
    "constant_0 = DummyClassifier(strategy=\"constant\", constant=0)\n",
    "constant_1 = DummyClassifier(strategy=\"constant\", constant=1)\n",
    "most_frequent = DummyClassifier(strategy=\"most_frequent\")\n",
    "stratified = DummyClassifier(strategy=\"stratified\")\n",
    "\n",
    "uniform.fit(train_pca, train_labels_resampled)\n",
    "constant_0.fit(train_pca, train_labels_resampled)\n",
    "constant_1.fit(train_pca, train_labels_resampled)\n",
    "most_frequent.fit(train_pca, train_labels_resampled)\n",
    "stratified.fit(train_pca, train_labels_resampled)\n",
    "\n",
    "\n",
    "#print('Dummy Uniform:')\n",
    "pred = uniform.predict(test_pca)\n",
    "f1_macro_opt[\"Uniform\"] = f1_score(test_labels, pred, average = 'macro')\n",
    "metavoli[\"Uniform\"] = f1_macro_opt[\"Uniform\"] - f1_macro[\"Uniform\"]\n",
    "\n",
    "#print('Dummy Constant 0:')\n",
    "pred = constant_0.predict(test_pca)\n",
    "f1_macro_opt[\"Constant 0\"] = f1_score(test_labels, pred, average = 'macro')\n",
    "metavoli[\"Constant 0\"] = f1_macro_opt[\"Constant 0\"] - f1_micro[\"Constant 0\"]\n",
    "\n",
    "#print('Dummy Constant_1:')\n",
    "pred = constant_1.predict(test_pca)\n",
    "f1_macro_opt[\"Constant 1\"] = f1_score(test_labels, pred, average = 'macro')\n",
    "metavoli[\"Constant 1\"] = f1_macro_opt[\"Constant 1\"] - f1_micro[\"Constant 1\"]\n",
    "\n",
    "#print('Dummy Most Frequent:')\n",
    "pred = most_frequent.predict(test_pca)\n",
    "f1_macro_opt[\"Most Frequent\"] = f1_score(test_labels, pred, average = 'macro')\n",
    "metavoli[\"Most Frequent\"] = f1_macro_opt[\"Most Frequent\"] - f1_micro[\"Most Frequent\"]\n",
    "\n",
    "#print('Dummy Stratified:')\n",
    "pred = stratified.predict(test)\n",
    "f1_macro_opt[\"Stratified\"] = f1_score(test_labels, pred, average = 'macro')\n",
    "metavoli[\"Stratified\"] = f1_macro_opt[\"Stratified\"] - f1_micro[\"Stratified\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "colab_type": "code",
    "id": "pmM4cadAlKh_",
    "outputId": "b1128719-9461-4a94-ca4e-c0c951a83096"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAAD4CAYAAADxeG0DAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff28b3d3a58>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar(range(len(f1_macro_opt)), list(f1_macro_opt.values()), align='center')\n",
    "plt.xticks(range(len(f1_macro_opt)), list(f1_macro_opt.keys()))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 118
    },
    "colab_type": "code",
    "id": "olPY-3q2lfRp",
    "outputId": "31e8deed-482e-40c6-8d89-15b2b9a31c15"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GNB 0.8844263553985563\n",
      "Uniform 0.004299229544804939\n",
      "Constant 0 -0.46475141761934546\n",
      "Constant 1 -0.0021434339619543705\n",
      "Most Frequent -0.46475141761934546\n",
      "Stratified -0.5402974701814941\n"
     ]
    }
   ],
   "source": [
    "for header in metavoli.keys():\n",
    "  print(header, metavoli[header])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tVSiZ3i5K5bV"
   },
   "source": [
    "##Σχολιασμός\n",
    "\n",
    "Παρατηρούμε πως οι βελτιστοποιημένοι ταξινομητές έχουν τεράστιες μεταβολές στην επίδοση και για τις 2 μετρικές που χρησιμοποιήσαμε. Ακόμα, έχουν την καλύτερη επίδοση από όλους τους default, συνεπώς το μοντέλο μας βελτιστοποιήθηκε στο σύνολο.\n",
    "\n",
    "Οι μεταβολές στους Dummys οφείλονται κυρίως στην τύχη αφού αυτοί επιλέγουν τυχαία μια κλάση και για το λόγο αυτό *δε θα ξανακάνουμε fit* σε αυτούς για τα επόμενα μοντέλα."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "btvWMIoKfNba"
   },
   "source": [
    "##Knn Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "obJhnOtdvb8r"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.preprocessing import StandardScaler # φέρνουμε τον StandarScaler ως transformer που έχει transform() kai όχι ως scale()\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import KFold\n",
    "from statistics import mean\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5CXTh5awdEzg"
   },
   "source": [
    "## Βελτιστοποίηση του kNN ως προς το f1_micro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 322
    },
    "colab_type": "code",
    "id": "BrcjV9U8vrl5",
    "outputId": "2d0e300b-4ec8-40ca-c444-58a6309f9abf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Συνολικός χρόνος fit και predict: 27.269882917404175 seconds\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.97      0.97       290\n",
      "           1       0.00      0.00      0.00        10\n",
      "\n",
      "   micro avg       0.94      0.94      0.94       300\n",
      "   macro avg       0.48      0.49      0.48       300\n",
      "weighted avg       0.93      0.94      0.94       300\n",
      "\n",
      "Best score:  0.9298998569384835\n",
      "Best estimator:  Pipeline(memory='tmp',\n",
      "     steps=[('selector', VarianceThreshold(threshold=10000)), ('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('sampler', RandomOverSampler(random_state=None, ratio=None, return_indices=False,\n",
      "         sampling_strategy='auto')), ('pca', PCA(copy=True, iterated_power='auto', n_compo...ki',\n",
      "           metric_params=None, n_jobs=None, n_neighbors=1, p=2,\n",
      "           weights='uniform'))])\n",
      "Best parameters:  {'knn__metric': 'minkowski', 'knn__n_neighbors': 1, 'knn__weights': 'uniform', 'pca__n_components': 9, 'selector__threshold': 10000}\n"
     ]
    }
   ],
   "source": [
    "#Δοκιμάσαμε κάθε εφικτό συνδυασμό για την αρχιτεκτονική. Χρειαστήκαμε και αρκετό trial and error για να βρούμε καλά εύρη και διαβαθμίσεις\n",
    "#Τα αποτελέσματα βρίσκονται στο markdown που ακολουθεί\n",
    "\n",
    "n_components = [5,9]\n",
    "vthreshold = [0,100,1000,10000]\n",
    "scaler = StandardScaler()\n",
    "ros = RandomOverSampler()\n",
    "\n",
    "neighbors = [1,5,9,19]\n",
    "metrics = ['minkowski','euclidean','manhattan','chebyshev']   \n",
    "weights = ['uniform','distance']\n",
    "\n",
    "pipe = Pipeline(steps=[('selector', selector), ('scaler', scaler), ('sampler', ros),('pca', pca),('knn', knn)], memory = 'tmp')\n",
    "estimator = GridSearchCV(pipe, dict(selector__threshold=vthreshold, pca__n_components=n_components, knn__n_neighbors=neighbors, knn__weights=weights, knn__metric=metrics), cv=5, scoring='f1_micro', n_jobs=-1)\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "estimator.fit(strain, strain_labels)\n",
    "preds = estimator.predict(stest)\n",
    "print(\"Συνολικός χρόνος fit και predict: %s seconds\" % (time.time() - start_time))\n",
    "print(classification_report(stest_labels, preds))\n",
    "\n",
    "print('Best score: ', estimator.best_score_)\n",
    "print('Best estimator: ', estimator.best_estimator_)\n",
    "print('Best parameters: ', estimator.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dAQkKnIkvepT"
   },
   "source": [
    "###Στάδια:\n",
    "1. KNeighbors Classifier\n",
    "\n",
    "###Επίδοση, βέλτιστη παράμετρος:\n",
    "Best f1_micro score is  0.9483793517406963\n",
    "\n",
    "Optimized params are: \n",
    "\n",
    ">neighbours = 13\n",
    "\n",
    ">metric = minkowski\n",
    "\n",
    ">weights = uniform\n",
    "\n",
    "---\n",
    "###Στάδια: \n",
    "1.  Variance Threshold\n",
    "2. KNeighbors Classifier\n",
    "\n",
    "### Επίδοση, βέλτιστες υπερπαράμετροι:\n",
    "Best f1_micro score is  0.9483793517406963\n",
    "\n",
    "Optimized params are: \n",
    "\n",
    ">threshold = 0\n",
    "\n",
    ">neighbours = 13\n",
    "\n",
    ">metric = minkowski\n",
    "\n",
    ">weights = uniform\n",
    "\n",
    "---\n",
    "\n",
    "###Στάδια: \n",
    "1.  Variance Threshold\n",
    "2. Standard Scaler\n",
    "3. KNeighbors Classifier\n",
    "\n",
    "### Επίδοση, βέλτιστες υπερπαράμετροι:\n",
    "Best f1_micro score is  0.9359743897559024\n",
    "\n",
    "Optimized params are: \n",
    "\n",
    ">threshold = 100\n",
    "\n",
    ">neighbours = 1\n",
    "\n",
    ">metric = minkowski\n",
    "\n",
    ">weights = uniform\n",
    "\n",
    "---\n",
    "\n",
    "###Στάδια: \n",
    "1.  Variance Threshold\n",
    "2. Standard Scaler\n",
    "3. Random Oversampler\n",
    "3. KNeighbors Classifier\n",
    "\n",
    "### Επίδοση, βέλτιστες υπερπαράμετροι:\n",
    "Best f1_micro score is** 0.9515806322529011**\n",
    "\n",
    "Optimized params are: \n",
    "\n",
    ">threshold = 100\n",
    "\n",
    ">neighbours = 5\n",
    "\n",
    ">metric = minkowski\n",
    "\n",
    ">weights = uniform\n",
    "\n",
    "---\n",
    "\n",
    "###Στάδια: \n",
    "1.  Variance Threshold\n",
    "2. Standard Scaler\n",
    "3. Random Oversampler\n",
    "4. PCA\n",
    "3. KNeighbors Classifier\n",
    "\n",
    "### Επίδοση, βέλτιστες υπερπαράμετροι:\n",
    "Best f1_micro score is  0.9298998569384835\n",
    "\n",
    "Optimized params are: \n",
    "\n",
    ">threshold = 10000\n",
    "\n",
    ">n_componenents = 9\n",
    "\n",
    ">neighbours = 1\n",
    "\n",
    ">metric = minkowski\n",
    "\n",
    ">weights = uniform\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "---\n",
    "---\n",
    "###Θα χρησιμοποιήσουμε τώρα Min-max Scaler αντί για Standard Scaler\n",
    "\n",
    "###Στάδια:\n",
    "1. MinMax Scaler\n",
    "2. KNeighbors Classifier\n",
    "\n",
    "###Επίδοση, βέλτιστες υπερπαράμετροι:\n",
    "Best f1_micro score is  0.9487795118047219\n",
    "\n",
    "Optimized params are: \n",
    "\n",
    "\n",
    ">neighbours = 9\n",
    "\n",
    ">metric = manhattan\n",
    "\n",
    ">weights = uniform\n",
    "\n",
    "---\n",
    "###Στάδια:\n",
    "1. MinMax Scaler\n",
    "2. Variance Threshold\n",
    "2. KNeighbors Classifier\n",
    "\n",
    "###Επίδοση, βέλτιστες υπερπαράμετροι:\n",
    "Best f1_micro score is  0.9487795118047219\n",
    "\n",
    "Optimized params are: \n",
    "\n",
    "\n",
    ">threshold = 0\n",
    "\n",
    ">neighbours = 9\n",
    "\n",
    ">metric = manhattan\n",
    "\n",
    ">weights = uniform\n",
    "\n",
    "---\n",
    "###Στάδια:\n",
    "1. MinMax Scaler\n",
    "2. Variance Threshold\n",
    "3. Random Oversampler\n",
    "2. KNeighbors Classifier\n",
    "\n",
    "###Επίδοση, βέλτιστες υπερπαράμετροι:\n",
    "Best f1_micro score is 0.9287715086034414\n",
    "\n",
    "Optimized params are:\n",
    "\n",
    "\n",
    ">threshold = 0\n",
    "\n",
    ">neighbours = 1\n",
    "\n",
    ">metric = manhattan\n",
    "\n",
    ">weights = uniform\n",
    "\n",
    "---\n",
    "\n",
    "###Στάδια:\n",
    "1. MinMax Scaler\n",
    "2. Variance Threshold\n",
    "3. Random Oversampler\n",
    "4. PCA\n",
    "2. KNeighbors Classifier\n",
    "\n",
    "###Επίδοση, βέλτιστες υπερπαράμετροι:\n",
    "Best f1_micro score is  0.9195839167833567\n",
    "\n",
    "Optimized params are: \n",
    "\n",
    "\n",
    ">threshold = 0\n",
    "\n",
    ">n_componenents = 9\n",
    "\n",
    ">neighbours = 1\n",
    "\n",
    ">metric = minkowski\n",
    "\n",
    ">weights = uniform\n",
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 322
    },
    "colab_type": "code",
    "id": "qI36LdGtwG1G",
    "outputId": "d353815c-3ce9-42ae-8245-8555076a35b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Συνολικός χρόνος fit και predict: 1.4686989784240723 seconds\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.94      0.96       290\n",
      "           1       0.15      0.30      0.20        10\n",
      "\n",
      "   micro avg       0.92      0.92      0.92       300\n",
      "   macro avg       0.56      0.62      0.58       300\n",
      "weighted avg       0.95      0.92      0.93       300\n",
      "\n",
      "Best score:  0.8912732474964234\n",
      "Best estimator:  Pipeline(memory='tmp',\n",
      "     steps=[('selector', VarianceThreshold(threshold=50)), ('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('sampler', RandomOverSampler(random_state=None, ratio=None, return_indices=False,\n",
      "         sampling_strategy='auto')), ('knn', KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=None, n_neighbors=3, p=2,\n",
      "           weights='uniform'))])\n",
      "Best parameters:  {'knn__metric': 'minkowski', 'knn__n_neighbors': 3, 'knn__weights': 'uniform', 'selector__threshold': 50}\n"
     ]
    }
   ],
   "source": [
    "#Optimized micro\n",
    "\n",
    "vthreshold = [50,100,150]\n",
    "scaler = StandardScaler()\n",
    "ros = RandomOverSampler()\n",
    "\n",
    "neighbors = [3,5,7] \n",
    "metrics = ['minkowski']   \n",
    "weights = ['uniform']\n",
    "\n",
    "pipe = Pipeline(steps=[('selector', selector), ('scaler', scaler), ('sampler', ros), ('knn', knn)], memory = 'tmp')\n",
    "estimator = GridSearchCV(pipe, dict(selector__threshold=vthreshold, knn__n_neighbors=neighbors, knn__weights=weights, knn__metric=metrics), cv=5, scoring='f1_micro', n_jobs=-1)\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "estimator.fit(strain, strain_labels)\n",
    "preds = estimator.predict(stest)\n",
    "print(\"Συνολικός χρόνος fit και predict: %s seconds\" % (time.time() - start_time))\n",
    "print(classification_report(stest_labels, preds))\n",
    "\n",
    "print('Best score: ', estimator.best_score_)\n",
    "print('Best estimator: ', estimator.best_estimator_)\n",
    "print('Best parameters: ', estimator.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 322
    },
    "colab_type": "code",
    "id": "34sb6mj_SSgg",
    "outputId": "ddf425f3-6c7d-45c2-aa26-49273325a892"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Συνολικός χρόνος fit και predict: 201.40760612487793 seconds\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.94      0.95     12405\n",
      "           1       0.22      0.35      0.27       617\n",
      "\n",
      "   micro avg       0.91      0.91      0.91     13022\n",
      "   macro avg       0.59      0.64      0.61     13022\n",
      "weighted avg       0.93      0.91      0.92     13022\n",
      "\n",
      "Best score:  0.9086331172036994\n",
      "Best estimator:  Pipeline(memory='tmp',\n",
      "     steps=[('selector', VarianceThreshold(threshold=50)), ('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('sampler', RandomOverSampler(random_state=None, ratio=None, return_indices=False,\n",
      "         sampling_strategy='auto')), ('knn', KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=None, n_neighbors=3, p=2,\n",
      "           weights='uniform'))])\n",
      "Best parameters:  {'knn__metric': 'minkowski', 'knn__n_neighbors': 3, 'knn__weights': 'uniform', 'selector__threshold': 50}\n"
     ]
    }
   ],
   "source": [
    "#Optimized, fit and test whole set\n",
    "\n",
    "vthreshold = [50]\n",
    "scaler = StandardScaler()\n",
    "ros = RandomOverSampler()\n",
    "\n",
    "neighbors = [3] \n",
    "metrics = ['minkowski']   \n",
    "weights = ['uniform']\n",
    "\n",
    "pipe = Pipeline(steps=[('selector', selector), ('scaler', scaler), ('sampler', ros), ('knn', knn)], memory = 'tmp')\n",
    "estimator = GridSearchCV(pipe, dict(selector__threshold=vthreshold, knn__n_neighbors=neighbors, knn__weights=weights, knn__metric=metrics), cv=5, scoring='f1_micro', n_jobs=-1)\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "estimator.fit(train, train_labels)\n",
    "preds = estimator.predict(test)\n",
    "print(\"Συνολικός χρόνος fit και predict: %s seconds\" % (time.time() - start_time))\n",
    "end_time=time.time()\n",
    "print(classification_report(test_labels, preds))\n",
    "\n",
    "print('Best score: ', estimator.best_score_)\n",
    "print('Best estimator: ', estimator.best_estimator_)\n",
    "print('Best parameters: ', estimator.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "colab_type": "code",
    "id": "BkqbO6znOtpn",
    "outputId": "9d4d11fc-a569-49af-f2bf-371cc23843f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(memory='tmp',\n",
      "     steps=[('selector', VarianceThreshold(threshold=50)), ('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('sampler', RandomOverSampler(random_state=None, ratio=None, return_indices=False,\n",
      "         sampling_strategy='auto')), ('knn', KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=None, n_neighbors=3, p=2,\n",
      "           weights='uniform'))])\n",
      "{'knn__metric': 'minkowski', 'knn__n_neighbors': 3, 'knn__weights': 'uniform', 'selector__threshold': 50}\n",
      "\n",
      " Optimized kNN:\n",
      "[[11641   764]\n",
      " [  401   216]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.94      0.95     12405\n",
      "           1       0.22      0.35      0.27       617\n",
      "\n",
      "   micro avg       0.91      0.91      0.91     13022\n",
      "   macro avg       0.59      0.64      0.61     13022\n",
      "weighted avg       0.93      0.91      0.92     13022\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#times_micro[\"Optimized kNN\"] = end_time-start_time\n",
    "#print(\"Συνολικός χρόνος fit και predict: %s seconds\\n\" %(end_time-start_time))\n",
    "\n",
    "print(estimator.best_estimator_)\n",
    "print(estimator.best_params_)\n",
    "\n",
    "print(\"\\n Optimized kNN:\")\n",
    "print(confusion_matrix(test_labels, preds))\n",
    "print(classification_report(test_labels, preds))\n",
    "\n",
    "f1_micro_opt[\"kNN_optimized\"] = f1_score(test_labels, preds, average = 'micro')\n",
    "\n",
    "f1_micro_opt[\"kNN_optimized\"] = f1_score(test_labels, preds, average = 'micro')\n",
    "metavoli['kNN'] = f1_micro_opt['kNN_optimized'] - f1_micro['KNN_default']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 324
    },
    "colab_type": "code",
    "id": "om1ZJbd3Or_v",
    "outputId": "7983dbee-bae5-47ad-9cb3-9b60f4660a4c"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlEAAAEzCAYAAAARqFYSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkiRJDQxRkiRJDQxRkiRJDQxRkiRJDf4TIEdvt6L6dTIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff2893c1ac8>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(1, figsize=(10, 5))\n",
    "plt.bar(range(len(f1_micro_opt)), list(f1_micro_opt.values()), align='center')\n",
    "plt.xticks(range(len(f1_micro_opt)), list(f1_micro_opt.keys()))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 134
    },
    "colab_type": "code",
    "id": "7mC9YsJ5b3Si",
    "outputId": "10c04acb-dcfc-4ba8-fab6-66094f766bf3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GNB 0.8844263553985563\n",
      "Uniform 0.004299229544804939\n",
      "Constant 0 -0.46475141761934546\n",
      "Constant 1 -0.0021434339619543705\n",
      "Most Frequent -0.46475141761934546\n",
      "Stratified -0.5402974701814941\n",
      "kNN -0.04093073260635849\n"
     ]
    }
   ],
   "source": [
    "for header in metavoli.keys():\n",
    "  print(header, metavoli[header])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6C__2DLhwINO"
   },
   "source": [
    "##Σχολιασμός\n",
    "\n",
    "Αντίθετα από αυτό που περιμέναμε, τύχαμε στις μάλλον σπάνιες περιπτώσεις, όπου η βελτιστοποιημένη kNN κατέληξε να μη βελτιώνει το f1_micro σε σχέση με τις απλές Dummy και τη GNB.\n",
    "\n",
    "Συνεπώς, το μοντέλο μας δεν απέδωσε όπως θα θέλαμε. Ενδεχομένως, με εξαντλητικό grid search να φτάναμε σε καλύτερα αποτελέσματα."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WEgkYvXYc39b"
   },
   "source": [
    "## Βελτιστοποίηση του kNN ως προς το f1_macro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 322
    },
    "colab_type": "code",
    "id": "QLL8Dxn8wcz4",
    "outputId": "b08b661c-51b3-41b0-acf5-ed5983d4aa7a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Συνολικός χρόνος fit και predict: 84.92602586746216 seconds\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.97      0.97       290\n",
      "           1       0.11      0.10      0.11        10\n",
      "\n",
      "   micro avg       0.94      0.94      0.94       300\n",
      "   macro avg       0.54      0.54      0.54       300\n",
      "weighted avg       0.94      0.94      0.94       300\n",
      "\n",
      "Best score:  0.5235613249881307\n",
      "Best estimator:  Pipeline(memory='tmp',\n",
      "     steps=[('selector', VarianceThreshold(threshold=0)), ('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('sampler', RandomOverSampler(random_state=None, ratio=None, return_indices=False,\n",
      "         sampling_strategy='auto')), ('pca', PCA(copy=True, iterated_power='auto', n_component...ki',\n",
      "           metric_params=None, n_jobs=None, n_neighbors=1, p=2,\n",
      "           weights='uniform'))])\n",
      "Best parameters:  {'knn__metric': 'minkowski', 'knn__n_neighbors': 1, 'knn__weights': 'uniform', 'pca__n_components': 7, 'selector__threshold': 0}\n"
     ]
    }
   ],
   "source": [
    "#Δοκιμάσαμε κάθε εφικτό συνδυασμό για την αρχιτεκτονική. Χρειαστήκαμε και αρκετό trial and error για να βρούμε καλά εύρη και διαβαθμίσεις\n",
    "#Τα αποτελέσματα βρίσκονται στο markdown που ακολουθεί\n",
    "\n",
    "n_components = [5,7,9]\n",
    "vthreshold = [0,100,1000,3000,7000,10000]\n",
    "scaler = StandardScaler()\n",
    "ros = RandomOverSampler()\n",
    "\n",
    "neighbors = [1,5,9,13,17,25]\n",
    "metrics = ['minkowski','manhattan','chebyshev','euclidean']   \n",
    "weights = ['uniform','distance']\n",
    "\n",
    "pipe = Pipeline(steps=[('selector', selector), ('scaler', scaler), ('sampler', ros), ('pca', pca), ('knn', knn)], memory = 'tmp')\n",
    "estimator = GridSearchCV(pipe, dict(selector__threshold=vthreshold, pca__n_components=n_components, knn__n_neighbors=neighbors, knn__weights=weights, knn__metric=metrics), cv=5, scoring='f1_macro', n_jobs=-1)\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "estimator.fit(strain, strain_labels)\n",
    "preds = estimator.predict(stest)\n",
    "print(\"Συνολικός χρόνος fit και predict: %s seconds\" % (time.time() - start_time))\n",
    "print(classification_report(stest_labels, preds))\n",
    "\n",
    "print('Best score: ', estimator.best_score_)\n",
    "print('Best estimator: ', estimator.best_estimator_)\n",
    "print('Best parameters: ', estimator.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eYrwfxaww0XX"
   },
   "source": [
    "###Στάδια:\n",
    "1. KNeighbors Classifier\n",
    "\n",
    "###Επίδοση, βέλτιστη παράμετρος:\n",
    "Best f1_macro score is  0.5647052416303147\n",
    "\n",
    "Optimized params are: \n",
    "\n",
    ">neighbours = 1\n",
    "\n",
    ">metric = manhattan\n",
    "\n",
    ">weights = uniform\n",
    "\n",
    "---\n",
    "###Στάδια: \n",
    "1.  Variance Threshold\n",
    "2. KNeighbors Classifier\n",
    "\n",
    "### Επίδοση, βέλτιστες υπερπαράμετροι:\n",
    "Best f1_macro score is  0.5675942652653336\n",
    "\n",
    "Optimized params are: \n",
    "\n",
    ">threshold = 1000\n",
    "\n",
    ">neighbours = 1\n",
    "\n",
    ">metric = manhattan\n",
    "\n",
    ">weights = uniform\n",
    "\n",
    "---\n",
    "\n",
    "###Στάδια: \n",
    "1.  Variance Threshold\n",
    "2. Standard Scaler\n",
    "3. KNeighbors Classifier\n",
    "\n",
    "### Επίδοση, βέλτιστες υπερπαράμετροι:\n",
    "Best f1_macro score is  0.5733605239700765\n",
    "\n",
    "Optimized params are: \n",
    "\n",
    ">threshold = 100\n",
    "\n",
    ">neighbours = 1\n",
    "\n",
    ">metric = manhattan\n",
    "\n",
    ">weights = uniform\n",
    "\n",
    "---\n",
    "\n",
    "###Στάδια: \n",
    "1.  Variance Threshold\n",
    "2. Standard Scaler\n",
    "3. Random Oversampler\n",
    "3. KNeighbors Classifier\n",
    "\n",
    "### Επίδοση, βέλτιστες υπερπαράμετροι:\n",
    "Best f1_macro score is **0.5803269142078952**\n",
    "\n",
    "Optimized params are: \n",
    "\n",
    ">threshold = 3000\n",
    "\n",
    ">neighbours = 5\n",
    "\n",
    ">metric = manhattan\n",
    "\n",
    ">weights = uniform\n",
    "\n",
    "---\n",
    "\n",
    "###Στάδια: \n",
    "1.  Variance Threshold\n",
    "2. Standard Scaler\n",
    "3. Random Oversampler\n",
    "4. PCA\n",
    "3. KNeighbors Classifier\n",
    "\n",
    "### Επίδοση, βέλτιστες υπερπαράμετροι:\n",
    "Best f1_macro score is  0.5235613249881307\n",
    "\n",
    "Optimized params are: \n",
    "\n",
    ">threshold = 0\n",
    "\n",
    ">n_componenents = 7\n",
    "\n",
    ">neighbours = 1\n",
    "\n",
    ">metric = minkowski\n",
    "\n",
    ">weights = uniform\n",
    "\n",
    "---\n",
    "\n",
    "---\n",
    "---\n",
    "###Θα χρησιμοποιήσουμε τώρα Min-max Scaler αντί για Standard Scaler\n",
    "\n",
    "###Στάδια:\n",
    "1. MinMax Scaler\n",
    "2. KNeighbors Classifier\n",
    "\n",
    "###Επίδοση, βέλτιστες υπερπαράμετροι:\n",
    "Best f1_macro score is 0.5633332097675882\n",
    "\n",
    "Optimized params are: \n",
    "\n",
    "\n",
    ">neighbours = 1\n",
    "\n",
    ">metric = manhattan\n",
    "\n",
    ">weights = uniform\n",
    "\n",
    "---\n",
    "###Στάδια:\n",
    "1. MinMax Scaler\n",
    "2. Variance Threshold\n",
    "2. KNeighbors Classifier\n",
    "\n",
    "###Επίδοση, βέλτιστες υπερπαράμετροι:\n",
    "Best f1_macro score is  0.5633332097675882\n",
    "\n",
    "Optimized params are: \n",
    "\n",
    "\n",
    ">threshold = 0\n",
    "\n",
    ">neighbours =  1\n",
    "\n",
    ">metric = manhattan\n",
    "\n",
    ">weights = uniform\n",
    "\n",
    "---\n",
    "###Στάδια:\n",
    "1. MinMax Scaler\n",
    "2. Variance Threshold\n",
    "3. Random Oversampler\n",
    "2. KNeighbors Classifier\n",
    "\n",
    "###Επίδοση, βέλτιστες υπερπαράμετροι:\n",
    "Best f1_macro score is 0.5684968677332359\n",
    "\n",
    "Optimized params are:\n",
    "\n",
    "\n",
    ">threshold = 0\n",
    "\n",
    ">neighbours = 5 \n",
    "\n",
    ">metric = manhattan\n",
    "\n",
    ">weights = uniform\n",
    "\n",
    "---\n",
    "\n",
    "###Στάδια:\n",
    "1. MinMax Scaler\n",
    "2. Variance Threshold\n",
    "3. Random Oversampler\n",
    "4. PCA\n",
    "2. KNeighbors Classifier\n",
    "\n",
    "###Επίδοση, βέλτιστες υπερπαράμετροι:\n",
    "Best f1_macro score is 0.5404186678625426\n",
    "\n",
    "Optimized params are: \n",
    "\n",
    "\n",
    ">threshold = 0\n",
    "\n",
    ">n_componenents = 9 \n",
    "\n",
    ">neighbours = 1\n",
    "\n",
    ">metric = minkowski\n",
    "\n",
    ">weights = uniform\n",
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "colab_type": "code",
    "id": "kV6fh-JLw2lR",
    "outputId": "e409c31d-86c3-4d64-f2be-e509bd52b53d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Συνολικός χρόνος fit και predict: 2.2928311824798584 seconds\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.94      0.96       290\n",
      "           1       0.15      0.30      0.20        10\n",
      "\n",
      "   micro avg       0.92      0.92      0.92       300\n",
      "   macro avg       0.56      0.62      0.58       300\n",
      "weighted avg       0.95      0.92      0.93       300\n",
      "\n",
      "Best score:  0.5248757130039602\n",
      "Best estimator:  Pipeline(memory='tmp',\n",
      "     steps=[('selector', VarianceThreshold(threshold=4000)), ('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('sampler', RandomOverSampler(random_state=None, ratio=None, return_indices=False,\n",
      "         sampling_strategy='auto')), ('knn', KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='manhattan',\n",
      "           metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
      "           weights='uniform'))])\n",
      "Best parameters:  {'knn__metric': 'manhattan', 'knn__n_neighbors': 5, 'knn__weights': 'uniform', 'selector__threshold': 4000}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "#Optimize macro\n",
    "\n",
    "vthreshold = [2500,3000,3500,4000,5000]\n",
    "scaler = StandardScaler()\n",
    "ros = RandomOverSampler()\n",
    "\n",
    "neighbors = [3,5,7]\n",
    "metrics = ['manhattan']   \n",
    "weights = ['uniform']\n",
    "\n",
    "pipe = Pipeline(steps=[('selector', selector), ('scaler', scaler), ('sampler', ros), ('knn', knn)], memory = 'tmp')\n",
    "estimator = GridSearchCV(pipe, dict(selector__threshold=vthreshold, knn__n_neighbors=neighbors, knn__weights=weights, knn__metric=metrics), cv=5, scoring='f1_macro', n_jobs=-1)\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "estimator.fit(strain, strain_labels)\n",
    "preds = estimator.predict(stest)\n",
    "print(\"Συνολικός χρόνος fit και predict: %s seconds\" % (time.time() - start_time))\n",
    "print(classification_report(stest_labels, preds))\n",
    "\n",
    "print('Best score: ', estimator.best_score_)\n",
    "print('Best estimator: ', estimator.best_estimator_)\n",
    "print('Best parameters: ', estimator.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 322
    },
    "colab_type": "code",
    "id": "UXWdl3jLTc82",
    "outputId": "7ae52f36-d2f3-4596-aa22-0da9290ed40b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Συνολικός χρόνος fit και predict: 222.202383518219 seconds\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.92      0.95     12405\n",
      "           1       0.23      0.47      0.31       617\n",
      "\n",
      "   micro avg       0.90      0.90      0.90     13022\n",
      "   macro avg       0.60      0.70      0.63     13022\n",
      "weighted avg       0.94      0.90      0.92     13022\n",
      "\n",
      "Best score:  0.6111518851370161\n",
      "Best estimator:  Pipeline(memory='tmp',\n",
      "     steps=[('selector', VarianceThreshold(threshold=4000)), ('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('sampler', RandomOverSampler(random_state=None, ratio=None, return_indices=False,\n",
      "         sampling_strategy='auto')), ('knn', KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='manhattan',\n",
      "           metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
      "           weights='uniform'))])\n",
      "Best parameters:  {'knn__metric': 'manhattan', 'knn__n_neighbors': 5, 'knn__weights': 'uniform', 'selector__threshold': 4000}\n"
     ]
    }
   ],
   "source": [
    "#Fit and transform whole dataset\n",
    "\n",
    "vthreshold = [4000]\n",
    "scaler = StandardScaler()\n",
    "ros = RandomOverSampler()\n",
    "\n",
    "neighbors = [5]\n",
    "metrics = ['manhattan']   \n",
    "weights = ['uniform']\n",
    "\n",
    "pipe = Pipeline(steps=[('selector', selector), ('scaler', scaler), ('sampler', ros), ('knn', knn)], memory = 'tmp')\n",
    "estimator = GridSearchCV(pipe, dict(selector__threshold=vthreshold, knn__n_neighbors=neighbors, knn__weights=weights, knn__metric=metrics), cv=5, scoring='f1_macro', n_jobs=-1)\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "estimator.fit(train, train_labels)\n",
    "preds = estimator.predict(test)\n",
    "print(\"Συνολικός χρόνος fit και predict: %s seconds\" % (time.time() - start_time))\n",
    "end_time=time.time()\n",
    "print(classification_report(test_labels, preds))\n",
    "\n",
    "print('Best score: ', estimator.best_score_)\n",
    "print('Best estimator: ', estimator.best_estimator_)\n",
    "print('Best parameters: ', estimator.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 390
    },
    "colab_type": "code",
    "id": "SSJIk_73OgyO",
    "outputId": "3596f0a9-ad62-4f80-c0f2-c15128236f26"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Συνολικός χρόνος fit και predict: 222.20260620117188 seconds\n",
      "\n",
      "Pipeline(memory='tmp',\n",
      "     steps=[('selector', VarianceThreshold(threshold=4000)), ('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('sampler', RandomOverSampler(random_state=None, ratio=None, return_indices=False,\n",
      "         sampling_strategy='auto')), ('knn', KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='manhattan',\n",
      "           metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
      "           weights='uniform'))])\n",
      "{'knn__metric': 'manhattan', 'knn__n_neighbors': 5, 'knn__weights': 'uniform', 'selector__threshold': 4000}\n",
      "\n",
      " Optimized kNN:\n",
      "[[11454   951]\n",
      " [  328   289]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.92      0.95     12405\n",
      "           1       0.23      0.47      0.31       617\n",
      "\n",
      "   micro avg       0.90      0.90      0.90     13022\n",
      "   macro avg       0.60      0.70      0.63     13022\n",
      "weighted avg       0.94      0.90      0.92     13022\n",
      "\n"
     ]
    }
   ],
   "source": [
    "times_macro[\"Optimized kNN\"] = end_time-start_time\n",
    "print(\"Συνολικός χρόνος fit και predict: %s seconds\\n\" %(end_time-start_time))\n",
    "\n",
    "print(estimator.best_estimator_)\n",
    "print(estimator.best_params_)\n",
    "\n",
    "print(\"\\n Optimized kNN:\")\n",
    "print(confusion_matrix(test_labels, preds))\n",
    "print(classification_report(test_labels, preds))\n",
    "\n",
    "f1_macro_opt[\"kNN_optimized\"] = f1_score(test_labels, preds, average = 'macro')\n",
    "\n",
    "f1_macro_opt[\"kNN_optimized\"] = f1_score(test_labels, preds, average = 'macro')\n",
    "metavoli['kNN'] = f1_macro_opt['kNN_optimized'] - f1_macro['KNN_default']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 320
    },
    "colab_type": "code",
    "id": "nBewfyikOjAx",
    "outputId": "f380af43-4ea9-48ce-e6cd-e902a4ea1a6e"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlEAAAEvCAYAAABlvJTyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJEmqYIiSJEmq8P8Bferl45+omW4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff2893c1320>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(1, figsize=(10, 5))\n",
    "plt.bar(range(len(f1_macro_opt)), list(f1_macro_opt.values()), align='center')\n",
    "plt.xticks(range(len(f1_macro_opt)), list(f1_macro_opt.keys()))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 134
    },
    "colab_type": "code",
    "id": "phobOl74cyyR",
    "outputId": "4cceca6a-eff4-4c48-d2ba-132c89745bf1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GNB 0.8844263553985563\n",
      "Uniform 0.004299229544804939\n",
      "Constant 0 -0.46475141761934546\n",
      "Constant 1 -0.0021434339619543705\n",
      "Most Frequent -0.46475141761934546\n",
      "Stratified -0.5402974701814941\n",
      "kNN 0.07693047953844345\n"
     ]
    }
   ],
   "source": [
    "for header in metavoli.keys():\n",
    "  print(header, metavoli[header])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eY1FOeovczpN"
   },
   "source": [
    "## Σχολιασμός\n",
    "\n",
    "Όπως φαίνεται και από το παραπάνω διάγραμμα, το βέλτιστο kNN βελτίωσε αισθητά το f1_macro.\n",
    "\n",
    "Από σύμπτωση, το βέλτιστο knn μοιάζει να είναι αρκετά κοντά στο default στο οποίο κάναμε τυχαίες αρχικοποιήσεις.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fbwEB2npkg4Y"
   },
   "source": [
    "##Multi Layer Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vX6R5ZjLuKCU"
   },
   "source": [
    "##f1 micro average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 339
    },
    "colab_type": "code",
    "id": "8dDzTuP5_sFe",
    "outputId": "11641482-fd98-4470-8dde-6649b50106f0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Συνολικός χρόνος fit και predict: 637.5502045154572 seconds\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.91      0.94       289\n",
      "           1       0.00      0.00      0.00        11\n",
      "\n",
      "   micro avg       0.88      0.88      0.88       300\n",
      "   macro avg       0.48      0.46      0.47       300\n",
      "weighted avg       0.92      0.88      0.90       300\n",
      "\n",
      "Best score:  0.8969957081545065\n",
      "Best estimator:  Pipeline(memory='tmp',\n",
      "     steps=[('selector', VarianceThreshold(threshold=100)), ('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('sampler', RandomOverSampler(random_state=None, ratio=None, return_indices=False,\n",
      "         sampling_strategy='auto')), ('pca', PCA(copy=True, iterated_power='auto', n_compone...True, solver='lbfgs', tol=0.0001,\n",
      "       validation_fraction=0.1, verbose=False, warm_start=False))])\n",
      "Best parameters:  {'mlp__activation': 'logistic', 'mlp__alpha': 0.0005, 'mlp__hidden_layer_sizes': 15, 'mlp__learning_rate': 'constant', 'mlp__max_iter': 300, 'mlp__solver': 'lbfgs', 'pca__n_components': 9, 'selector__threshold': 100}\n"
     ]
    }
   ],
   "source": [
    "#Δοκιμάσαμε κάθε εφικτό συνδυασμό για την αρχιτεκτονική. Χρειαστήκαμε και αρκετό trial and error για να βρούμε καλά εύρη και διαβαθμίσεις\n",
    "#Τα αποτελέσματα βρίσκονται στο markdown που ακολουθεί\n",
    "\n",
    "mlp = MLPClassifier()\n",
    "\n",
    "n_components = [5,9]\n",
    "vthreshold = [0,100,1000,3000]\n",
    "scaler = StandardScaler()\n",
    "ros = RandomOverSampler()\n",
    "\n",
    "sizes = [5,10,15]\n",
    "activ = ['identity','logistic','relu']\n",
    "alpha = [0.0005, 0.001, 0.005]\n",
    "learn_rate = ['constant','adaptive']\n",
    "itera = [200,250,300]\n",
    "solv = ['lbfgs','adam']\n",
    "\n",
    "pipe = Pipeline(steps=[('selector', selector),('scaler', scaler),('sampler', ros), ('pca', pca), ('mlp', mlp)], memory = 'tmp')\n",
    "estimator = GridSearchCV(pipe, dict(selector__threshold=vthreshold, pca__n_components=n_components, mlp__hidden_layer_sizes=sizes, mlp__activation=activ,mlp__alpha=alpha, mlp__learning_rate=learn_rate, mlp__max_iter=itera, mlp__solver=solv), cv=5, scoring='f1_micro', n_jobs=-1)\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "estimator.fit(strain, strain_labels)\n",
    "preds = estimator.predict(stest)\n",
    "print(\"Συνολικός χρόνος fit και predict: %s seconds\" % (time.time() - start_time))\n",
    "print(classification_report(stest_labels, preds))\n",
    "\n",
    "print('Best score: ', estimator.best_score_)\n",
    "print('Best estimator: ', estimator.best_estimator_)\n",
    "print('Best parameters: ', estimator.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wr6C7UddU4po"
   },
   "source": [
    "###Στάδια:\n",
    "1. MLP Classifier\n",
    "\n",
    "###Επίδοση, βέλτιστη παράμετρος:\n",
    "Best f1_micro score is  0.949928469241774\n",
    "\n",
    "Optimized params are: \n",
    "\n",
    ">activation = logistic\n",
    "\n",
    ">alpha = 0.0005\n",
    "\n",
    ">hidden layer size = 5\n",
    "\n",
    ">learning rate = constant\n",
    "\n",
    ">maximum iterations = 200\n",
    "\n",
    ">solver = lbfgs\n",
    "\n",
    "---\n",
    "###Στάδια: \n",
    "1.  Variance Threshold\n",
    "2. MLP Classifier\n",
    "\n",
    "### Επίδοση, βέλτιστες υπερπαράμετροι:\n",
    "Best f1_micro score is  0.9527896995708155\n",
    "\n",
    "Optimized params are: \n",
    "\n",
    ">threshold = 0\n",
    "\n",
    ">activation = logistic\n",
    "\n",
    ">alpha = 0.0005\n",
    "\n",
    ">hidden layer size = 5\n",
    "\n",
    ">learning rate = adaptive\n",
    "\n",
    ">maximum iterations = 200\n",
    "\n",
    ">solver = adam\n",
    "---\n",
    "\n",
    "###Στάδια: \n",
    "1.  Variance Threshold\n",
    "2. Standard Scaler\n",
    "3. MLP Classifier\n",
    "\n",
    "### Επίδοση, βέλτιστες υπερπαράμετροι:\n",
    "Best f1_micro score is  0.9541423054353536\n",
    "\n",
    "Optimized params are: \n",
    "\n",
    ">threshold = 100\n",
    "\n",
    ">activation = logistic\n",
    "\n",
    ">alpha = 0.0005\n",
    "\n",
    ">hidden layer size = 10\n",
    "\n",
    ">learning rate = constant\n",
    "\n",
    ">maximum iterations = 200\n",
    "\n",
    ">solver = lbfgs\n",
    "\n",
    "---\n",
    "\n",
    "###Στάδια: \n",
    "1.  Variance Threshold\n",
    "2. Standard Scaler\n",
    "3. Random Oversampler\n",
    "3. KNeighbors Classifier\n",
    "\n",
    "### Επίδοση, βέλτιστες υπερπαράμετροι:\n",
    "Best f1_micro score is 0.9213161659513591\n",
    "\n",
    "Optimized params are: \n",
    "\n",
    ">threshold = 1000\n",
    "\n",
    ">activation = logistic\n",
    "\n",
    ">alpha = 0.0005\n",
    "\n",
    ">hidden layer size = 10\n",
    "\n",
    ">learning rate = adaptive\n",
    "\n",
    ">maximum iterations = 200\n",
    "\n",
    ">solver = adam\n",
    "\n",
    "---\n",
    "\n",
    "###Στάδια: \n",
    "1.  Variance Threshold\n",
    "2. Standard Scaler\n",
    "3. Random Oversampler\n",
    "4. PCA\n",
    "3. MLP Classifier\n",
    "\n",
    "### Επίδοση, βέλτιστες υπερπαράμετροι:\n",
    "Best f1_micro score is  0.8969957081545065\n",
    "\n",
    "Optimized params are: \n",
    "\n",
    ">threshold = 100\n",
    "\n",
    ">n_componenents = 9\n",
    "\n",
    ">activation = logistic\n",
    "\n",
    ">alpha = 0.0005\n",
    "\n",
    ">hidden layer size = 15\n",
    "\n",
    ">learning rate = constant\n",
    "\n",
    ">maximum iterations = 300\n",
    "\n",
    ">solver = lbfgs\n",
    "\n",
    "---\n",
    "\n",
    "---\n",
    "---\n",
    "###Θα χρησιμοποιήσουμε τώρα Min-max Scaler αντί για Standard Scaler\n",
    "\n",
    "###Στάδια:\n",
    "1. MinMax Scaler\n",
    "2. MLP Classifier\n",
    "\n",
    "###Επίδοση, βέλτιστες υπερπαράμετροι:\n",
    "Best f1_micro score is **0.9542203147353362**\n",
    "\n",
    "Optimized params are: \n",
    "\n",
    ">activation = logistic\n",
    "\n",
    ">alpha = 0.005\n",
    "\n",
    ">hidden layer size = 5\n",
    "\n",
    ">learning rate = constant\n",
    "\n",
    ">maximum iterations = 200\n",
    "\n",
    ">solver = adam\n",
    "\n",
    "---\n",
    "###Στάδια:\n",
    "1. MinMax Scaler\n",
    "2. Variance Threshold\n",
    "2. MLP Classifier\n",
    "\n",
    "###Επίδοση, βέλτιστες υπερπαράμετροι:\n",
    "Best f1_micro score is  0.9542203147353362\n",
    "\n",
    "Optimized params are: \n",
    "\n",
    "\n",
    ">threshold = 0\n",
    "\n",
    ">activation = logistic\n",
    "\n",
    ">alpha = 0.005\n",
    "\n",
    ">hidden layer size = 5\n",
    "\n",
    ">learning rate = constant\n",
    "\n",
    ">maximum iterations = 200\n",
    "\n",
    ">solver = adam\n",
    "\n",
    "---\n",
    "###Στάδια:\n",
    "1. MinMax Scaler\n",
    "2. Variance Threshold\n",
    "3. Random Oversampler\n",
    "2. MLP Classifier\n",
    "\n",
    "###Επίδοση, βέλτιστες υπερπαράμετροι:\n",
    "Best f1_micro score is 0.8025751072961373\n",
    "\n",
    "Optimized params are:\n",
    "\n",
    ">threshold = 0\n",
    "\n",
    ">activation = logistic\n",
    "\n",
    ">alpha = 0.001\n",
    "\n",
    ">hidden layer size = 10\n",
    "\n",
    ">learning rate = adaptive\n",
    "\n",
    ">maximum iterations = 200\n",
    "\n",
    ">solver = adam\n",
    "\n",
    "---\n",
    "\n",
    "###Στάδια:\n",
    "1. MinMax Scaler\n",
    "2. Variance Threshold\n",
    "3. Random Oversampler\n",
    "4. PCA\n",
    "2. MLP Classifier\n",
    "\n",
    "###Επίδοση, βέλτιστες υπερπαράμετροι:\n",
    "Best f1_micro score is 0.8097281831187411\n",
    "\n",
    "Optimized params are: \n",
    "\n",
    ">threshold = 0\n",
    "\n",
    ">n_componenents = 5\n",
    "\n",
    ">activation = logistic\n",
    "\n",
    ">alpha = 0.005\n",
    "\n",
    ">hidden layer size = 5\n",
    "\n",
    ">learning rate = constant\n",
    "\n",
    ">maximum iterations = 200\n",
    "\n",
    ">solver = adam\n",
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 406
    },
    "colab_type": "code",
    "id": "xVfDNdRZCP3y",
    "outputId": "5dbf44a7-3d7a-4950-c3bc-70d54728401c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Συνολικός χρόνος fit και predict: 32.82880878448486 seconds\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98       289\n",
      "           1       0.00      0.00      0.00        11\n",
      "\n",
      "   micro avg       0.96      0.96      0.96       300\n",
      "   macro avg       0.48      0.50      0.49       300\n",
      "weighted avg       0.93      0.96      0.95       300\n",
      "\n",
      "Best score:  0.9470672389127325\n",
      "Best estimator:  Pipeline(memory='tmp',\n",
      "     steps=[('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('mlp', MLPClassifier(activation='logistic', alpha=0.003, batch_size='auto',\n",
      "       beta_1=0.9, beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=4, learning_rate='constant',\n",
      "       learning_rate_...=True, solver='adam', tol=0.0001,\n",
      "       validation_fraction=0.1, verbose=False, warm_start=False))])\n",
      "Best parameters:  {'mlp__activation': 'logistic', 'mlp__alpha': 0.003, 'mlp__hidden_layer_sizes': 4, 'mlp__learning_rate': 'constant', 'mlp__max_iter': 220, 'mlp__solver': 'adam'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (220) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "#Optimized f1_micro\n",
    "\n",
    "sizes = [4,5,6]\n",
    "activ = ['logistic']\n",
    "alpha = [0.003, 0.005, 0.008]\n",
    "learn_rate = ['constant']\n",
    "itera = [180,200,220]\n",
    "solv = ['adam']\n",
    "\n",
    "pipe = Pipeline(steps=[('scaler', scaler), ('mlp', mlp)], memory = 'tmp')\n",
    "estimator = GridSearchCV(pipe, dict(mlp__hidden_layer_sizes=sizes, mlp__activation=activ,mlp__alpha=alpha, mlp__learning_rate=learn_rate, mlp__max_iter=itera, mlp__solver=solv), cv=5, scoring='f1_micro', n_jobs=-1)\n",
    "\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "estimator.fit(strain, strain_labels)\n",
    "preds = estimator.predict(stest)\n",
    "print(\"Συνολικός χρόνος fit και predict: %s seconds\" % (time.time() - start_time))\n",
    "print(classification_report(stest_labels, preds))\n",
    "\n",
    "print('Best score: ', estimator.best_score_)\n",
    "print('Best estimator: ', estimator.best_estimator_)\n",
    "print('Best parameters: ', estimator.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 373
    },
    "colab_type": "code",
    "id": "IAPiaaCAVGd4",
    "outputId": "8c5ca872-c811-49f7-b560-50d416343d62"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Συνολικός χρόνος fit και predict: 15.848507642745972 seconds\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.98     12405\n",
      "           1       0.00      0.00      0.00       617\n",
      "\n",
      "   micro avg       0.95      0.95      0.95     13022\n",
      "   macro avg       0.48      0.50      0.49     13022\n",
      "weighted avg       0.91      0.95      0.93     13022\n",
      "\n",
      "Best score:  0.9514860283711286\n",
      "Best estimator:  Pipeline(memory='tmp',\n",
      "     steps=[('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('mlp', MLPClassifier(activation='logistic', alpha=0.003, batch_size='auto',\n",
      "       beta_1=0.9, beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=4, learning_rate='constant',\n",
      "       learning_rate_...=True, solver='adam', tol=0.0001,\n",
      "       validation_fraction=0.1, verbose=False, warm_start=False))])\n",
      "Best parameters:  {'mlp__activation': 'logistic', 'mlp__alpha': 0.003, 'mlp__hidden_layer_sizes': 4, 'mlp__learning_rate': 'constant', 'mlp__max_iter': 220, 'mlp__solver': 'adam'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "#Fit and transform on the whole dataset\n",
    "\n",
    "sizes = [4]\n",
    "activ = ['logistic']\n",
    "alpha = [0.003]\n",
    "learn_rate = ['constant']\n",
    "itera = [220]\n",
    "solv = ['adam']\n",
    "\n",
    "pipe = Pipeline(steps=[('scaler', scaler), ('mlp', mlp)], memory = 'tmp')\n",
    "estimator = GridSearchCV(pipe, dict(mlp__hidden_layer_sizes=sizes, mlp__activation=activ,mlp__alpha=alpha, mlp__learning_rate=learn_rate, mlp__max_iter=itera, mlp__solver=solv), cv=5, scoring='f1_micro', n_jobs=-1)\n",
    "\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "estimator.fit(train, train_labels)\n",
    "preds = estimator.predict(test)\n",
    "print(\"Συνολικός χρόνος fit και predict: %s seconds\" % (time.time() - start_time))\n",
    "end_time=time.time()\n",
    "print(classification_report(test_labels, preds))\n",
    "\n",
    "print('Best score: ', estimator.best_score_)\n",
    "print('Best estimator: ', estimator.best_estimator_)\n",
    "print('Best parameters: ', estimator.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 440
    },
    "colab_type": "code",
    "id": "snf7PP5LOMF0",
    "outputId": "4236f5b2-001d-4f49-bc72-d1008192f5fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Συνολικός χρόνος fit και predict: 15.8494713306427 seconds\n",
      "\n",
      "Pipeline(memory='tmp',\n",
      "     steps=[('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('mlp', MLPClassifier(activation='logistic', alpha=0.003, batch_size='auto',\n",
      "       beta_1=0.9, beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=4, learning_rate='constant',\n",
      "       learning_rate_...=True, solver='adam', tol=0.0001,\n",
      "       validation_fraction=0.1, verbose=False, warm_start=False))])\n",
      "{'mlp__activation': 'logistic', 'mlp__alpha': 0.003, 'mlp__hidden_layer_sizes': 4, 'mlp__learning_rate': 'constant', 'mlp__max_iter': 220, 'mlp__solver': 'adam'}\n",
      "\n",
      " Optimized MLP:\n",
      "[[12405     0]\n",
      " [  617     0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.98     12405\n",
      "           1       0.00      0.00      0.00       617\n",
      "\n",
      "   micro avg       0.95      0.95      0.95     13022\n",
      "   macro avg       0.48      0.50      0.49     13022\n",
      "weighted avg       0.91      0.95      0.93     13022\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "times_micro[\"Optimized MLP\"] = end_time-start_time\n",
    "print(\"Συνολικός χρόνος fit και predict: %s seconds\\n\" %(end_time-start_time))\n",
    "\n",
    "print(estimator.best_estimator_)\n",
    "print(estimator.best_params_)\n",
    "\n",
    "print(\"\\n Optimized MLP:\")\n",
    "print(confusion_matrix(test_labels, preds))\n",
    "print(classification_report(test_labels, preds))\n",
    "\n",
    "f1_micro_opt[\"MLP_optimized\"] = f1_score(test_labels, preds, average = 'micro')\n",
    "\n",
    "f1_micro_opt[\"MLP_optimized\"] = f1_score(test_labels, preds, average = 'micro')\n",
    "metavoli['MLP'] = f1_micro_opt['MLP_optimized'] - f1_micro['MLP_default']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 324
    },
    "colab_type": "code",
    "id": "K_JQvOz1OL3Q",
    "outputId": "4368b4f1-036f-4be9-feb6-c28e7cc6b95d"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlEAAAEzCAYAAAARqFYSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAHSNJREFUeJzt3XvcXFV97/FPTIRCDRjLo4CieIm/\nindATKpcg9dqvRAVFW24eDyKiqcVTxQVvAGKHEpEbbEq1XorqCBVaRAE0SBitBQ9+EMRvCWVB8kB\nFJRLcv5Ya8hkfC6TlXnyzBM+79crrzyz9549a9asvfd3r7Vnz6x169YhSZKkjXOv6S6AJEnSTGSI\nkiRJamCIkiRJamCIkiRJamCIkiRJamCIkiRJajCnn4Ui4jHAOcApmXlaz7wDgeOBu4CvZua7B15K\nSZKkITNpT1RE/DnwQeCCcRZZBhwEPAV4ekTsNrjiSZIkDad+hvP+CDwbWNU7IyIeBtyYmb/MzLXA\nV4FFgy2iJEnS8Jk0RGXmnZl52zizdwRGux5fD+w0iIJJkiQNs76uidoIsyZb4M4771o3Z87sAb+s\nJEnSlBg322xqiFpF6Y3qeCBjDPt1W7Pm1k18yeEyMjKX0dFbprsY08o6sA7AOgDrAKwDsA5gy6qD\nkZG5487bpFscZOZ1wHYRsWtEzAGeAyzflHVKkiTNBJP2REXEHsDJwK7AHRGxGPgycG1mfgl4DfDZ\nuvjnM/PqKSqrJEnS0Jg0RGXmSmC/CeZ/E1g4wDJJkiQNPe9YLkmS1MAQJUmS1MAQJUmS1MAQJUmS\n1MAQJUmS1MAQJUmS1MAQJUmS1MAQJUmS1GDQP0A8NA478cLpLsJAfXzpAdNdhBnJdiCwHUgdbguD\nZU+UJElSA0OUJElSA0OUJElSA0OUJElSA0OUJElSA0OUJElSA0OUJElSA0OUJElSA0OUJElSA0OU\nJElSgy32Z18kSermT55o0OyJkiRJamCIkiRJamCIkiRJamCIkiRJamCIkiRJamCIkiRJamCIkiRJ\namCIkiRJamCIkiRJamCIkiRJamCIkiRJamCIkiRJamCIkiRJamCIkiRJamCIkiRJamCIkiRJamCI\nkiRJamCIkiRJamCIkiRJamCIkiRJamCIkiRJamCIkiRJamCIkiRJamCIkiRJamCIkiRJamCIkiRJ\najCnn4Ui4hRgAbAOOCozL++adyRwCHAX8L3MfONUFFSSJGmYTNoTFRH7AvMzcyFwOLCsa952wNHA\n3pn5VGC3iFgwVYWVJEkaFv0M5y0CzgbIzKuAeTU8Adxe/90nIuYA2wI3TkVBJUmShkk/w3k7Aiu7\nHo/WaTdn5h8i4p3Az4DbgM9l5tUTrWzevG2ZM2d2a3nvsUZG5k53ESY07OXbUgx7PQ97+bYUw17P\nw16+LYX1PP110Nc1UT1mdf6oPVJvBR4J3AxcGBGPz8wrxnvymjW3NrykRkdvme4ijGtkZO5Ql29L\nMsz1bDvYfIa5nm0Hm4/1vHnqYKKg1s9w3ipKz1PHzsDq+vejgJ9l5g2ZeTtwCbBHYzklSZJmjH5C\n1HJgMUBE7A6sysxO9LsOeFREbFMf7wn8ZNCFlCRJGjaTDudl5oqIWBkRK4C1wJERsQS4KTO/FBEn\nAd+IiDuBFZl5ydQWWZIkafr1dU1UZi7tmXRF17x/Av5pkIWSJEkadt6xXJIkqYEhSpIkqYEhSpIk\nqYEhSpIkqYEhSpIkqYEhSpIkqYEhSpIkqYEhSpIkqYEhSpIkqYEhSpIkqYEhSpIkqYEhSpIkqYEh\nSpIkqYEhSpIkqYEhSpIkqYEhSpIkqYEhSpIkqYEhSpIkqYEhSpIkqYEhSpIkqYEhSpIkqYEhSpIk\nqYEhSpIkqYEhSpIkqYEhSpIkqYEhSpIkqYEhSpIkqYEhSpIkqYEhSpIkqYEhSpIkqYEhSpIkqYEh\nSpIkqYEhSpIkqYEhSpIkqYEhSpIkqYEhSpIkqYEhSpIkqYEhSpIkqYEhSpIkqYEhSpIkqYEhSpIk\nqYEhSpIkqYEhSpIkqYEhSpIkqYEhSpIkqcGcfhaKiFOABcA64KjMvLxr3i7AZ4GtgO9n5v+cioJK\nkiQNk0l7oiJiX2B+Zi4EDgeW9SxyMnByZu4F3BURDx58MSVJkoZLP8N5i4CzATLzKmBeRGwHEBH3\nAvYGvlznH5mZv5iiskqSJA2NfkLUjsBo1+PROg1gBLgFOCUivhURJwy4fJIkSUOpr2uieszq+fuB\nwKnAdcBXIuKvM/Mr4z153rxtmTNndsPL3rONjMyd7iJMaNjLt6UY9noe9vJtKYa9noe9fFsK63n6\n66CfELWK9T1PADsDq+vfNwA/z8xrACLiAuDRwLghas2aW9tKeg83OnrLdBdhXCMjc4e6fFuSYa5n\n28HmM8z1bDvYfKznzVMHEwW1fobzlgOLASJid2BVZt4CkJl3Aj+LiPl12T2A3KTSSpIkzQCT9kRl\n5oqIWBkRK4C1wJERsQS4KTO/BLwROKNeZH4lcO5UFliSJGkY9HVNVGYu7Zl0Rde8nwJPHWShJEmS\nhp13LJckSWpgiJIkSWpgiJIkSWpgiJIkSWpgiJIkSWpgiJIkSWpgiJIkSWpgiJIkSWpgiJIkSWpg\niJIkSWpgiJIkSWpgiJIkSWpgiJIkSWpgiJIkSWpgiJIkSWpgiJIkSWpgiJIkSWpgiJIkSWpgiJIk\nSWpgiJIkSWpgiJIkSWowZ7oLIEmaeoedeOF0F2GgPr70gOkugmSI2pK505Qkaeo4nCdJktTAECVJ\nktTAECVJktTAECVJktTAECVJktTAECVJktTAECVJktTAECVJktTAECVJktTAECVJktTAECVJktTA\nECVJktTAECVJktTAECVJktTAECVJktTAECVJktTAECVJktTAECVJktTAECVJktTAECVJktTAECVJ\nktTAECVJktRgTj8LRcQpwAJgHXBUZl4+xjInAAszc7+BllCSJGkITdoTFRH7AvMzcyFwOLBsjGV2\nA/YZfPEkSZKGUz/DeYuAswEy8ypgXkRs17PMycAxAy6bJEnS0OonRO0IjHY9Hq3TAIiIJcDFwHWD\nLJgkSdIw6+uaqB6zOn9ExP2AQ4EDgQf28+R587ZlzpzZDS97zzYyMne6izDtrIPhr4NhL9+Wwnq2\nDsA6gOmvg35C1Cq6ep6AnYHV9e8DgBHgEmBr4OERcUpm/q/xVrZmza2NRb1nGx29ZbqLMO2sg+Gu\ng5GRuUNdvi2J9WwdgHUAm6cOJgpq/QznLQcWA0TE7sCqzLwFIDPPyszdMnMB8ALg+xMFKEmSpC3F\npCEqM1cAKyNiBeWbeUdGxJKIeMGUl06SJGlI9XVNVGYu7Zl0xRjLXAfst+lFkiRJGn7esVySJKmB\nIUqSJKmBIUqSJKmBIUqSJKmBIUqSJKmBIUqSJKmBIUqSJKmBIUqSJKmBIUqSJKmBIUqSJKmBIUqS\nJKmBIUqSJKmBIUqSJKmBIUqSJKmBIUqSJKmBIUqSJKmBIUqSJKmBIUqSJKmBIUqSJKmBIUqSJKmB\nIUqSJKmBIUqSJKmBIUqSJKmBIUqSJKmBIUqSJKmBIUqSJKmBIUqSJKmBIUqSJKmBIUqSJKmBIUqS\nJKmBIUqSJKmBIUqSJKmBIUqSJKmBIUqSJKmBIUqSJKmBIUqSJKmBIUqSJKmBIUqSJKmBIUqSJKmB\nIUqSJKmBIUqSJKmBIUqSJKmBIUqSJKmBIUqSJKmBIUqSJKmBIUqSJKnBnH4WiohTgAXAOuCozLy8\na97+wAnAXUACR2Tm2ikoqyRJ0tCYtCcqIvYF5mfmQuBwYFnPIqcDizPzKcBc4JkDL6UkSdKQ6Wc4\nbxFwNkBmXgXMi4jtuubvkZm/qn+PAn8x2CJKkiQNn36G83YEVnY9Hq3TbgbIzJsBImIn4OnA2yda\n2bx52zJnzuymwt6TjYzMne4iTDvrYPjrYNjLt6Wwnq0DsA5g+uugr2uieszqnRAR9wfOBV6bmb+d\n6Mlr1tza8JIaHb1luosw7ayD4a6DkZG5Q12+LYn1bB2AdQCbpw4mCmr9hKhVlJ6njp2B1Z0HdWjv\na8Axmbm8sYySJEkzSj/XRC0HFgNExO7Aqszsjn4nA6dk5nlTUD5JkqShNGlPVGauiIiVEbECWAsc\nGRFLgJuA/wBeCcyPiCPqUz6TmadPVYElSZKGQV/XRGXm0p5JV3T9vfXgiiNJkjQzeMdySZKkBoYo\nSZKkBoYoSZKkBoYoSZKkBoYoSZKkBoYoSZKkBoYoSZKkBoYoSZKkBoYoSZKkBoYoSZKkBoYoSZKk\nBoYoSZKkBoYoSZKkBoYoSZKkBoYoSZKkBoYoSZKkBoYoSZKkBoYoSZKkBoYoSZKkBoYoSZKkBoYo\nSZKkBoYoSZKkBoYoSZKkBoYoSZKkBoYoSZKkBoYoSZKkBoYoSZKkBoYoSZKkBoYoSZKkBoYoSZKk\nBoYoSZKkBoYoSZKkBoYoSZKkBoYoSZKkBoYoSZKkBoYoSZKkBnOmuwCSptZhJ1443UUYqI8vPWC6\niyBJgD1RkiRJTQxRkiRJDQxRkiRJDQxRkiRJDQxRkiRJDQxRkiRJDQxRkiRJDQxRkiRJDfq62WZE\nnAIsANYBR2Xm5V3zDgSOB+4CvpqZ756KgkqSJA2TSXuiImJfYH5mLgQOB5b1LLIMOAh4CvD0iNht\n4KWUJEkaMv0M5y0CzgbIzKuAeRGxHUBEPAy4MTN/mZlrga/W5SVJkrZo/YSoHYHRrsejddpY864H\ndhpM0SRJkobXrHXr1k24QEScDnwlM8+pj78FHJaZV0fEXwFHZ+YL6rwjgIdl5lunuNySJEnTqp+e\nqFWs73kC2BlYPc68B9ZpkiRJW7R+QtRyYDFAROwOrMrMWwAy8zpgu4jYNSLmAM+py0uSJG3RJh3O\nA4iIE4F9gLXAkcATgZsy80sRsQ/wvrroFzLzA1NVWEmSpGHRV4iSJEnShrxjuSRJUgNDlCRJUoO+\nfvZlGEXEI4D/AzygTvo58FrKxe3vptxl/Q912TOA4+pyVwIrKT9h82eUWzR8a0Bl+hvgPOB+wDsz\n89V9PGcpcHFmXtr4mq8DdsjM4xqfvx/wusxc3DXtOOCGzDxtvPIC3wO+Bfw4M/+25bU3p4iYD/wD\nMALMBlYAb8rMPw5g3Ysz86xBPC8itgc+A2wP/A54WWbeuKllrOueEXVQp78I+ASwIDN/2Oe6dgWu\nBRZm5ne6pl8O/Cgzl2xk2Q7KzC/0TDsOeDnw667JJ2bmeRuz7kGKiAcDO2bmdzdhHUcCrwD+CGwD\nvJVy378/ZObVfa5jcWaeFRFPAF6QmcdGxJuBVwKvAQ7pZ59Y13VDZu7Q8D6WAI/JzDfVx7sC1wBP\nzMz/6lqGzDwjIq4DTs7MD3Ytf9zGtpUJytOpk2cCD83Mj/TxnM8Bh2bmbY2v+VXgWYyzHdSHZ2Xm\nv/c87w7g2/XhNsAnMvMfW8owRpn2oRwrro+IczLzeX08Zwn12uvG13wOsHhQn+V4ZmRPVETMBr4A\nvD8zn5yZT6YEo85P0qwBjhrn6ZmZ+2Xm/sD/Bt4+wKL9HbBVZv53vzuLzDyxNUBNh67y7gRsPUMC\nVHd72QvYs856x4BeYmlDmbaitJdebwQuysynAl+ktNFNNpPqoP7U1LOA/2oox8+Al3at6xHAvIay\n7dq9nh6n1n1I59+0BajqAGCv1ifX9/oqYO/M3JcSEt8OvBB45EasailAZv5nZh5bpz2TEp4u6Xef\nOAX+L3DiOPN+A7wqIuYO+kW723dmntdPgKrLHtwaoLqsYuO3g5s6bZryRbLXR8RDNrEcHYcB9wfo\nJ0DV5c5oDVCb00ztiXoa8MOeHqSTgFmUs6kPA6+NiI9Ochb/ADY8o/wTEfFiyoZwJ7AyM4+qZ6MP\nAh5MCRNHU87uFwBfi4jDgc9k5p4RcQ3wUcptIn5KCXsvAn6SmS+vvWRnAbsAL6kvOx84DXg/cDrw\nMODewDsy88KIWETpUfhvyj27fjbRe2gVERdTzuIeD/wgM4/oKu+hwMMj4hOUA/8ZwH1rOd+Qmd+P\niJ8A36fc9uIVwDcon91a4F+AJZQfrl6UmXdNxXuonkY5C7oYIDPX1TPktfV9HgUcXJc9OzPfV9/n\nKmAPyuf8ckov5r9SAyRwLPBY4PER8UXgxfV9PQj4c8oZ7b9HxEXA+ZSD3Q7Acynh6LER8eHMfG1X\nWRdRdjgA5wIbnC3eQ+rg+5l5cX3OxvoO8LSImF3b1MGU9rdtfZ/7UX4w/Q7gV5S6fkB9T3dR9omH\nAB8C9oqId2TmuyZ70VpXtwN/UetgrO32QMp2uxr4JfAL4CK6eoI7PTD1N0hPo/SY30LZVu5Lqdu7\nt0ngLZRe9jsi4heZ+eWGOtue0iu/FXBHZv6k9nCfD4xGxPXApyk/63U9pU1+iFKHayn7s8NZ3waW\nAa8DzgF2Bz4aEYcAn677xL1Z/xn8khLg1lJ6YHcB7v6B+00REScAv6fsc7eNiAMy88KexW6j1OnR\n9HlCERHvp/xW7BzgtMz8VG2rl1NOTrah7MvfTG3fwHeBx1A+009RPsO/Aj4CPA54MvChzPxQ7R17\nDHAyEPVlnwQcSBlx+Rjls7oLOCIzf1G35ZfW+felBMdxt4PJZOYfI+JKShv++Tj1sB9/ui29lBKc\nt6PsA06p854PPDoiDqJs3zvUOhv3mEAJ8jdQjtGdTpFdgK9n5qsj4r3A3pRe9dMy87MR8Vjgk8CN\ntY6n3IzsiQL+krIzv1tmru06EP+BMtR3zBjPjYi4KCK+U5cZ95YMEXEfSiM5sPYMPCwi9q+zH5iZ\nTwdeBpyQmZ+ihJpnUXamHbMpQeJJlA3vutoTsHdE3Ler/B+pZwCHUHZUH6nrXl17zZ5P2QEDnEA5\nu3sa5YA0VfagdOs/CXh2d3mBvy/FzkMpDfw7tZxvpGw4UDbAd2Xmx+rj1bUeZwP3y8zOBvDYKXwP\nUNrLf3ZPyMzb6o7ioZQNd+/67yUR8fC62NaZ+QzgVMqQxGMpQ6f7AM+o7+EkyhncCynDuMvr2fyL\ngXd2veTNmbkI+BrlDP+kUowNwgNs+FNKg/wZpRlTB1nvQ9foDuAyoLOdPo9y8O/4R+AltXxrKNvY\nYuD82n6PotT5SZRh9kkDVJcbM/MgJt5uD67b7c6TrOuDwKtrfS2n3FoGerbJ+n7PoPSOtQQoMvMK\nykH+2og4o544XkW5NOEtWYYJ7w18LTPfS+lReH19f98GXt7TBjrr/RSlzR1KGSbsWAY8LzMPoPQE\nvQh4OnDvLD90/2lKGG1Wh4N3oYRjKMeC90bErDEWPx14bkTsOMa83vXuQxkufArlhOC4rl6s39Y6\n+TRlPzjeNv4Eyv7zrym3B3ob5aTiVd0LZear6zHhJEpwuJRyqcrJtV38A/D2ul9+LbCQcrL6SEoQ\nmWg7mOx93q+W88oJFhtrWwJ4NPA3lPp5D3ABtR1k5i961jHpMSEzv1Tr4bnATcCJNYg/pO6HDgDe\nFhHbUILXcbV+pvLE/G4zNUStpasXLSLOqcHop6xP2p8E9hmjOzJrl+UCSgL+fJQbhY7lkZQeo9/V\nxxdR7pEFpWGQmVdS7tQ+ke9m5jrKDuMHddr1lDPAu0XEvShp/A2Z+f8oZyrPr4n9LGCb2kW8a93x\nQbk+aSqsA36aZWhyLaVHYvtxlt2TUjdk5veAR9Tpv8/MH3Ut17lmYzXr6+E3E6x3UNZRNsyxPJES\nAO/MzDspB4XH13mX1P9/RSnjj4G5EfEpyob7uZ51rQGeFBHfpnyO3QeC3nX1Y6wdfquZWgctzgRe\nGhGPoZzF/g7uPjCsy8xf1uW+QXnvy4FXRsTJlND4nTHW2e2our/p/NujTu+07/G2211y/fVdF03y\nGntRenAuohwYO9d+9rtNbpTMfCWwL+Vg92ZKL1Rv++u8v98Ax9ee6peyEYEnIh5A6Wn/Yn1v+1P2\nn7tRrtEjMy+j9BC1ejQlnBzRmZCZnV7xl/QuXNv88ay/bnYie1L3uZn5e0qPz/w67+v1/0tZ34M0\nlmsy87eU/eD1mflrxtkP1mD3XkpPH5S2dVytu7dQ6v4RlGv+/lBPQDptbMztYALbd9o0pd0enZk3\njLXgBNsSlJOPO+tz1zDxif7GHBM+BHwgM6+l1MOCWtb/oGSZnehqR0y+jQ3ETB3O+xHwhs6DrGOs\ntRv0XnXa2jrs9m7qkEWvzPxxRNxGOWO5doxF1rHhjmQr1m/cGxNA7xzn796d1FuAb2dm52BzO/De\nzPxs90IR0f1+NjUIj1K6f7uNADf3lBXGP6j31lPnYH17z3L91sOg/ZgyvHC3iNiasvMb6zPu1O8G\nZczMWyNiAWUDXkL5EsNhXcu8jNITs3f9/3td8/p9v52fUrqJwf6M0kyqg031dcqwyWrKwaBjzPeZ\nmT+MiMdTekNOiIiPU4baxnNq9nzpIiJgfXsfb7vtftipi94b9d27/n8rsH89+eo8f1f63yb7Vntn\nts7Mq4CrIuKDlPbSq/P+TgXel5nnRcSbgPtsxMvdDvy69ix0l+FoNtxPb8p+bVfKMWIx5csvHe+i\nHHA7Q5F3y8wzI+KNTH4N2ETbSqfMs/jTz7VbX/vB+rl8Avj7rjBzO/CizFzdtdyT2LDuOusZbzsY\nz029n8sE+qmHTlkGURcvo4S2z9RJtwMfy8wTepabNU45psxM7Ym6ENglIp7bmRDlJ2nm0tWFl5lf\noYzLPm6sldQ0vRPjXxd1NTC/q7t2X9YfFJ5a1/E41o8Zb9BDtjEi4smUnXj38MdllG5YIuL+EXF8\nnf7rKGYB+7W8XpergQdFufCQiBihnB1+e8Jnbejy+hzqAbavb1NtRucDD+m0l9rj9z7KWekPgIUR\nMaf2SD6Z9WdEG6ht7GVZrsV7DeWsB9ZvRzsA19ZeghdSdizjGa+tLKcMbwAcRBlSGYSZVAebJDNv\nB75JOXs/t2v6GmBdlG+zQd2eI+JgyhDN2ZShlT03sWwTbbed+lpU/7+ZOmRb9yWdfc0VlGtLiIiD\no1wHOZ5NrcfDgdO7hrq2p3ye142z3h2Aa2oIfzbrP+NJjyf1M6BTDxHx+vq+k/plhyg/bL9165sB\nvkIJ9m9nfQ8emfkb4GxgvAvcj6H0SE3kcuo+t17u8XDgJ3Xe3vX/hZQeqk39XP4OuDIzL+iadhll\niJiIOKCGi2uAR0XEVhGxHXUobLztYBDG25bq3wsjYnZE7EBpz79l046NDwXexIYngZdRhmDvFRF/\nVoM/dLUj1g9lTqkZGaLq2dkzgVdExOV16OBEyphpbzfwUtZ3M8L6a6IuoowRv642trFe5/eUCw7P\ni4hLKBdXd85sbo6IL1PGvzvfTLqIcubTcp3Su+rzLqjlew/wb8DvImIFZSPo9FAdQzmzOJdyYWaz\nzLyDcsHw6V3duG+gdKn261Rgj4i4kPI5jPfNyGlRD+jPAP5HRHRuzXATcGyW3388ndJFfwnwz5k5\n5oWUlN7KQ2pbOJ9yrQLADyLiu5Rvvz03Ii6gXND6q4gY72LV1cBWEXFmz/RlwJ71Nfbveo1NMpPq\nICIOr23xCcAnIuKTDW/5TMoFrDf1TH8V8Jm6/ntThiOvBk6r7fdYyvWIVwG7R8QpbLzxttu3Af8W\nEedThvOhhKXf12VfQQkuULaht9YhsyWME2qrS4E3R8TLG8oKpbfjeuCyWgfnUPYB3wSWjRHgPkgJ\nI2fWv/+29uR12sBkDqd8rpdQTkaTcp3cNvX9HswkX/iZTGaOUj7L3m+3foAy8jDWcy5ikv1e3f+v\njIhvUtr/0nqcAHhwRJxH6Y3tfIFgrG28X8cDT4n1w8aLKUOOz6+vfyxwaZYvT/0LpR18jA2/1Tre\ndnBC13o/3Fi+sbYlKG34TEpnxzF133MxcFZEPLrhdZZSgv25tbz/nJkrKEOIl1La6cq67HuA90e5\nzcOYx/VB82dfGsQE91GSpMnEJt7fTcOlBonXZZ/3NNtSRc99uu4JZuo1UQMV5SaZY92z59ScAfep\nkCQNTu09PWCMWYfWC5vvESJiL8qtdnp9Pvu879WWzp4oSZKkBjPymihJkqTpZoiSJElqYIiSJElq\nYIiSJElqYIiSJElqYIiSJElq8P8BxOqA9FisvpQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff2890aa198>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(1, figsize=(10, 5))\n",
    "plt.bar(range(len(f1_micro_opt)), list(f1_micro_opt.values()), align='center')\n",
    "plt.xticks(range(len(f1_micro_opt)), list(f1_micro_opt.keys()))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 151
    },
    "colab_type": "code",
    "id": "gatxjVaZhIAL",
    "outputId": "0c76d9e8-e41e-47d0-9abd-07ac842aec8d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GNB 0.8844263553985563\n",
      "Uniform 0.004299229544804939\n",
      "Constant 0 -0.46475141761934546\n",
      "Constant 1 -0.0021434339619543705\n",
      "Most Frequent -0.46475141761934546\n",
      "Stratified -0.5402974701814941\n",
      "kNN 0.07693047953844345\n",
      "MLP 0.04630625095991403\n"
     ]
    }
   ],
   "source": [
    "for header in metavoli.keys():\n",
    "  print(header, metavoli[header])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NvBrjIDXhGRq"
   },
   "source": [
    "## Σχολιασμός\n",
    "\n",
    "Παρατηρούμε πως με το βέλτιστο MLP ξεπεράσαμε κάθε άλλο μοντέλο. Τη μέγιστη μεταβολή στην επίδοση είχε ο GNB\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-1tcMGNXq0HS"
   },
   "source": [
    "##f1 macro average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 339
    },
    "colab_type": "code",
    "id": "rSOhN2MKCTky",
    "outputId": "b563e2c8-0576-4fb9-cc59-89a480862838"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Συνολικός χρόνος fit και predict: 173.6690912246704 seconds\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.79      0.87       289\n",
      "           1       0.06      0.36      0.11        11\n",
      "\n",
      "   micro avg       0.78      0.78      0.78       300\n",
      "   macro avg       0.52      0.58      0.49       300\n",
      "weighted avg       0.94      0.78      0.84       300\n",
      "\n",
      "Best score:  0.5324560604576718\n",
      "Best estimator:  Pipeline(memory='tmp',\n",
      "     steps=[('scaler', MinMaxScaler(copy=True, feature_range=(0, 1))), ('selector', VarianceThreshold(threshold=0.0005)), ('sampler', RandomOverSampler(random_state=None, ratio=None, return_indices=False,\n",
      "         sampling_strategy='auto')), ('pca', PCA(copy=True, iterated_power='auto', n_components=9, r...True, solver='lbfgs', tol=0.0001,\n",
      "       validation_fraction=0.1, verbose=False, warm_start=False))])\n",
      "Best parameters:  {'mlp__activation': 'logistic', 'mlp__alpha': 0.001, 'mlp__hidden_layer_sizes': 10, 'mlp__learning_rate': 'constant', 'mlp__max_iter': 300, 'mlp__solver': 'lbfgs', 'pca__n_components': 9, 'selector__threshold': 0.0005}\n"
     ]
    }
   ],
   "source": [
    "#Δοκιμάσαμε κάθε εφικτό συνδυασμό για την αρχιτεκτονική. Χρειαστήκαμε και αρκετό trial and error για να βρούμε καλά εύρη και διαβαθμίσεις\n",
    "#Τα αποτελέσματα βρίσκονται στο markdown που ακολουθεί\n",
    "\n",
    "mlp = MLPClassifier()\n",
    "\n",
    "n_components = [5,9]\n",
    "vthreshold = [0,0.0002,0.0005]\n",
    "scaler = MinMaxScaler()\n",
    "ros = RandomOverSampler()\n",
    "\n",
    "sizes = [5,10,15]\n",
    "activ = ['logistic','relu']\n",
    "alpha = [0.001, 0.005]\n",
    "learn_rate = ['constant','adaptive']\n",
    "itera = [200,250,300]\n",
    "solv = ['lbfgs','adam']\n",
    "\n",
    "pipe = Pipeline(steps=[('scaler', scaler), ('selector', selector),('sampler', ros), ('pca', pca),('mlp', mlp)], memory = 'tmp')\n",
    "estimator = GridSearchCV(pipe, dict(selector__threshold=vthreshold, pca__n_components=n_components,mlp__hidden_layer_sizes=sizes, mlp__activation=activ,mlp__alpha=alpha, mlp__learning_rate=learn_rate, mlp__max_iter=itera, mlp__solver=solv), cv=5, scoring='f1_macro', n_jobs=-1)\n",
    "\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "estimator.fit(strain, strain_labels)\n",
    "preds = estimator.predict(stest)\n",
    "print(\"Συνολικός χρόνος fit και predict: %s seconds\" % (time.time() - start_time))\n",
    "print(classification_report(stest_labels, preds))\n",
    "\n",
    "print('Best score: ', estimator.best_score_)\n",
    "print('Best estimator: ', estimator.best_estimator_)\n",
    "print('Best parameters: ', estimator.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A-WlEtuNLhb5"
   },
   "source": [
    "###Στάδια:\n",
    "1. MLP Classifier\n",
    "\n",
    "###Επίδοση, βέλτιστη παράμετρος:\n",
    "Best f1_macro score is  0.556411203114356\n",
    "\n",
    "Optimized params are: \n",
    "\n",
    ">activation = relu\n",
    "\n",
    ">alpha = 0.01\n",
    "\n",
    ">hidden layer size = 15\n",
    "\n",
    ">learning rate = adaptive\n",
    "\n",
    ">maximum iterations = 200\n",
    "\n",
    ">solver = lbfgs\n",
    "\n",
    "---\n",
    "###Στάδια: \n",
    "1.  Variance Threshold\n",
    "2. MLP Classifier\n",
    "\n",
    "### Επίδοση, βέλτιστες υπερπαράμετροι:\n",
    "Best f1_macro score is  0.5756587816305031\n",
    "\n",
    "Optimized params are: \n",
    "\n",
    ">threshold = 0\n",
    "\n",
    ">activation = relu\n",
    "\n",
    ">alpha = 0.005\n",
    "\n",
    ">hidden layer size = 10\n",
    "\n",
    ">learning rate = adaptive\n",
    "\n",
    ">maximum iterations = 300\n",
    "\n",
    ">solver = adam\n",
    "---\n",
    "\n",
    "###Στάδια: \n",
    "1.  Variance Threshold\n",
    "2. Standard Scaler\n",
    "3. MLP Classifier\n",
    "\n",
    "### Επίδοση, βέλτιστες υπερπαράμετροι:\n",
    "Best f1_macro score is **0.5833782068322692**\n",
    "\n",
    "Optimized params are: \n",
    "\n",
    ">threshold = 0\n",
    "\n",
    ">activation = logistic\n",
    "\n",
    ">alpha = 0.001\n",
    "\n",
    ">hidden layer size = 5\n",
    "\n",
    ">learning rate = adaptive\n",
    "\n",
    ">maximum iterations = 300\n",
    "\n",
    ">solver = lbfgs\n",
    "\n",
    "---\n",
    "\n",
    "###Στάδια: \n",
    "1.  Variance Threshold\n",
    "2. Standard Scaler\n",
    "3. Random Oversampler\n",
    "3. KNeighbors Classifier\n",
    "\n",
    "### Επίδοση, βέλτιστες υπερπαράμετροι:\n",
    "Best f1_macro score is 0.5165911982231136\n",
    "\n",
    "Optimized params are: \n",
    "\n",
    ">threshold = 1000\n",
    "\n",
    ">activation = logistic\n",
    "\n",
    ">alpha = 0.001\n",
    "\n",
    ">hidden layer size = 5\n",
    "\n",
    ">learning rate = constant\n",
    "\n",
    ">maximum iterations = 200\n",
    "\n",
    ">solver = adam\n",
    "\n",
    "---\n",
    "\n",
    "###Στάδια: \n",
    "1.  Variance Threshold\n",
    "2. Standard Scaler\n",
    "3. Random Oversampler\n",
    "4. PCA\n",
    "3. MLP Classifier\n",
    "\n",
    "### Επίδοση, βέλτιστες υπερπαράμετροι:\n",
    "Best f1_macro score is  0.52398813560042\n",
    "\n",
    "Optimized params are: \n",
    "\n",
    ">threshold = 0\n",
    "\n",
    ">n_componenents = 9\n",
    "\n",
    ">activation = logistic\n",
    "\n",
    ">alpha = 0.005\n",
    "\n",
    ">hidden layer size = 5\n",
    "\n",
    ">learning rate = adaptive\n",
    "\n",
    ">maximum iterations = 300\n",
    "\n",
    ">solver = lbfgs\n",
    "\n",
    "---\n",
    "\n",
    "---\n",
    "---\n",
    "###Θα χρησιμοποιήσουμε τώρα Min-max Scaler αντί για Standard Scaler\n",
    "\n",
    "###Στάδια:\n",
    "1. MinMax Scaler\n",
    "2. MLP Classifier\n",
    "\n",
    "###Επίδοση, βέλτιστες υπερπαράμετροι:\n",
    "Best f1_macro score is 0.5673125989670121\n",
    "\n",
    "Optimized params are: \n",
    "\n",
    ">activation = logistic\n",
    "\n",
    ">alpha = 0.005\n",
    "\n",
    ">hidden layer size = 10\n",
    "\n",
    ">learning rate = adaptive\n",
    "\n",
    ">maximum iterations = 200\n",
    "\n",
    ">solver = lbfgs\n",
    "\n",
    "---\n",
    "###Στάδια:\n",
    "1. MinMax Scaler\n",
    "2. Variance Threshold\n",
    "2. MLP Classifier\n",
    "\n",
    "###Επίδοση, βέλτιστες υπερπαράμετροι:\n",
    "Best f1_macro score is 0.5673125989670121\n",
    "\n",
    "Optimized params are: \n",
    "\n",
    ">threshold = 0\n",
    "\n",
    ">activation = logistic\n",
    "\n",
    ">alpha = 0.005\n",
    "\n",
    ">hidden layer size = 10\n",
    "\n",
    ">learning rate = adaptive\n",
    "\n",
    ">maximum iterations = 200\n",
    "\n",
    ">solver = lbfgs\n",
    "\n",
    "---\n",
    "###Στάδια:\n",
    "1. MinMax Scaler\n",
    "2. Variance Threshold\n",
    "3. Random Oversampler\n",
    "2. MLP Classifier\n",
    "\n",
    "###Επίδοση, βέλτιστες υπερπαράμετροι:\n",
    "Best f1_macro score is 0.5144931233141627\n",
    "\n",
    "Optimized params are:\n",
    "\n",
    ">threshold = 0\n",
    "\n",
    ">activation = relu\n",
    "\n",
    ">alpha = 0.001\n",
    "\n",
    ">hidden layer size = 5\n",
    "\n",
    ">learning rate = adaptive\n",
    "\n",
    ">maximum iterations = 300\n",
    "\n",
    ">solver = adam\n",
    "\n",
    "---\n",
    "\n",
    "###Στάδια:\n",
    "1. MinMax Scaler\n",
    "2. Variance Threshold\n",
    "3. Random Oversampler\n",
    "4. PCA\n",
    "2. MLP Classifier\n",
    "\n",
    "###Επίδοση, βέλτιστες υπερπαράμετροι:\n",
    "Best f1_macro score is 0.5324560604576718\n",
    "\n",
    "Optimized params are: \n",
    "\n",
    ">threshold = 0.0005\n",
    "\n",
    ">n_componenents = 9\n",
    "\n",
    ">activation = logistic\n",
    "\n",
    ">alpha = 0.001\n",
    "\n",
    ">hidden layer size = 10\n",
    "\n",
    ">learning rate = constant\n",
    "\n",
    ">maximum iterations = 300\n",
    "\n",
    ">solver = lbfgs\n",
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "colab_type": "code",
    "id": "DhWvFVzNLZp5",
    "outputId": "6c9396fc-78b7-4cfd-db8b-826bbdbd024a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Συνολικός χρόνος fit και predict: 58.473142862319946 seconds\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.95      0.96       289\n",
      "           1       0.12      0.18      0.14        11\n",
      "\n",
      "   micro avg       0.92      0.92      0.92       300\n",
      "   macro avg       0.54      0.56      0.55       300\n",
      "weighted avg       0.94      0.92      0.93       300\n",
      "\n",
      "Best score:  0.6126160024746692\n",
      "Best estimator:  Pipeline(memory='tmp',\n",
      "     steps=[('selector', VarianceThreshold(threshold=0)), ('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('mlp', MLPClassifier(activation='logistic', alpha=0.002, batch_size='auto',\n",
      "       beta_1=0.9, beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=4, l...True, solver='lbfgs', tol=0.0001,\n",
      "       validation_fraction=0.1, verbose=False, warm_start=False))])\n",
      "Best parameters:  {'mlp__activation': 'logistic', 'mlp__alpha': 0.002, 'mlp__hidden_layer_sizes': 4, 'mlp__learning_rate': 'adaptive', 'mlp__max_iter': 300, 'mlp__solver': 'lbfgs', 'selector__threshold': 0}\n"
     ]
    }
   ],
   "source": [
    "#Optimized macro\n",
    "mlp = MLPClassifier()\n",
    "\n",
    "vthreshold = [0,10,50]\n",
    "scaler = StandardScaler()\n",
    "ros = RandomOverSampler()\n",
    "\n",
    "sizes = [4,5,6]\n",
    "activ = ['logistic']\n",
    "alpha = [0.0005, 0.001, 0.002]\n",
    "learn_rate = ['adaptive']\n",
    "itera = [270,300,350]\n",
    "solv = ['lbfgs']\n",
    "\n",
    "pipe = Pipeline(steps=[('selector', selector),('scaler', scaler), ('mlp', mlp)], memory = 'tmp')\n",
    "estimator = GridSearchCV(pipe, dict(selector__threshold=vthreshold,mlp__hidden_layer_sizes=sizes, mlp__activation=activ,mlp__alpha=alpha, mlp__learning_rate=learn_rate, mlp__max_iter=itera, mlp__solver=solv), cv=5, scoring='f1_macro', n_jobs=-1)\n",
    "\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "estimator.fit(strain, strain_labels)\n",
    "preds = estimator.predict(stest)\n",
    "print(\"Συνολικός χρόνος fit και predict: %s seconds\" % (time.time() - start_time))\n",
    "print(classification_report(stest_labels, preds))\n",
    "\n",
    "print('Best score: ', estimator.best_score_)\n",
    "print('Best estimator: ', estimator.best_estimator_)\n",
    "print('Best parameters: ', estimator.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 322
    },
    "colab_type": "code",
    "id": "juqHFLSDW5OP",
    "outputId": "daffea36-732d-44b9-9ff1-34c0e3b6906d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Συνολικός χρόνος fit και predict: 31.82580041885376 seconds\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.98     12405\n",
      "           1       0.65      0.22      0.33       617\n",
      "\n",
      "   micro avg       0.96      0.96      0.96     13022\n",
      "   macro avg       0.81      0.61      0.66     13022\n",
      "weighted avg       0.95      0.96      0.95     13022\n",
      "\n",
      "Best score:  0.61495363458022\n",
      "Best estimator:  Pipeline(memory='tmp',\n",
      "     steps=[('selector', VarianceThreshold(threshold=0)), ('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('mlp', MLPClassifier(activation='logistic', alpha=0.002, batch_size='auto',\n",
      "       beta_1=0.9, beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=4, l...True, solver='lbfgs', tol=0.0001,\n",
      "       validation_fraction=0.1, verbose=False, warm_start=False))])\n",
      "Best parameters:  {'mlp__activation': 'logistic', 'mlp__alpha': 0.002, 'mlp__hidden_layer_sizes': 4, 'mlp__learning_rate': 'adaptive', 'mlp__max_iter': 300, 'mlp__solver': 'lbfgs', 'selector__threshold': 0}\n"
     ]
    }
   ],
   "source": [
    "#Fit and transform whole dataset\n",
    "\n",
    "mlp = MLPClassifier()\n",
    "\n",
    "vthreshold = [0]\n",
    "scaler = StandardScaler()\n",
    "ros = RandomOverSampler()\n",
    "\n",
    "sizes = [4]\n",
    "activ = ['logistic']\n",
    "alpha = [0.002]\n",
    "learn_rate = ['adaptive']\n",
    "itera = [300]\n",
    "solv = ['lbfgs']\n",
    "\n",
    "pipe = Pipeline(steps=[('selector', selector),('scaler', scaler), ('mlp', mlp)], memory = 'tmp')\n",
    "estimator = GridSearchCV(pipe, dict(selector__threshold=vthreshold,mlp__hidden_layer_sizes=sizes, mlp__activation=activ,mlp__alpha=alpha, mlp__learning_rate=learn_rate, mlp__max_iter=itera, mlp__solver=solv), cv=5, scoring='f1_macro', n_jobs=-1)\n",
    "\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "estimator.fit(train, train_labels)\n",
    "preds = estimator.predict(test)\n",
    "print(\"Συνολικός χρόνος fit και predict: %s seconds\" % (time.time() - start_time))\n",
    "print(classification_report(test_labels, preds))\n",
    "\n",
    "print('Best score: ', estimator.best_score_)\n",
    "print('Best estimator: ', estimator.best_estimator_)\n",
    "print('Best parameters: ', estimator.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yjLG9xlzOCuP"
   },
   "outputs": [],
   "source": [
    "#times_macro[\"Optimized MLP\"] = end_time-start_time\n",
    "print(\"Συνολικός χρόνος fit και predict: %s seconds\\n\" %(end_time-start_time))\n",
    "\n",
    "print(estimator.best_estimator_)\n",
    "print(estimator.best_params_)\n",
    "\n",
    "print(\"\\n Optimized MLP:\")\n",
    "print(confusion_matrix(test_labels, preds))\n",
    "print(classification_report(test_labels, preds))\n",
    "\n",
    "f1_macro_opt[\"MLP_optimized\"] = f1_score(test_labels, preds, average = 'macro')\n",
    "\n",
    "f1_macro_opt[\"MLP_optimized\"] = f1_score(test_labels, preds, average = 'macro')\n",
    "metavoli['MLP'] = f1_macro_opt['MLP_optimized'] - f1_macro['MLP_default']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 320
    },
    "colab_type": "code",
    "id": "Deu1ISNxOG0V",
    "outputId": "5eca9e2b-22c2-45bb-f541-118aafaaeb90"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlEAAAEvCAYAAABlvJTyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n8F+McP6HFzmjPQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff289056390>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(1, figsize=(10, 5))\n",
    "plt.bar(range(len(f1_macro_opt)), list(f1_macro_opt.values()), align='center')\n",
    "plt.xticks(range(len(f1_macro_opt)), list(f1_macro_opt.keys()))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 151
    },
    "colab_type": "code",
    "id": "QiJiBQcBhLms",
    "outputId": "7a5cac34-d97d-45bf-ff51-b5222a7d976a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GNB 0.8844263553985563\n",
      "Uniform 0.004299229544804939\n",
      "Constant 0 -0.46475141761934546\n",
      "Constant 1 -0.0021434339619543705\n",
      "Most Frequent -0.46475141761934546\n",
      "Stratified -0.5402974701814941\n",
      "kNN 0.07693047953844345\n",
      "MLP 0.05816782707827017\n"
     ]
    }
   ],
   "source": [
    "for header in metavoli.keys():\n",
    "  print(header, metavoli[header])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T50iZBenhNE5"
   },
   "source": [
    "## Σχολιασμός\n",
    "\n",
    "Βλέπουμε πως το MLP απέδωσε καλύτερα από τα υπόλοιπα μοντέλα και στο f1_macro. Και πάλι, τη σημαντικότερη βελτίωση είχε ο GNB\n",
    "\n",
    "Συμπεραίνουμε πως για το συγκεκριμένο dataset, το MLP (με την αρχιτεκτονική pipeline που αναλύθηκε), είναι μάλλον ο καλύτερος ταξινομητής από αυτούς που έχουμε στη διάθεσή μας, αλλά λόγω του μεγαλύτερου αριθμού προς βελτιστοποίηση είναι ο πιο χρονοβόρος.\n",
    "\n",
    "*(Δε θα μπορούσαμε να συγκρίνουμε χρόνους καθώς το optimized grid search δεν έγινε σε όλο το dataset αλλά σε τυχαίο δείγμα. Πάντως, από τους χρόνους που εξάγαμε από το Grid Search στα τυχαία δείγματα είναι χωρίς αμφιβολία ο MLP.)*\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Άσκ 1 - Big Dataset.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
